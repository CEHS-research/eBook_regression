[
["index.html", "Encyclopedia of Quantitative Methods in R, vol. 4: Multiple Linear Regression Welcome Blocked Notes Code and Output The Authors", " Encyclopedia of Quantitative Methods in R, vol. 4: Multiple Linear Regression Sarah Schwartz &amp; Tyson Barrett Last updated: 2018-10-17 Welcome Backgroup and links to other volumes of this encyclopedia may be found at the Encyclopedia’s Home Website. Blocked Notes Thoughout all the eBooks in this encyclopedia, several small secitons will be blocked out in the following ways: These blocks denote an area UNDER CONSTRUCTION, so check back often. This massive undertaking started during the summer of 2018 and is far from complete. The outline of seven volumes is given above despite any one being complete. Feedback is welcome via either author’s email. These blocks denote something EXTREMELY IMPORTANT. Do NOT skip these notes as they will be used very sparingly. These blocks denote something to DOWNLOAD. This may include software installations, example datasets, or notebook code files. These blocks denote something INTERESTING. These point out information we found of interest or added value. These blocks denote LINKS to other websites. This may include instructional video clips, articles, or blog posts. We are all about NOT re-creating the wheel. If somebody else has described or illustrated a topic well, we celebrate it! Code and Output This is how \\(R\\) code is shown: 1 + 1 This is what the output of the \\(R\\) code above will look: ## [1] 2 The Authors Dr. Sarah Schwartz Dr. Tyson Barrett www.SarahSchwartzStats.com www.TysonBarrett.com Sarah.Schwartz@usu.edu Tyson.Barrett@usu.edu Statistical Consulting Studio Data Science and Discover Unit Why choose R ? Check it out: an article from Fall 2016… No more excuses: R is better than SPSS for psychology undergrads, and students agree FYI This entire encyclopedia is written in \\(R Markdown\\), using \\(R Studio\\) as the text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The book’s source code is hosted on GitHub. If you notice typos or other issues, feel free to email either of the authors. This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. "],
["simple-linear-regression-ex-ventricular-shortening-velocity-single-continuous-iv.html", "1 Simple Linear Regression - Ex: Ventricular Shortening Velocity (single continuous IV) 1.1 Purpose 1.2 Exploratory Data Analysis 1.3 Regression Analysis 1.4 Conclusion", " 1 Simple Linear Regression - Ex: Ventricular Shortening Velocity (single continuous IV) library(tidyverse) # super helpful everything! library(magrittr) # includes other versions of the pipe library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools library(ISwR) # Introduction to Statistics with R (datasets) 1.1 Purpose 1.1.1 Research Question Is there a relationship between fasting blood flucose and shortening of ventricular velocity among type 1 diabetic patiences? If so, what is the nature of the association? 1.1.2 Data Description This dataset is included in the ISwR package (Dalgaard 2015), which was a companion to the texbook “Introductory Statistics with R, 2nd ed.” (Dalgaard 2008), although it was first published by Altman (1991) in table 11.6. The thuesen data frame has 24 rows and 2 columns. It contains ventricular shortening velocity and blood glucose for type 1 diabetic patients. blood.glucose a numeric vector, fasting blood glucose (mmol/l). short.velocity a numeric vector, mean circumferential shortening velocity (%/s). data(thuesen, package = &quot;ISwR&quot;) tibble::glimpse(thuesen) # view the class and 1st few values of each variable Observations: 24 Variables: 2 $ blood.glucose &lt;dbl&gt; 15.3, 10.8, 8.1, 19.5, 7.2, 5.3, 9.3, 11.1, 7.5... $ short.velocity &lt;dbl&gt; 1.76, 1.34, 1.27, 1.47, 1.27, 1.49, 1.31, 1.09,... 1.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 1.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). thuesen %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max blood.glucose 24 10.300 4.338 4.200 7.075 12.700 19.500 short.velocity 23 1.326 0.233 1.030 1.185 1.420 1.950 The stargazer() function has many handy options, should you wish to change the default settings. thuesen %&gt;% stargazer(type = &quot;html&quot;, digits = 4, flip = TRUE, summary.stat = c(&quot;n&quot;, &quot;mean&quot;, &quot;sd&quot;, &quot;min&quot;, &quot;median&quot;, &quot;max&quot;), title = &quot;Descriptives&quot;) Descriptives Statistic blood.glucose short.velocity N 24 23 Mean 10.3000 1.3257 St. Dev. 4.3375 0.2329 Min 4.2000 1.0300 Median 9.4000 1.2700 Max 19.5000 1.9500 Although the table1() function from the furniture package creates a nice summary table, it ‘hides’ the nubmer of missing values for each continuous variable (Barrett, Brignone, and Laxman 2018). thuesen %&gt;% furniture::table1(blood.glucose, short.velocity, output = &quot;html&quot;) Mean/Count (SD/%) n = 23 blood glucose 10.4 (4.4) short velocity 1.3 (0.2) 1.2.2 Univariate Visualizations thuesen %&gt;% ggplot() + aes(blood.glucose) + # variable of interest (just one) geom_histogram(binwidth = 2) # specify the width of the bars thuesen %&gt;% ggplot() + aes(short.velocity) + # variable of interest (just one) geom_histogram(bins = 10) # specify the number of bars 1.2.3 Bivariate Statistics (Unadjusted Pearson’s correlation) The cor() fucntion in base \\(R\\) doesn’t like NA or missing values thuesen %&gt;% cor() blood.glucose short.velocity blood.glucose 1 NA short.velocity NA 1 You may specify how to handle cases that are missing on at least one of the variables of interest: use = &quot;everything&quot; NAs will propagate conceptually, i.e., a resulting value will be NA whenever one of its contributing observations is NA &lt;– DEFAULT *use = &quot;all.obs&quot; the presence of missing observations will produce an error use = &quot;complete.obs&quot; missing values are handled by casewise deletion (and if there are no complete cases, that gives an error). use = &quot;na.or.complete&quot; is the same as above unless there are no complete cases, that gives NA use = &quot;pairwise.complete.obs&quot; the correlation between each pair of variables is computed using all complete pairs of observations on those variables. This can result in covariance matrices which are not positive semi-definite, as well as NA entries if there are no complete pairs for that pair of variables. Commonly, we want listwise deletion: thuesen %&gt;% cor(use = &quot;complete.obs&quot;) # list-wise deletion blood.glucose short.velocity blood.glucose 1.0000000 0.4167546 short.velocity 0.4167546 1.0000000 It is also handy to specify the number of decimal places desired, but adding a rounding step: thuesen %&gt;% cor(use=&quot;complete.obs&quot;) %&gt;% round(2) # number od decimal places blood.glucose short.velocity blood.glucose 1.00 0.42 short.velocity 0.42 1.00 If you desire a correlation single value of a single PAIR of variables, instead of a matrix, then you must use a magrittr exposition pipe (%$%) thuesen %$% # notice the special kind of pipe cor(blood.glucose, short.velocity, # specify exactly TWO variables use=&quot;complete.obs&quot;) [1] 0.4167546 In addition to the cor() funciton, the base \\(R\\) stats package also includes the cor.test() function to test if the correlation is zero (\\(H_0: R = 0\\)) This TESTS if the cor == 0 thuesen %$% # notice the special kind of pipe cor.test(blood.glucose, short.velocity, # specify exactly TWO variables use=&quot;complete.obs&quot;) Pearson&#39;s product-moment correlation data: blood.glucose and short.velocity t = 2.101, df = 21, p-value = 0.0479 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.005496682 0.707429479 sample estimates: cor 0.4167546 The default correltaion type for cor()is Pearson’s \\(R\\), which assesses linear relationships. Spearman’s correlation assesses monotonic relationships. thuesen %$% # notice the special kind of pipe cor(blood.glucose, short.velocity, # specify exactly TWO variables use = &#39;complete&#39;, method = &#39;spearman&#39;) # spearman&#39;s (rho) [1] 0.318002 1.2.4 Bivariate Visualization Scatterplots show the relationship between two continuous measures (one on the \\(x-axis\\) and the other on the \\(y-axis\\)), with one point for each observation. thuesen %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = short.velocity)) + # y-axis variable geom_point() + # place a point for each observation theme_bw() # black-and-white theme Both the code chunk above and below produce the same plot. ggplot(thuesen, aes(x = blood.glucose, # x-axis variable y = short.velocity)) + # y-axis variable geom_point() + # place a point for each observation theme_bw() # black-and-white theme 1.3 Regression Analysis 1.3.1 Fit A Simple Linear Model \\[ Y = \\beta_0 + \\beta_1 \\times X \\] short.velocity dependent variable or outcome (\\(Y\\)) blood.glucose independent variable or predictor (\\(X\\)) The lm() function must be supplied with at least two options: a formula: Y ~ X a dataset: data = XXXXXXX When a model is fit and directly saved as a named object via the assignment opperator (&lt;-), no output is produced. fit_vel_glu &lt;- lm(short.velocity ~ blood.glucose, data = thuesen) Running the name of the fit object yields very little output: fit_vel_glu Call: lm(formula = short.velocity ~ blood.glucose, data = thuesen) Coefficients: (Intercept) blood.glucose 1.09781 0.02196 Appling the summary() funciton produced a good deal more output: summary(fit_vel_glu) Call: lm(formula = short.velocity ~ blood.glucose, data = thuesen) Residuals: Min 1Q Median 3Q Max -0.40141 -0.14760 -0.02202 0.03001 0.43490 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.09781 0.11748 9.345 6.26e-09 *** blood.glucose 0.02196 0.01045 2.101 0.0479 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2167 on 21 degrees of freedom (1 observation deleted due to missingness) Multiple R-squared: 0.1737, Adjusted R-squared: 0.1343 F-statistic: 4.414 on 1 and 21 DF, p-value: 0.0479 You may request specific pieces of the output: Coefficients or beta estimates: coef(fit_vel_glu) (Intercept) blood.glucose 1.09781488 0.02196252 95% confidence intervals for the coefficients or beta estimates: confint(fit_vel_glu) 2.5 % 97.5 % (Intercept) 0.8534993816 1.34213037 blood.glucose 0.0002231077 0.04370194 The F-test for overall modle fit vs. a \\(null\\) or empty model having only an intercept and no predictors. anova(fit_vel_glu) # A tibble: 2 x 5 Df `Sum Sq` `Mean Sq` `F value` `Pr(&gt;F)` * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0.207 0.207 4.41 0.0479 2 21 0.986 0.0470 NA NA Various other model fit indicies: logLik(fit_vel_glu) &#39;log Lik.&#39; 3.583612 (df=3) AIC(fit_vel_glu) [1] -1.167223 BIC(fit_vel_glu) [1] 2.239259 1.3.2 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_vel_glu, which = 1) plot(fit_vel_glu, which = 2) plot(fit_vel_glu, which = 5) plot(fit_vel_glu, which = 6) Viewing potentially influencial or outlier points based on plots above: thuesen %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::filter(id == c(13, 20, 24)) # A tibble: 3 x 3 blood.glucose short.velocity id &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 19 1.95 13 2 16.1 1.05 20 3 9.5 1.7 24 The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2018). car::residualPlots(fit_vel_glu) Test stat Pr(&gt;|Test stat|) blood.glucose 0.9289 0.3640 Tukey test 0.9289 0.3529 Here is a fancy way to visulaize ‘potential problem cases’ with ggplot2: thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases ggplot() + # name the FULL dataset aes(x = blood.glucose, # x-axis variable name y = short.velocity) + # y-axis variable name geom_point() + # do a scatterplot stat_smooth(method = &quot;lm&quot;) + # smooth: linear model theme_bw() + # black-and-while theme geom_point(data = thuesen %&gt;% # override the dataset from above filter(row_number() == c(13, 20, 24)), # with a reduced subset of cases size = 4, # make the points bigger in size color = &quot;red&quot;) # give the points a different color 1.3.3 Manually checking residual diagnostics You may extract values from the model in dataset form and then you can maually plot the residuals. thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) # residual values # A tibble: 23 x 4 blood.glucose short.velocity pred resid &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 15.3 1.76 1.43 0.326 2 10.8 1.34 1.34 0.00499 3 8.1 1.27 1.28 -0.00571 4 19.5 1.47 1.53 -0.0561 5 7.2 1.27 1.26 0.0141 6 5.3 1.49 1.21 0.276 7 9.3 1.31 1.30 0.00793 8 11.1 1.09 1.34 -0.252 9 7.5 1.18 1.26 -0.0825 10 12.2 1.22 1.37 -0.146 # ... with 13 more rows Check for equal spread of points along the \\(y=0\\) horizontal line: thuesen %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) %&gt;% # residual values ggplot() + aes(x = id, y = resid) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, size = 1, linetype = &quot;dashed&quot;) + theme_classic() + labs(title = &quot;Looking for homogeneity of residuals&quot;, subtitle = &quot;want to see equal spread all across&quot;) Check for normality: thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) %&gt;% # residual values ggplot() + aes(resid) + geom_histogram(bins = 12, color = &quot;blue&quot;, fill = &quot;blue&quot;, alpha = 0.3) + geom_vline(xintercept = 0, size = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + theme_classic() + labs(title = &quot;Looking for normality of residuals&quot;, subtitle = &quot;want to see roughly a bell curve&quot;) 1.4 Conclusion 1.4.1 Tabulate the Final Model Summary You may also present the output in a table using two different packages: The stargazer package has stargazer() function: stargazer::stargazer(fit_vel_glu, type = &quot;html&quot;) Dependent variable: short.velocity blood.glucose 0.022** (0.010) Constant 1.098*** (0.117) Observations 23 R2 0.174 Adjusted R2 0.134 Residual Std. Error 0.217 (df = 21) F Statistic 4.414** (df = 1; 21) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 The stargazer package can produce the regression table in various output types: type = “latex Default Use when knitting your .Rmd file to a .pdf via LaTeX type = “text Default Use when working on a project and viewing tables on your computer screen type = “html Default Use when knitting your .Rmd file to a .html document The texreg package has the texreg() fucntion: texreg::htmlreg(fit_vel_glu) Statistical models Model 1 (Intercept) 1.10*** (0.12) blood.glucose 0.02* (0.01) R2 0.17 Adj. R2 0.13 Num. obs. 23 RMSE 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 The texreg package contains three version of the regression table function. screenreg() Use when working on a project and viewing tables on your computer screen htmlreg() Use when knitting your .Rmd file to a .html document texreg() Use when knitting your .Rmd file to a .pdf via LaTeX 1.4.2 Plot the Model When a model only contains main effects, a plot is not important for interpretation, but can help understand the relationship between multiple predictors. The Effect() function from the effects package chooses ‘5 or 6 nice values’ for your continuous independent variable (\\(X\\)) based on the range of values found in the dataset on which the model was fit and plugs them into the regression equation \\(Y = \\beta_0 + \\beta_1 \\times X\\) to compute the predicted mean value of the outcome (\\(Y\\)) (Fox et al. 2018). effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), # IV variable name mod = fit_vel_glu) # fitted model name blood.glucose effect blood.glucose 4.2 8 12 16 20 1.190057 1.273515 1.361365 1.449215 1.537065 You may override the ‘nice values’ using the xlevels = list(var_name = c(#, #, ...#) option. effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = c(5, 10, 15, 20))) blood.glucose effect blood.glucose 5 10 15 20 1.207627 1.317440 1.427253 1.537065 Adding a piped data frame step (%&gt;% data.frame()) will arrange the predicted \\(Y\\) values into a column called fit. This tidy data format is ready for plotting. effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu) %&gt;% data.frame() # A tibble: 5 x 5 blood.glucose fit se lower upper * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4.2 1.19 0.0788 1.03 1.35 2 8 1.27 0.0516 1.17 1.38 3 12 1.36 0.0483 1.26 1.46 4 16 1.45 0.0742 1.29 1.60 5 20 1.54 0.110 1.31 1.77 effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = c(5, 12, 20))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .5) + # ribbon transparency level geom_line() + theme_bw() Notice that although the regression line is smooth, the ribbon is choppy. This is because we are basing it on only THREE values of \\(X\\). c(5, 12, 20) [1] 5 12 20 Use the seq() function in base \\(R\\) to request many values of \\(X\\) seq(from = 5, to = 20, by = 5) [1] 5 10 15 20 seq(from = 5, to = 20, by = 2) [1] 5 7 9 11 13 15 17 19 seq(from = 5, to = 20, by = 1) [1] 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 seq(from = 5, to = 20, by = .5) [1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 11.5 [15] 12.0 12.5 13.0 13.5 14.0 14.5 15.0 15.5 16.0 16.5 17.0 17.5 18.0 18.5 [29] 19.0 19.5 20.0 effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .5) + # ribbon transparency level geom_line() + theme_bw() Now that we are basing our ribbon on MANY more points of \\(X\\), the ribbon is much smoother. For publication, you would of course want to clean up the plot a bit more: effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .3) + # ribbon transparency level geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) # axis labels The above plot has a ribbon that represents a 95% confidence interval (lower toupper) for the MEAN (fit) outcome. Sometimes we would rather display a ribbon for only the MEAN (fit) plus-or-minus ONE STANDARD ERROR (se) for the mean. You would do that by changing the variables that define the min and max edges of the ribbon (notice the range of the y-axis has changed): effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, y = fit) + geom_ribbon(aes(ymin = fit - se, # bottom edge of the ribbon ymax = fit + se), # top edge of the ribbon alpha = .3) + geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) Of course, you could do both ribbons together: effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, y = fit) + geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon = lower of the 95% CI ymax = upper), # top edge of the ribbon = upper of the 95% CI alpha = .3) + geom_ribbon(aes(ymin = fit - se, # bottom edge of the ribbon = mean - SE ymax = fit + se), # top edge of the ribbon = Mean + SE alpha = .3) + geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) # axis labels "],
["multiple-linear-regression-ex-obesity-and-blood-pressure-interaction-between-a-continuous-and-categorical-ivs.html", "2 Multiple Linear Regression - Ex: Obesity and Blood Pressure (interaction between a continuous and categorical IVs) 2.1 Purpose 2.2 Exploratory Data Analysis 2.3 Regression Analysis 2.4 Conclusion", " 2 Multiple Linear Regression - Ex: Obesity and Blood Pressure (interaction between a continuous and categorical IVs) library(tidyverse) # super helpful everything! library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools library(GGally) # extensions to ggplot2 library(ISwR) # Introduction to Statistics with R (datasets) 2.1 Purpose 2.1.1 Research Question Is obsesity associated with higher blood pressure and is that relationship the same among men and women? 2.1.2 Data Description This dataset is included in the ISwR package (Dalgaard 2015), which was a companion to the texbook “Introductory Statistics with R, 2nd ed.” (Dalgaard 2008), although it was first published by Brown and Hollander (1977). To view the documentation for the dataset, type ?bp.obese in the console and enter or search the help tab for `bp.obese’. The bp.obese data frame has 102 rows and 3 columns. It contains data from a random sample of Mexican-American adults in a small California town. This data frame contains the following columns: sex a numeric vector code, 0: male, 1: female obese a numeric vector, ratio of actual weight to ideal weight from New York Metropolitan Life Tables bp a numeric vector,systolic blood pressure (mm Hg) data(bp.obese, package = &quot;ISwR&quot;) bp.obese &lt;- bp.obese %&gt;% dplyr::mutate(sex = factor(sex, labels = c(&quot;Male&quot;, &quot;Female&quot;))) tibble::glimpse(bp.obese) Observations: 102 Variables: 3 $ sex &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Ma... $ obese &lt;dbl&gt; 1.31, 1.31, 1.19, 1.11, 1.34, 1.17, 1.56, 1.18, 1.04, 1.... $ bp &lt;int&gt; 130, 148, 146, 122, 140, 146, 132, 110, 124, 150, 120, 1... 2.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 2.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). bp.obese %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max obese 102 1.313 0.258 0.810 1.143 1.430 2.390 bp 102 127.020 18.184 94 116 137.5 208 2.2.2 Bivariate Relationships The furniture package’s table1() function is a clean way to create a descriptive table that compares distinct subgroups of your sample (Barrett, Brignone, and Laxman 2018). bp.obese %&gt;% furniture::table1(obese, bp, splitby = ~ sex, test = TRUE, output = &quot;html&quot;) Male Female P-Value n = 44 n = 58 obese &lt;.001 1.2 (0.2) 1.4 (0.3) bp 0.646 128.0 (16.6) 126.3 (19.4) The ggpairs() function in the GGally package is helpful for showing all pairwise relationships in raw data, especially seperating out two or three groups (Schloerke et al. 2018). GGally::ggpairs(bp.obese, mapping = aes(fill = sex, col = sex, alpha = 0.1), upper = list(continuous = &quot;smooth&quot;, combo = &quot;facethist&quot;, discrete = &quot;ratio&quot;), lower = list(continuous = &quot;cor&quot;, combo = &quot;box&quot;, discrete = &quot;facetbar&quot;), title = &quot;Very Useful for Exploring Data&quot;) bp.obese %&gt;% ggplot() + aes(x = sex, y = bp, fill = sex) + geom_boxplot(alpha = 0.6) + scale_fill_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;)) + labs(x = &quot;Gender&quot;, y = &quot;Blood Pressure (mmHg)&quot;) + guides(fill = FALSE) + theme_bw() Visual inspection for an interaction (is gender a moderator?) bp.obese %&gt;% ggplot(aes(x = obese, y = bp, color = sex)) + geom_point(size = 3) + geom_smooth(aes(fill = sex), alpha = 0.2, method = &quot;lm&quot;) + scale_color_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;), breaks = c(&quot;male&quot;, &quot;female&quot;), labels = c(&quot;Men&quot;, &quot;Women&quot;)) + scale_fill_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;), breaks = c(&quot;male&quot;, &quot;female&quot;), labels = c(&quot;Men&quot;, &quot;Women&quot;)) + labs(title = &quot;Does Gender Moderate the Association Between Obesity and Blood Pressure?&quot;, x = &quot;Ratio: Actual Weight vs. Ideal Weight (NYM Life Tables)&quot;, y = &quot;Systolic Blood Pressure (mmHg)&quot;) + theme_bw() + scale_x_continuous(breaks = seq(from = 0, to = 3, by = 0.25 )) + scale_y_continuous(breaks = seq(from = 75, to = 300, by = 25)) + theme(legend.title = element_blank(), legend.key = element_rect(fill = &quot;white&quot;), legend.background = element_rect(color = &quot;black&quot;), legend.justification = c(1, 0), legend.position = c(1, 0)) bp.obese %&gt;% dplyr::mutate(sex = as.numeric(sex)) %&gt;% # cor needs only numeric cor() %&gt;% round(3) sex obese bp sex 1.000 0.405 -0.045 obese 0.405 1.000 0.326 bp -0.045 0.326 1.000 Often it is easier to digest a correlation matrix if it is visually presented, instead of just given as a table of many numbers. The corrplot package has a useful function called corrplot.mixed() for doing just that (Wei and Simko 2017). bp.obese %&gt;% dplyr::mutate(sex = as.numeric(sex)) %&gt;% # cor needs only numeric cor() %&gt;% corrplot::corrplot.mixed(lower = &quot;ellipse&quot;, upper = &quot;number&quot;, tl.col = &quot;black&quot;) 2.3 Regression Analysis 2.3.1 Fit Nested Models The bottom-up approach consists of starting with an initial NULL model with only an intercept term and them building additional models that are nested. Two models are considered nested if one is conains a subset of the terms (predictors or IV) compared to the other. fit_bp_null &lt;- lm(bp ~ 1, data = bp.obese) # intercept only or NULL model fit_bp_sex &lt;- lm(bp ~ sex, data = bp.obese) fit_bp_obe &lt;- lm(bp ~ obese, data = bp.obese) fit_bp_obesex &lt;- lm(bp ~ obese + sex, data = bp.obese) fit_bp_inter &lt;- lm(bp ~ obese*sex, data = bp.obese) 2.3.2 Comparing Nested Models 2.3.2.1 Model Comparison Table In single level, multiple linear regression significance of predictors (independent variables, IV) is usually based on both the Wald tests of significance for each beta estimate (shown with stars here) and comparisons in the model fit via the \\(R^2\\) values. Again the texreg package comes in handy to display several models in the same tal e (Leifeld 2017). texreg::htmlreg(list(fit_bp_null, fit_bp_sex, fit_bp_obe, fit_bp_obesex, fit_bp_inter), custom.model.names = c(&quot;No Predictors&quot;, &quot;Only Sex Quiz&quot;, &quot;Only Obesity&quot;, &quot;Both IVs&quot;, &quot;Add Interaction&quot;)) Statistical models No Predictors Only Sex Quiz Only Obesity Both IVs Add Interaction (Intercept) 127.02*** 127.95*** 96.82*** 93.29*** 102.11*** (1.80) (2.75) (8.92) (8.94) (18.23) sexFemale -1.64 -7.73* -19.60 (3.65) (3.72) (21.66) obese 23.00*** 29.04*** 21.65 (6.67) (7.17) (15.12) obese:sexFemale 9.56 (17.19) R2 0.00 0.00 0.11 0.14 0.15 Adj. R2 0.00 -0.01 0.10 0.13 0.12 Num. obs. 102 102 102 102 102 RMSE 18.18 18.26 17.28 17.00 17.05 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 2.3.2.2 Likelihood Ratio Test of Nested Models An alternative method for determing model fit and variable importance is the likelihood ratio test. This involves comparing the \\(-2LL\\) or inverse of twice the log of the likelihood value for the model. The difference in these values follows a Chi Squared distribution with degrees of freedom equal to the difference in the number of parameters estimated (number of betas). Test the main effect of math quiz: anova(fit_bp_null, fit_bp_sex) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 101 33398. NA NA NA NA 2 100 33330. 1 67.6 0.203 0.653 Test the main effect of math phobia anova(fit_bp_null, fit_bp_obe) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 101 33398. NA NA NA NA 2 100 29846. 1 3552. 11.9 0.000822 Test the main effect of math phobia, after controlling for math test anova(fit_bp_obe, fit_bp_obesex) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 100 29846. NA NA NA NA 2 99 28595. 1 1250. 4.33 0.0401 Test the interaction between math test and math phobia (i.e. moderation) anova(fit_bp_obesex, fit_bp_inter) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 99 28595. NA NA NA NA 2 98 28505. 1 89.9 0.309 0.579 2.3.3 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_bp_obesex, which = 1) plot(fit_bp_obesex, which = 4, id.n = 10) # Change the number labeled The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2018). car::residualPlots(fit_bp_obesex) Test stat Pr(&gt;|Test stat|) obese -0.2759 0.7832 sex Tukey test -0.6141 0.5391 you can adjust any part of a ggplot bp.obese %&gt;% dplyr::mutate(e_bp = resid(fit_bp_obesex)) %&gt;% # add the resid to the dataset ggplot(aes(x = sex, # x-axis variable name y = e_bp, # y-axis variable name color = sex, # color is the outline fill = sex)) + # fill is the inside geom_hline(yintercept = 0, # set at a meaningful value size = 1, # adjust line thickness linetype = &quot;dashed&quot;, # set type of line color = &quot;purple&quot;) + # color of line geom_boxplot(alpha = 0.5) + # level of transparency theme_bw() + # my favorite theme labs(title = &quot;Check Assumptions&quot;, # main title&#39;s text x = &quot;Gender&quot;, # x-axis text label y = &quot;Blood Pressure, Residual (bpm)&quot;) + # y-axis text label scale_y_continuous(breaks = seq(from = -40, # declare a sequence of to = 80, # values to make the by = 20)) + # tick marks at guides(color = FALSE, fill = FALSE) # no legends included bp.obese %&gt;% dplyr::mutate(e_bp = resid(fit_bp_obesex)) %&gt;% # add the resid to the dataset ggplot(aes(x = e_bp, # y-axis variable name color = sex, # color is the outline fill = sex)) + # fill is the inside geom_density(alpha = 0.5) + geom_vline(xintercept = 0, # set at a meaningful value size = 1, # adjust line thickness linetype = &quot;dashed&quot;, # set type of line color = &quot;purple&quot;) + # color of line theme_bw() + # my favorite theme labs(title = &quot;Check Assumptions&quot;, # main title&#39;s text x = &quot;Blood Pressure, Residual (bpm)&quot;) + # y-axis text label scale_x_continuous(breaks = seq(from = -40, # declare a sequence of to = 80, # values to make the by = 20)) # tick marks at 2.4 Conclusion Violations to the assumtions call the reliabity of the regression results into question. The data should be further investigated, specifically the \\(102^{nd}\\) case. "],
["multiple-linear-regression-ex-ihnos-experiment-interaction-between-two-continuous-ivs.html", "3 Multiple Linear Regression - Ex: Ihno’s Experiment (interaction between two continuous IVs) 3.1 Purpose 3.2 Exploratory Data Analysis 3.3 Regression Analysis 3.4 Conclusion 3.5 Write-up", " 3 Multiple Linear Regression - Ex: Ihno’s Experiment (interaction between two continuous IVs) library(tidyverse) # super helpful everything! library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools 3.1 Purpose 3.1.1 Research Question Does math phobia moderate the relationship between math and statistics performance? That is, does the assocation between math and stat quiz performance differ at variaous levels of math phobia? 3.1.2 Data Description Inho’s dataset is included in the textbook “Explaining Psychological Statistics” (Cohen 2013) and details regarding the sample and measures is describe in this Encyclopedia’s Vol. 2 - Ihno’s Dataset. data_ihno &lt;- haven::read_spss(&quot;http://www.psych.nyu.edu/cohen/Ihno_dataset.sav&quot;) %&gt;% dplyr::rename_all(tolower) %&gt;% dplyr::mutate(gender = factor(gender, levels = c(1, 2), labels = c(&quot;Female&quot;, &quot;Male&quot;))) %&gt;% dplyr::mutate(major = factor(major, levels = c(1, 2, 3, 4,5), labels = c(&quot;Psychology&quot;, &quot;Premed&quot;, &quot;Biology&quot;, &quot;Sociology&quot;, &quot;Economics&quot;))) %&gt;% dplyr::mutate(reason = factor(reason, levels = c(1, 2, 3), labels = c(&quot;Program requirement&quot;, &quot;Personal interest&quot;, &quot;Advisor recommendation&quot;))) %&gt;% dplyr::mutate(exp_cond = factor(exp_cond, levels = c(1, 2, 3, 4), labels = c(&quot;Easy&quot;, &quot;Moderate&quot;, &quot;Difficult&quot;, &quot;Impossible&quot;))) %&gt;% dplyr::mutate(coffee = factor(coffee, levels = c(0, 1), labels = c(&quot;Not a regular coffee drinker&quot;, &quot;Regularly drinks coffee&quot;))) 3.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 3.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% data.frame() %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max phobia 100 3.310 2.444 0 1 4 10 mathquiz 85 29.071 9.480 9.000 22.000 35.000 49.000 statquiz 100 6.860 1.700 1 6 8 10 3.2.2 Bivariate Relationships The furniture package’s table1() function is a clean way to create a descriptive table that compares distinct subgroups of your sample (Barrett, Brignone, and Laxman 2018). Although categorizing continuous variables results in a loss of information (possible signal or noise), it is often done to investigate relationships in an exploratory way. data_ihno %&gt;% dplyr::mutate(phobia_cut3 = cut(phobia, breaks = c(0, 2, 4, 10), include.lowest = TRUE)) %&gt;% furniture::table1(mathquiz, statquiz, splitby = ~ phobia_cut3, test = TRUE, output = &quot;html&quot;) [0,2] (2,4] (4,10] P-Value n = 35 n = 31 n = 19 mathquiz 0.014 32.6 (8.5) 26.5 (9.8) 26.8 (8.9) statquiz 0.008 7.5 (1.4) 6.6 (1.7) 6.1 (2.0) One of the quickest ways to get a feel for all the pairwise relationships in your dataset (provided there aren’t too many variables) is with the pairs.panels() function in the psych package (Revelle 2018). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% data.frame() %&gt;% psych::pairs.panels(lm = TRUE, ci = TRUE, stars = TRUE) When two variables are both continuous, correlations (Pearson’s \\(R\\)) are an important measure of association. Notice the discrepincy between the correlation between statquiz and phobia. Above, the psych::pairs.panels() function uses pairwise complete cases by default, so \\(r=-.39\\) is computed on all \\(n=100\\) subjects. Below, we specified use = &quot;complete.obs&quot; in the cor() fucntion, so all correlations will be based on the same \\(n=85\\) students, making it listwise complete. The choice of which method to you will vary by situation. Often it is easier to digest a correlation matrix if it is visually presented, instead of just given as a table of many numbers. The corrplot package has a useful function called corrplot.mixed() for doing just that (Wei and Simko 2017). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% cor(use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot.mixed(lower = &quot;ellipse&quot;, upper = &quot;number&quot;, tl.col = &quot;black&quot;) 3.3 Regression Analysis 3.3.1 Subset the Sample All regression models can only be fit to complete observations regarding the variables included in the model (dependent and independent). Removing any case that is incomplete with respect to even one variables is called “list-wise deletion”. In this analysis, models including the mathquiz variable will be fit on only 85 students (sincle 15 students did not take the math quiz), where as models not including this variable will be fit to all 100 studnets. This complicates model comparisons, which require nested models be fit to the same data (exactly). For this reason, the dataset has been reduced to the subset of students that are complete regarding the three variables utilized throughout the set of five nested models. data_ihno_fitting &lt;- data_ihno %&gt;% dplyr::filter(complete.cases(mathquiz, statquiz, phobia)) dim(data_ihno_fitting) [1] 85 18 3.3.2 Fit Nested Models The bottom-up approach consists of starting with an initial NULL model with only an intercept term and them building additional models that are nested. Two models are considered nested if one is conains a subset of the terms (predictors or IV) compared to the other. fit_ihno_lm_0 &lt;- lm(statquiz ~ 1, # null model: intercept only data = data_ihno_fitting) fit_ihno_lm_1 &lt;- lm(statquiz ~ mathquiz, # only main effect of mathquiz data = data_ihno_fitting) fit_ihno_lm_2 &lt;- lm(statquiz ~ phobia, # only mian effect of phobia data = data_ihno_fitting) fit_ihno_lm_3 &lt;- lm(statquiz ~ mathquiz + phobia, # both main effects data = data_ihno_fitting) fit_ihno_lm_4 &lt;- lm(statquiz ~ mathquiz*phobia, # additional interaction data = data_ihno_fitting) 3.3.3 Comparing Nested Models 3.3.3.1 Model Comparison Table In single level, multiple linear regression significance of predictors (independent variables, IV) is usually based on both the Wald tests of significance for each beta estimate (shown with stars here) and comparisons in the model fit via the \\(R^2\\) values. Again the texreg package comes in handy to display several models in the same tal e (Leifeld 2017). texreg::htmlreg(list(fit_ihno_lm_0, fit_ihno_lm_1, fit_ihno_lm_2, fit_ihno_lm_3, fit_ihno_lm_4), custom.model.names = c(&quot;No Predictors&quot;, &quot;Only Math Quiz&quot;, &quot;Only Phobia&quot;, &quot;Both IVs&quot;, &quot;Add Interaction&quot;)) Statistical models No Predictors Only Math Quiz Only Phobia Both IVs Add Interaction (Intercept) 6.85*** 4.14*** 7.65*** 5.02*** 5.60*** (0.19) (0.53) (0.29) (0.63) (0.91) mathquiz 0.09*** 0.08*** 0.06* (0.02) (0.02) (0.03) phobia -0.25*** -0.16* -0.34 (0.07) (0.07) (0.21) mathquiz:phobia 0.01 (0.01) R2 0.00 0.26 0.13 0.31 0.31 Adj. R2 0.00 0.25 0.12 0.29 0.29 Num. obs. 85 85 85 85 85 RMSE 1.74 1.50 1.63 1.46 1.46 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.3.3.2 Likelihood Ratio Test of Nested Models An alternative method for determing model fit and variable importance is the likelihood ratio test. This involves comparing the \\(-2LL\\) or inverse of twice the log of the likelihood value for the model. The difference in these values follows a Chi Squared distribution with degrees of freedom equal to the difference in the number of parameters estimated (number of betas). Test the main effect of math quiz: anova(fit_ihno_lm_0, fit_ihno_lm_1) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 84 253. NA NA NA NA 2 83 188. 1 65.3 28.8 0.000000700 Test the main effect of math phobia anova(fit_ihno_lm_0, fit_ihno_lm_2) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 84 253. NA NA NA NA 2 83 221. 1 32.3 12.1 0.000791 Test the main effect of math phobia, after controlling for math test anova(fit_ihno_lm_1, fit_ihno_lm_3) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 83 188. NA NA NA NA 2 82 175. 1 12.6 5.88 0.0175 Test the interaction between math test and math phobia (i.e. moderation) anova(fit_ihno_lm_3, fit_ihno_lm_4) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 82 175. NA NA NA NA 2 81 173. 1 1.69 0.789 0.377 3.3.4 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_ihno_lm_3, which = 1) plot(fit_ihno_lm_3, which = 2) The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2018). car::residualPlots(fit_ihno_lm_3) Test stat Pr(&gt;|Test stat|) mathquiz -1.7778 0.07918 . phobia 0.5004 0.61813 Tukey test -1.5749 0.11527 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 While the model tables give starts to denote significance, you may print the actual p-values with the summary() function applied to the model name. summary(fit_ihno_lm_3) Call: lm(formula = statquiz ~ mathquiz + phobia, data = data_ihno_fitting) Residuals: Min 1Q Median 3Q Max -4.3436 -0.8527 0.2805 0.9857 2.7370 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.01860 0.62791 7.993 7.23e-12 *** mathquiz 0.08097 0.01754 4.617 1.42e-05 *** phobia -0.16176 0.06670 -2.425 0.0175 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.462 on 82 degrees of freedom Multiple R-squared: 0.3076, Adjusted R-squared: 0.2907 F-statistic: 18.21 on 2 and 82 DF, p-value: 2.849e-07 summary(fit_ihno_lm_4) Call: lm(formula = statquiz ~ mathquiz * phobia, data = data_ihno_fitting) Residuals: Min 1Q Median 3Q Max -4.1634 -0.8433 0.2832 0.9685 2.9434 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.600183 0.907824 6.169 2.57e-08 *** mathquiz 0.061216 0.028334 2.161 0.0337 * phobia -0.339426 0.210907 -1.609 0.1114 mathquiz:phobia 0.006485 0.007303 0.888 0.3771 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.464 on 81 degrees of freedom Multiple R-squared: 0.3143, Adjusted R-squared: 0.2889 F-statistic: 12.37 on 3 and 81 DF, p-value: 9.637e-07 3.4 Conclusion 3.4.1 Tabulate the Final Model Summary Many journals prefer that regression tables include 95% confidence intervals, rater than standard errors for the beta estimates. The texreg package contains three version of the regression table function (Leifeld 2017). screenreg() Use when working on a project and viewing tables on your computer screen htmlreg() Use when knitting your .Rmd file to a .html document texreg() Use when knitting your .Rmd file to a .pdf via LaTeX texreg::htmlreg(fit_ihno_lm_3, custom.model.names = &quot;Main Effects Model&quot;, ci.force = TRUE, # request 95% conf interv caption = &quot;Final Model for Stat&#39;s Quiz&quot;, single.row = TRUE) Final Model for Stat’s Quiz Main Effects Model (Intercept) 5.02 [3.79; 6.25]* mathquiz 0.08 [0.05; 0.12]* phobia -0.16 [-0.29; -0.03]* R2 0.31 Adj. R2 0.29 Num. obs. 85 RMSE 1.46 * 0 outside the confidence interval 3.4.2 Plot the Model When a model only contains main effects, a plot is not important for interpretation, but can help understand the relationship between multiple predictors. The Effect() function from the effects package chooses ‘5 or 6 nice values’ for each of your continuous independent variable (\\(X&#39;s\\)) based on the range of values found in the dataset on which the model and plugs all possible combinations of them into the regression equation \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 \\dots \\beta_k X_k\\) to compute the predicted mean value of the outcome (\\(Y\\)) (Fox et al. 2018). When plotting a regression model the outcome (dependent variable) is always on the y-axis (fit) and only one predictor (independent variable) may be used on the x-axis. You may incorporate additional predictor using colors, shapes, linetypes, or facets. For these predictors, you will want to specify only 2-4 values for illustration and then declare them as factors prior to plotting. effects::Effect(focal.predictors = c(&quot;mathquiz&quot;, &quot;phobia&quot;), mod = fit_ihno_lm_3, xlevels = list(phobia = c(0, 5, 10))) %&gt;% # values for illustration data.frame %&gt;% dplyr::mutate(phobia = factor(phobia)) %&gt;% # factor for illustration ggplot() + aes(x = mathquiz, y = fit, fill = phobia, color = phobia) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se), alpha = .3) + geom_point() + geom_line() + theme_bw() + labs(x = &quot;Score on Math Quiz&quot;, y = &quot;Estimated Marginal Mean\\nScore on Stat Quiz&quot;, fill = &quot;Self Rated\\nMath Phobia&quot;, color = &quot;Self Rated\\nMath Phobia&quot;) + theme(legend.background = element_rect(color = &quot;black&quot;), legend.position = c(0, 1), legend.justification = c(0, 1)) 3.5 Write-up There is evidence both mathquiz and phobia are associated with statquiz and that the relationship is addative (i.e. no interaction). There is a strong association between math and stats quiz scores, \\(r = .51\\). Math phobia is associated with lower math, \\(r = -.28\\), and stats quiz scores, \\(r = -.36\\). When considered togehter, the combined effects of math phobia and math score account for 31% of the variance in statistical achievement. Not surprizingly, while higher self-reported math phobia was associated with lower statists scores, \\(b = -0.162\\), \\(p=.018\\), \\(95CI = [-0.29, -0.03]\\), higher math quiz scores were associated with higher stats score, \\(b = -0.081\\), \\(p&lt;.001\\), \\(95CI = [0.05, 0.12]\\). There was no evidence that math phobia moderated the relationship between math and quiz performance, \\(p=.377\\). "],
["logistic-regression-ex-bronchopulmonary-dysplasia-in-premature-infants.html", "4 Logistic Regression - Ex: Bronchopulmonary Dysplasia in Premature Infants 4.1 Background 4.2 Logistic Regresion", " 4 Logistic Regression - Ex: Bronchopulmonary Dysplasia in Premature Infants example walk through: https://stats.idre.ucla.edu/r/dae/logit-regression/ info: https://onlinecourses.science.psu.edu/stat504/node/216/ sjPlot::tab_model (HTML only) http://www.strengejacke.de/sjPlot/articles/sjtlm.html#changing-summary-style-and-content finafit https://www.r-bloggers.com/elegant-regression-results-tables-and-plots-in-r-the-finalfit-package/ library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(pscl) # psudo R-squared function 4.1 Background Simple example demonstrating basic modeling approach: Data on Bronchopulmonary Dysplasia (BPD) from 223 low birth weight infants (weighing less than 1750 grams). 4.1.1 Source Data courtesy of Dr. Linda Van Marter. 4.1.2 Reference Van Marter, L.J., Leviton, A., Kuban, K.C.K., Pagano, M. &amp; Allred, E.N. (1990). Maternal glucocorticoid therapy and reduced risk of bronchopulmonary dysplasia. Pediatrics, 86, 331-336. The data are from a study of low birth weight infants in a neonatal intensive care unit. The study was designed to examine the development of bronchopulmonary dysplasia (BPD), a chronic lung disease, in a sample of 223 infants weighing less than 1750 grams. The response variable is binary, denoting whether an infant develops BPD by day 28 of life (where BPD is defined by both oxygen requirement and compatible chest radiograph). 4.1.3 Variables bpd(0 [N],1 [Y]) brthwght (grams) gestage (weeks) toxemia (0 [N] ,1 [Y]) in mother bpd_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/eBook_regression/master/data/VanMarter_%20BPD.txt?token=AScXBcwRurGPiBMhNmlD3RyY9VU1Bh7lks5bz50qwA%3D%3D&quot;, header = TRUE, strip.white = TRUE) n &lt;- nrow(bpd_raw) n [1] 223 str(bpd_raw) &#39;data.frame&#39;: 223 obs. of 4 variables: $ bpd : int 1 0 1 0 0 0 1 0 1 1 ... $ brthwght: int 850 1500 1360 960 1560 1120 810 1620 1000 700 ... $ gestage : int 27 33 32 35 33 29 28 32 30 26 ... $ toxemia : int 0 0 0 1 0 0 0 0 0 0 ... head(bpd_raw) # A tibble: 6 x 4 bpd brthwght gestage toxemia * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 850 27 0 2 0 1500 33 0 3 1 1360 32 0 4 0 960 35 1 5 0 1560 33 0 6 0 1120 29 0 bpd_clean &lt;- bpd_raw %&gt;% dplyr::mutate(toxemia = factor(toxemia, levels = c(0, 1), labels = c(&quot;No&quot;, &quot;Yes&quot;))) summary(bpd_clean) bpd brthwght gestage toxemia Min. :0.0000 Min. : 450 Min. :25.00 No :194 1st Qu.:0.0000 1st Qu.: 895 1st Qu.:28.00 Yes: 29 Median :0.0000 Median :1140 Median :30.00 Mean :0.3408 Mean :1173 Mean :30.09 3rd Qu.:1.0000 3rd Qu.:1465 3rd Qu.:32.00 Max. :1.0000 Max. :1730 Max. :37.00 4.2 Logistic Regresion 4.2.1 Fit the Models fit_glm_0 &lt;- glm(bpd ~ 1, data = bpd_clean, family = binomial(link = &quot;logit&quot;)) fit_glm_1 &lt;- glm(bpd ~ I(brthwght/100) + gestage + toxemia, data = bpd_clean, family = binomial(link = &quot;logit&quot;)) 4.2.1.1 Log Likelihood logLik(fit_glm_0) &#39;log Lik.&#39; -143.07 (df=1) logLik(fit_glm_1) &#39;log Lik.&#39; -101.8538 (df=4) 4.2.1.2 Deviance deviance(fit_glm_0) [1] 286.14 deviance(fit_glm_1) [1] 203.7075 4.2.2 GoF Measures 4.2.2.1 AIC AIC(fit_glm_0) [1] 288.14 AIC(fit_glm_1) [1] 211.7075 4.2.2.2 Logistic R^2 http://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/ Technically, \\(R^2\\) cannot be computed the same way in logistic regression as it is in OLS regression. The \\(pseudo-R^2\\), in logistic regression, is defined as \\(1−\\frac{L_1}{L_0}\\), where \\(L_0\\) represents the log likelihood for the “constant-only” or NULL model and \\(L_1\\) is the log likelihood for the full model with constant and predictors. 4.2.2.3 McFadden’s pseud- R^2 \\[ R^2_{McF} = 1 - \\frac{L_1}{L_0} \\] MFR2 &lt;- 1 - (logLik(fit_glm_1)/logLik(fit_glm_0)) MFR2 &#39;log Lik.&#39; 0.2880843 (df=4) 4.2.2.4 Cox &amp; Snell \\(l = e^{L}\\), sinc \\(L\\) is the log of the likelihood and \\(l\\) is the likelihood…\\(log(l) = L\\) \\[ R^2_{CS} = 1 - \\Bigg( \\frac{l_0}{l_1} \\Bigg) ^{2 \\backslash n} \\\\ n = \\text{sample size} \\] CSR2 &lt;- 1 - (exp(logLik(fit_glm_0))/exp(logLik(fit_glm_1)))^(2/n) CSR2 &#39;log Lik.&#39; 0.3090253 (df=1) 4.2.2.5 Nagelkerke or Cragg and Uhler’s \\[ R^2_{Nag} = \\frac{1 - \\Bigg( \\frac{l_0}{l_1} \\Bigg) ^{2 \\backslash n}} {1 - \\Big( l_0 \\Big) ^{2 \\backslash n}} \\] NR2 &lt;- CSR2 / (1 - exp(logLik(fit_glm_0))^(2/n)) NR2 &#39;log Lik.&#39; 0.4275191 (df=1) 4.2.2.6 Several with the pscl::pR2() function pscl::pR2(fit_glm_1) llh llhNull G2 McFadden r2ML -101.8537711 -143.0699809 82.4324196 0.2880843 0.3090253 r2CU 0.4275191 4.2.3 Parameter Estimates 4.2.3.1 Logit Scale fit_glm_1 %&gt;% coef() (Intercept) I(brthwght/100) gestage toxemiaYes 13.9360826 -0.2643578 -0.3885357 -1.3437865 4.2.3.2 Odds Ratio Scale fit_glm_1 %&gt;% coef() %&gt;% exp() (Intercept) I(brthwght/100) gestage toxemiaYes 1.128142e+06 7.676988e-01 6.780490e-01 2.608561e-01 4.2.3.3 Confidence Intervals - OR sclae fit_glm_1 %&gt;% confint() %&gt;% exp() 2.5 % 97.5 % (Intercept) 4.402379e+03 5.591330e+08 I(brthwght/100) 6.511832e-01 8.967757e-01 gestage 5.351280e-01 8.414808e-01 toxemiaYes 7.314875e-02 8.078916e-01 4.2.4 Significance of Terms 4.2.4.1 Likelihood Ratio Test of all Nested Models anova(fit_glm_0, fit_glm_1) # A tibble: 2 x 4 `Resid. Df` `Resid. Dev` Df Deviance * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 222 286. NA NA 2 219 204. 3 82.4 4.2.4.2 Sequential LRTs: for adding one variable at a time anova(fit_glm_1, test = &quot;Chisq&quot;) # A tibble: 4 x 5 Df Deviance `Resid. Df` `Resid. Dev` `Pr(&gt;Chi)` * &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 NA NA 222 286. NA 2 1 62.4 221 224. 2.78e-15 3 1 14.5 220 209. 1.41e- 4 4 1 5.52 219 204. 1.88e- 2 4.2.5 Parameter Estimates 4.2.5.1 Raw Output summary(fit_glm_1) Call: glm(formula = bpd ~ I(brthwght/100) + gestage + toxemia, family = binomial(link = &quot;logit&quot;), data = bpd_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.8400 -0.7029 -0.3352 0.7261 2.9902 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 13.93608 2.98255 4.673 2.98e-06 *** I(brthwght/100) -0.26436 0.08123 -3.254 0.00114 ** gestage -0.38854 0.11489 -3.382 0.00072 *** toxemiaYes -1.34379 0.60750 -2.212 0.02697 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 286.14 on 222 degrees of freedom Residual deviance: 203.71 on 219 degrees of freedom AIC: 211.71 Number of Fisher Scoring iterations: 5 4.2.5.2 sjPlot - HTML tables JUST HTML for now… Parameters Exponentiated: sjPlot::tab_model(fit_glm_1) bpd Predictors Odds Ratios CI p (Intercept) 1128141.99 4402.38 – 559132968.51 &lt;0.001 I(brthwght/100) 0.77 0.65 – 0.90 0.001 gestage 0.68 0.54 – 0.84 0.001 toxemiaYes 0.26 0.07 – 0.81 0.027 Observations 223 Cox &amp; Snell’s R2 / Nagelkerke’s R2 0.309 / 0.428 sjPlot::tab_model(fit_glm_1, emph.p = TRUE, pred.labels = c(&quot;(Intercept)&quot;, &quot;Birthweight, 100 grams&quot;, &quot;Gestational Age, week&quot;, &quot;Mother had Toxemia&quot;)) bpd Predictors Odds Ratios CI p (Intercept) 1128141.99 4402.38 – 559132968.51 &lt;0.001 Birthweight, 100 grams 0.77 0.65 – 0.90 0.001 Gestational Age, week 0.68 0.54 – 0.84 0.001 Mother had Toxemia 0.26 0.07 – 0.81 0.027 Observations 223 Cox &amp; Snell’s R2 / Nagelkerke’s R2 0.309 / 0.428 4.2.5.3 texreg default texreg::screenreg(fit_glm_1) ============================ Model 1 ---------------------------- (Intercept) 13.94 *** (2.98) I(brthwght/100) -0.26 ** (0.08) gestage -0.39 *** (0.11) toxemiaYes -1.34 * (0.61) ---------------------------- AIC 211.71 BIC 225.34 Log Likelihood -101.85 Deviance 203.71 Num. obs. 223 ============================ *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 4.2.5.4 texreg Confidence Intervals on Logit Scale texreg::screenreg(fit_glm_1, ci.force = TRUE) =============================== Model 1 ------------------------------- (Intercept) 13.94 * [ 8.09; 19.78] I(brthwght/100) -0.26 * [-0.42; -0.11] gestage -0.39 * [-0.61; -0.16] toxemiaYes -1.34 * [-2.53; -0.15] ------------------------------- AIC 211.71 BIC 225.34 Log Likelihood -101.85 Deviance 203.71 Num. obs. 223 =============================== * 0 outside the confidence interval 4.2.5.5 texreg exponentiate the betas (SE are not exp) texreg::screenreg(fit_glm_1, override.coef = list(fit_glm_1 %&gt;% coef() %&gt;% exp())) =============================== Model 1 ------------------------------- (Intercept) 1128141.99 *** (2.98) I(brthwght/100) 0.77 ** (0.08) gestage 0.68 *** (0.11) toxemiaYes 0.26 * (0.61) ------------------------------- AIC 211.71 BIC 225.34 Log Likelihood -101.85 Deviance 203.71 Num. obs. 223 =============================== *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 4.2.6 Marginal Model Plot 4.2.6.1 Manually Specified summary(bpd_clean) bpd brthwght gestage toxemia Min. :0.0000 Min. : 450 Min. :25.00 No :194 1st Qu.:0.0000 1st Qu.: 895 1st Qu.:28.00 Yes: 29 Median :0.0000 Median :1140 Median :30.00 Mean :0.3408 Mean :1173 Mean :30.09 3rd Qu.:1.0000 3rd Qu.:1465 3rd Qu.:32.00 Max. :1.0000 Max. :1730 Max. :37.00 effects::Effect(focal.predictors = c(&quot;brthwght&quot;, &quot;toxemia&quot;, &quot;gestage&quot;), mod = fit_glm_1, xlevels = list(brthwght = seq(from = 450, to = 1730, by = 10), gestage = c(28, 30, 32))) %&gt;% data.frame() %&gt;% dplyr::mutate(gestage = factor(gestage)) %&gt;% ggplot(aes(x = brthwght, y = fit)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = toxemia), alpha = .2) + geom_line(aes(linetype = toxemia, color = toxemia), size = 1) + facet_grid(. ~ gestage, labeller = label_both) + theme_bw() effects::Effect(focal.predictors = c(&quot;brthwght&quot;, &quot;toxemia&quot;, &quot;gestage&quot;), mod = fit_glm_1, xlevels = list(brthwght = seq(from = 450, to = 1730, by = 10), gestage = c(28, 30, 32))) %&gt;% data.frame() %&gt;% dplyr::mutate(gestage = factor(gestage)) %&gt;% ggplot(aes(x = brthwght, y = fit)) + geom_line(aes(linetype = toxemia, color = toxemia), size = 1) + facet_grid(. ~ gestage, labeller = label_both) + theme_bw() 4.2.7 Residual Diagnostics 4.2.7.1 sjPlot sjPlot::plot_model(fit_glm_1, type = &quot;diag&quot;) NULL 4.2.7.2 base R graphics plot(fit_glm_1) "],
["logistic-regression-ex-maternal-risk-factor-for-low-birth-weight-delivery.html", "5 Logistic Regression - Ex: Maternal Risk Factor for Low Birth Weight Delivery 5.1 Background 5.2 Exploratory Data Analysis 5.3 Logistic Regression - Simple, un-adjusted 5.4 Logistic Regression - Multivariate, with Main Effects Only 5.5 Logistic Regression - Multivariate, with Interactions 5.6 Logistic Regression - Multivariate, Simplify 5.7 Logistic Regression - Multivariate, Final Model", " 5 Logistic Regression - Ex: Maternal Risk Factor for Low Birth Weight Delivery library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(pscl) # psudo R-squared function library(glue) # Interpreted String Literals 5.1 Background More complex example demonstrating modeling decisions Another set of data from a study investigating predictors of low birth weight id infant’s unique identification number Dependent variable or outcome low Low birth weight (outcome) (0 = birth weight &gt;2500 g (normal), 1 = birth weight &lt; 2500 g (low)) bwt actual infant birth weight in grams Indepdentend variables or predictors age Age of mother, in years lwt Mother’s weight at last menstrual period, in pounds race Race: 1 = White, 2 = Black, 3 = Other smoke Smoking status during pregnancy:1 = Yes, 0 = No ptl History of premature labor: 0 = None, 1 = One, 2 = two, 3 = three ht History of hypertension: 1 = Yes, 0 = No ui Uterine irritability: 1 = Yes, 0 = No ftv Number of physician visits in 1st trimester: 0 = None, 1 = One, … 6 = six lowbwt_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/eBook_regression/master/data/lowbwt.txt?token=AScXBTYDbui3sy0ah-7-yknbzAyAwsUoks5bz51LwA%3D%3D&quot;, header = TRUE, sep = &quot;&quot;, na.strings = &quot;NA&quot;, dec = &quot;.&quot;, strip.white = TRUE) tibble::glimpse(lowbwt_raw) Observations: 189 Variables: 11 $ id &lt;int&gt; 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, ... $ low &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... $ age &lt;int&gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, ... $ lwt &lt;int&gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 15... $ race &lt;int&gt; 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3,... $ smoke &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,... $ ptl &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,... $ ht &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,... $ ui &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,... $ ftv &lt;int&gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0,... $ bwt &lt;int&gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 26... 5.1.1 Declare Factors lowbwt_clean &lt;- lowbwt_raw %&gt;% dplyr::mutate(id = factor(id)) %&gt;% dplyr::mutate(low = factor(low, levels = c(0, 1), labels = c(&quot;birth weight &gt;2500 g (normal)&quot;, &quot;birth weight &lt; 2500 g (low)&quot;))) %&gt;% dplyr::mutate(race = factor(race, levels = 1:3, labels = c(&quot;White&quot;, &quot;Black&quot;, &quot;Other&quot;))) %&gt;% dplyr::mutate(ptl_any = as.numeric(ptl &gt; 0)) %&gt;% # colapse into 0 = none vs. 1 = at least one dplyr::mutate(ptl = factor(ptl)) %&gt;% # declare the number of pre-term labors to be a factor: 0, 1, 2, 3 dplyr::mutate_at(vars(smoke, ht, ui, ptl_any), # declare all there variables to be factors with the same two levels factor, levels = 0:1, labels = c(&quot;No&quot;, &quot;Yes&quot;)) 5.1.1.1 Display the structure of the ‘clean’ version of the dataset str(lowbwt_clean) &#39;data.frame&#39;: 189 obs. of 12 variables: $ id : Factor w/ 189 levels &quot;4&quot;,&quot;10&quot;,&quot;11&quot;,..: 60 61 62 63 64 65 66 67 68 69 ... $ low : Factor w/ 2 levels &quot;birth weight &gt;2500 g (normal)&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ age : int 19 33 20 21 18 21 22 17 29 26 ... $ lwt : int 182 155 105 108 107 124 118 103 123 113 ... $ race : Factor w/ 3 levels &quot;White&quot;,&quot;Black&quot;,..: 2 3 1 1 1 3 1 3 1 1 ... $ smoke : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 2 2 2 1 1 1 2 2 ... $ ptl : Factor w/ 4 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ ht : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ ui : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 2 2 1 1 1 1 1 ... $ ftv : int 0 3 1 2 0 0 1 1 1 0 ... $ bwt : int 2523 2551 2557 2594 2600 2622 2637 2637 2663 2665 ... $ ptl_any: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... 5.1.1.2 Display the first few rows of the dataset head(lowbwt_clean) # A tibble: 6 x 12 id low age lwt race smoke ptl ht ui ftv bwt ptl_any * &lt;fct&gt; &lt;fc&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; 1 85 bir~ 19 182 Black No 0 No Yes 0 2523 No 2 86 bir~ 33 155 Other No 0 No No 3 2551 No 3 87 bir~ 20 105 White Yes 0 No No 1 2557 No 4 88 bir~ 21 108 White Yes 0 No Yes 2 2594 No 5 89 bir~ 18 107 White Yes 0 No Yes 0 2600 No 6 91 bir~ 21 124 Other No 0 No No 0 2622 No 5.2 Exploratory Data Analysis lowbwt_clean %&gt;% furniture::table1(&quot;Age, years&quot; = age, &quot;Weight, pounds&quot; = lwt, &quot;Race&quot; = race, &quot;Smoking During pregnancy&quot; = smoke, &quot;History of Premature Labor, any&quot; = ptl_any, &quot;History of Premature Labor, number&quot; = ptl, &quot;History of Hypertension&quot; = ht, &quot;Uterince Irritability&quot; = ui, &quot;1st Tri Dr Visits&quot; = ftv, splitby = ~ low, test = TRUE, output = &quot;html&quot;) birth weight &gt;2500 g (normal) birth weight &lt; 2500 g (low) P-Value n = 130 n = 59 Age, years 0.078 23.7 (5.6) 22.3 (4.5) Weight, pounds 0.013 133.3 (31.7) 122.1 (26.6) Race 0.082 White 73 (56.2%) 23 (39%) Black 15 (11.5%) 11 (18.6%) Other 42 (32.3%) 25 (42.4%) Smoking During pregnancy 0.04 No 86 (66.2%) 29 (49.2%) Yes 44 (33.8%) 30 (50.8%) History of Premature Labor, any &lt;.001 No 118 (90.8%) 41 (69.5%) Yes 12 (9.2%) 18 (30.5%) History of Premature Labor, number &lt;.001 0 118 (90.8%) 41 (69.5%) 1 8 (6.2%) 16 (27.1%) 2 3 (2.3%) 2 (3.4%) 3 1 (0.8%) 0 (0%) History of Hypertension 0.076 No 125 (96.2%) 52 (88.1%) Yes 5 (3.8%) 7 (11.9%) Uterince Irritability 0.035 No 116 (89.2%) 45 (76.3%) Yes 14 (10.8%) 14 (23.7%) 1st Tri Dr Visits 0.385 0.8 (1.1) 0.7 (1.0) 5.3 Logistic Regression - Simple, un-adjusted low1.age &lt;- glm(low ~ age, family=binomial(logit), data=lowbwt_clean) low1.lwt &lt;- glm(low ~ lwt, family=binomial(logit), data=lowbwt_clean) low1.race &lt;- glm(low ~ race, family=binomial(logit), data=lowbwt_clean) low1.smoke &lt;- glm(low ~ smoke, family=binomial(logit), data=lowbwt_clean) low1.ptl &lt;- glm(low ~ ptl_any, family=binomial(logit), data=lowbwt_clean) low1.ht &lt;- glm(low ~ ht, family=binomial(logit), data=lowbwt_clean) low1.ui &lt;- glm(low ~ ui, family=binomial(logit), data=lowbwt_clean) low1.ftv &lt;- glm(low ~ ftv, family=binomial(logit), data=lowbwt_clean) Note: the parameter estimates here are for the LOGIT scale, not the odds ration (OR) or even the probability. # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(low1.age, low1.lwt, low1.race, low1.smoke), custom.model.names = c(&quot;Age&quot;, &quot;Weight&quot;, &quot;Race&quot;, &quot;Smoker&quot;), caption = &quot;Simple, Unadjusted Logistic Regression&quot;) Simple, Unadjusted Logistic Regression Age Weight Race Smoker (Intercept) 0.38 1.00 -1.15*** -1.09*** (0.73) (0.79) (0.24) (0.21) age -0.05 (0.03) lwt -0.01* (0.01) raceBlack 0.84 (0.46) raceOther 0.64 (0.35) smokeYes 0.70* (0.32) AIC 235.91 232.69 235.66 233.80 BIC 242.40 239.17 245.39 240.29 Log Likelihood -115.96 -114.35 -114.83 -114.90 Deviance 231.91 228.69 229.66 229.80 Num. obs. 189 189 189 189 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(low1.ptl, low1.ht, low1.ui, low1.ftv), custom.model.names = c(&quot;Pre-Labor&quot;, &quot;Hypertension&quot;, &quot;Uterine&quot;, &quot;Visits&quot;)) Statistical models Pre-Labor Hypertension Uterine Visits (Intercept) -1.06*** -0.88*** -0.95*** -0.69*** (0.18) (0.17) (0.18) (0.19) ptl_anyYes 1.46*** (0.41) htYes 1.21* (0.61) uiYes 0.95* (0.42) ftv -0.14 (0.16) AIC 225.90 234.65 233.60 237.90 BIC 232.38 241.13 240.08 244.38 Log Likelihood -110.95 -115.32 -114.80 -116.95 Deviance 221.90 230.65 229.60 233.90 Num. obs. 189 189 189 189 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 5.4 Logistic Regression - Multivariate, with Main Effects Only Main-effects multiple logistic regression model low1_1 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui, family = binomial(logit), data = lowbwt_clean) summary(low1_1) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6459 -0.7992 -0.5103 0.9388 2.2018 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.63691 1.23028 0.518 0.60467 age -0.03775 0.03781 -0.998 0.31808 lwt -0.01491 0.00704 -2.118 0.03419 * raceBlack 1.21274 0.53248 2.278 0.02275 * raceOther 0.80412 0.44843 1.793 0.07294 . smokeYes 0.84640 0.40806 2.074 0.03806 * ptl_anyYes 1.22175 0.46301 2.639 0.00832 ** htYes 1.83869 0.70324 2.615 0.00893 ** uiYes 0.71113 0.46311 1.536 0.12465 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 196.83 on 180 degrees of freedom AIC: 214.83 Number of Fisher Scoring iterations: 4 5.5 Logistic Regression - Multivariate, with Interactions Before removing non-significant main effects, test plausible interactions Try interactions between age and lwt, age and smoke, lwt and smoke, 1 at a time 5.5.1 Age and Weight low1_2 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:lwt, family = binomial(logit), data = lowbwt_clean) summary(low1_2) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:lwt, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6426 -0.8004 -0.5163 0.9400 2.1989 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.1396293 4.0443041 0.282 0.77811 age -0.0597273 0.1724339 -0.346 0.72906 lwt -0.0188377 0.0309225 -0.609 0.54240 raceBlack 1.2071359 0.5341716 2.260 0.02383 * raceOther 0.7977750 0.4505381 1.771 0.07661 . smokeYes 0.8433655 0.4083953 2.065 0.03892 * ptl_anyYes 1.2260463 0.4642795 2.641 0.00827 ** htYes 1.8389418 0.7034162 2.614 0.00894 ** uiYes 0.7142688 0.4642468 1.539 0.12391 age:lwt 0.0001718 0.0013146 0.131 0.89600 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 196.82 on 179 degrees of freedom AIC: 216.82 Number of Fisher Scoring iterations: 5 anova(low1_1, low1_2, test = &#39;Chi&#39;) # A tibble: 2 x 5 `Resid. Df` `Resid. Dev` Df Deviance `Pr(&gt;Chi)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 180 197. NA NA NA 2 179 197. 1 0.0170 0.896 Anova(low1_2, test = &#39;LR&#39;) #Type II Analysis of Deviance table # A tibble: 8 x 3 `LR Chisq` Df `Pr(&gt;Chisq)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1.02 1 0.313 2 5.00 1 0.0254 3 6.27 2 0.0436 4 4.38 1 0.0364 5 7.13 1 0.00758 6 7.18 1 0.00738 7 2.33 1 0.127 8 0.0170 1 0.896 Anova(low1_2, test = &#39;LR&#39;, type = &#39;III&#39;) #Type III Analysis of Deviance table # A tibble: 8 x 3 `LR Chisq` Df `Pr(&gt;Chisq)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.119 1 0.730 2 0.372 1 0.542 3 6.27 2 0.0436 4 4.38 1 0.0364 5 7.13 1 0.00758 6 7.18 1 0.00738 7 2.33 1 0.127 8 0.0170 1 0.896 5.5.2 Age and Smoking low1_3 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:smoke, family = binomial(logit), data = lowbwt_clean) summary(low1_3) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:smoke, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.7020 -0.8016 -0.4970 0.9055 2.2124 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.311029 1.483078 0.884 0.37670 age -0.068293 0.053865 -1.268 0.20485 lwt -0.014701 0.007001 -2.100 0.03575 * raceBlack 1.126482 0.543684 2.072 0.03827 * raceOther 0.768241 0.451199 1.703 0.08863 . smokeYes -0.601286 1.782234 -0.337 0.73583 ptl_anyYes 1.197295 0.460992 2.597 0.00940 ** htYes 1.860851 0.705311 2.638 0.00833 ** uiYes 0.783999 0.472451 1.659 0.09703 . age:smokeYes 0.063744 0.076634 0.832 0.40552 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 196.14 on 179 degrees of freedom AIC: 216.14 Number of Fisher Scoring iterations: 5 anova(low1_1, low1_3, test = &#39;Chi&#39;) # A tibble: 2 x 5 `Resid. Df` `Resid. Dev` Df Deviance `Pr(&gt;Chi)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 180 197. NA NA NA 2 179 196. 1 0.699 0.403 5.5.3 Weight and Smoking low1_4 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui + lwt:smoke, family = binomial(logit), data = lowbwt_clean) summary(low1_4) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui + lwt:smoke, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6816 -0.7874 -0.5251 0.8876 2.2365 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.74840 1.59439 1.097 0.27282 age -0.03768 0.03809 -0.989 0.32259 lwt -0.02362 0.01077 -2.193 0.02828 * raceBlack 1.23110 0.53358 2.307 0.02104 * raceOther 0.71646 0.45183 1.586 0.11281 smokeYes -1.13566 1.75557 -0.647 0.51770 ptl_anyYes 1.26472 0.46488 2.721 0.00652 ** htYes 1.74326 0.70738 2.464 0.01373 * uiYes 0.80121 0.47044 1.703 0.08855 . lwt:smokeYes 0.01555 0.01344 1.157 0.24740 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 195.44 on 179 degrees of freedom AIC: 215.44 Number of Fisher Scoring iterations: 5 anova(low1_1, low1_4, test = &#39;Chi&#39;) # A tibble: 2 x 5 `Resid. Df` `Resid. Dev` Df Deviance `Pr(&gt;Chi)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 180 197. NA NA NA 2 179 195. 1 1.39 0.238 5.6 Logistic Regression - Multivariate, Simplify No interactions are significant Remove non-significant main effects 5.6.1 Remove the least significant perdictor: ui low1_5 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht, family = binomial(logit), data = lowbwt_clean) summary(low1_5) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6533 -0.8202 -0.5299 0.9709 2.1982 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.924910 1.202693 0.769 0.44187 age -0.042784 0.037567 -1.139 0.25475 lwt -0.015436 0.007044 -2.191 0.02843 * raceBlack 1.168452 0.532577 2.194 0.02824 * raceOther 0.814620 0.442740 1.840 0.06578 . smokeYes 0.858332 0.404787 2.120 0.03397 * ptl_anyYes 1.333970 0.457573 2.915 0.00355 ** htYes 1.740511 0.703104 2.475 0.01331 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 199.15 on 181 degrees of freedom AIC: 215.15 Number of Fisher Scoring iterations: 4 5.7 Logistic Regression - Multivariate, Final Model age is theoretically a meaningful variable, should probably retain it Revise so that age is interpreted in 5-year and lwt in 20 lb increments and the intercept has meaning. low1_6 &lt;- glm(low ~ I((age - 20)/5) + I((lwt - 125)/20) + race + smoke + ptl_any + ht, family = binomial(logit), data = lowbwt_clean) summary(low1_6) Call: glm(formula = low ~ I((age - 20)/5) + I((lwt - 125)/20) + race + smoke + ptl_any + ht, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6533 -0.8202 -0.5299 0.9709 2.1982 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.8603 0.4092 -4.546 5.48e-06 *** I((age - 20)/5) -0.2139 0.1878 -1.139 0.25475 I((lwt - 125)/20) -0.3087 0.1409 -2.191 0.02843 * raceBlack 1.1685 0.5326 2.194 0.02824 * raceOther 0.8146 0.4427 1.840 0.06578 . smokeYes 0.8583 0.4048 2.120 0.03397 * ptl_anyYes 1.3340 0.4576 2.915 0.00355 ** htYes 1.7405 0.7031 2.475 0.01331 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 199.15 on 181 degrees of freedom AIC: 215.15 Number of Fisher Scoring iterations: 4 5.7.1 Several \\(R^2\\) measures with the pscl::pR2() function pscl::pR2(low1_6) llh llhNull G2 McFadden r2ML -99.5757045 -117.3359981 35.5205871 0.1513627 0.1713353 r2CU 0.2409463 5.7.2 Parameter Estiamtes Table 5.7.2.1 Using texreg::screenreg() Default: parameters are in terms of the ‘logit’ or log odds ratio # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(low1_6) Statistical models Model 1 (Intercept) -1.86*** (0.41) I((age - 20)/5) -0.21 (0.19) I((lwt - 125)/20) -0.31* (0.14) raceBlack 1.17* (0.53) raceOther 0.81 (0.44) smokeYes 0.86* (0.40) ptl_anyYes 1.33** (0.46) htYes 1.74* (0.70) AIC 215.15 BIC 241.09 Log Likelihood -99.58 Deviance 199.15 Num. obs. 189 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 The texreg package uses an intermediate function called extract() to extract information for the model and then put it in the right places in the table. We can invervene by writing our own extract_OR() function to use instead. extract_OR &lt;- function(fit_glm){ beta = coef(fit_glm) betaci = confint(fit_glm) fit_glm_OR = texreg::extract(fit_glm) fit_glm_OR@coef = exp(beta) fit_glm_OR@ci.low = exp(betaci[, 1]) fit_glm_OR@ci.up = exp(betaci[, 2]) return(fit_glm_OR) } # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(extract_OR(low1_6), custom.coef.names = c(&quot;BL: 120 lb, 20 yr old White Mother&quot;, &quot;Additional 5 years older&quot;, &quot;Additional 20 lbs pre-pregnancy&quot;, &quot;Race: Black vs. White&quot;, &quot;Race: Other vs. White&quot;, &quot;Smoking During pregnancy&quot;, &quot;History of Any Premature Labor&quot;, &quot;History of Hypertension&quot;), custom.model.names = &quot;OR, Low Birth Weight&quot;, single.row = TRUE, custom.note = &quot;* The value of &#39;1&#39; is outside the confidence interval for the OR&quot;) Statistical models OR, Low Birth Weight BL: 120 lb, 20 yr old White Mother 0.16 [0.07; 0.33]* Additional 5 years older 0.81 [0.55; 1.16]* Additional 20 lbs pre-pregnancy 0.73 [0.55; 0.95]* Race: Black vs. White 3.22 [1.13; 9.31]* Race: Other vs. White 2.26 [0.96; 5.50]* Smoking During pregnancy 2.36 [1.08; 5.32]* History of Any Premature Labor 3.80 [1.57; 9.53]* History of Hypertension 5.70 [1.49; 24.76]* AIC 215.15 BIC 241.09 Log Likelihood -99.58 Deviance 199.15 Num. obs. 189 * The value of ‘1’ is outside the confidence interval for the OR 5.7.2.2 Using sjPlot::tab_model() Parameters Exponentiated: sjPlot::tab_model(low1_6, emph.p = TRUE, pred.labels = c(&quot;BL: 120 lb, 20 yr old White Mother&quot;, &quot;Additional 5 years old&quot;, &quot;Additional 20 lbs pre-pregnancy&quot;, &quot;Race: Black vs. White&quot;, &quot;Race: Other vs. White&quot;, &quot;Smoking During pregnancy&quot;, &quot;History of Any Premature Labor&quot;, &quot;History of Hypertension&quot;)) low Predictors Odds Ratios CI p BL: 120 lb, 20 yr old White Mother 0.16 0.07 – 0.33 &lt;0.001 Additional 5 years old 0.81 0.55 – 1.16 0.255 Additional 20 lbs pre-pregnancy 0.73 0.55 – 0.95 0.028 Race: Black vs. White 3.22 1.13 – 9.31 0.028 Race: Other vs. White 2.26 0.96 – 5.50 0.066 Smoking During pregnancy 2.36 1.08 – 5.32 0.034 History of Any Premature Labor 3.80 1.57 – 9.53 0.004 History of Hypertension 5.70 1.49 – 24.76 0.013 Observations 189 Cox &amp; Snell’s R2 / Nagelkerke’s R2 0.171 / 0.241 5.7.3 Marginal Model Plot effects::Effect(focal.predictors = c(&quot;age&quot;, &quot;lwt&quot;, &quot;race&quot;), mod = low1_6, xlevels = list(age = c(20, 30, 40), lwt = seq(from = 80, to = 250, by = 5))) %&gt;% data.frame() %&gt;% dplyr::mutate(age_labels = glue(&quot;Mother Age: {age}&quot;)) %&gt;% ggplot(aes(x = lwt, y = fit)) + geom_line(aes(color = race, linetype = race), size = 1) + theme_bw() + facet_grid(.~ age_labels) + labs(title = &quot;Risk of Low Birth Weight&quot;, subtitle = &quot;Illustates risk given mother is a non-smoker, without a history of pre-term labor or hypertension&quot;, x = &quot;Mother&#39;s Weight Pre-Pregnancy, pounds&quot;, y = &quot;Predicted Probability\\nBaby has Low Birth Weight (&lt; 2500 grams)&quot;, color = &quot;Mother&#39;s Race&quot;, linetype = &quot;Mother&#39;s Race&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + scale_linetype_manual(values = c(&quot;longdash&quot;, &quot;dotted&quot;, &quot;solid&quot;)) + scale_color_manual(values = c( &quot;coral2&quot;, &quot;dodger blue&quot;, &quot;gray50&quot;)) effects::Effect(focal.predictors = c(&quot;lwt&quot;, &quot;smoke&quot;, &quot;ptl_any&quot;, &quot;ht&quot;), fixed.predictors = list(age = 20), mod = low1_6, xlevels = list(lwt = seq(from = 80, to = 250, by = 5))) %&gt;% data.frame() %&gt;% dplyr::mutate(smoke = forcats::fct_rev(smoke)) %&gt;% dplyr::mutate(ptl_any_labels = glue(&quot;History of Preterm Labor: {ptl_any}&quot;)) %&gt;% dplyr::mutate(ht_labels = glue(&quot;History of Hypertension: {ht}&quot;) %&gt;% forcats::fct_rev()) %&gt;% ggplot(aes(x = lwt, y = fit)) + geom_line(aes(color = smoke, linetype = smoke), size = 1) + theme_bw() + facet_grid(ht_labels ~ ptl_any_labels) + labs(title = &quot;Risk of Low Birth Weight&quot;, subtitle = &quot;Illustates risk given the mother is 20 years old and white&quot;, x = &quot;Mother&#39;s Weight Pre-Pregnancy, pounds&quot;, y = &quot;Predicted Probability\\nBaby has Low Birth Weight (&lt; 2500 grams)&quot;, color = &quot;Mother Smoked&quot;, linetype = &quot;Mother Smoked&quot;) + theme(legend.position = c(1, .5), legend.justification = c(1.1, 1.15), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + scale_linetype_manual(values = c(&quot;longdash&quot;, &quot;solid&quot;)) + scale_color_manual(values = c( &quot;coral2&quot;, &quot;dodger blue&quot;)) "],
["references.html", "References", " References "]
]
