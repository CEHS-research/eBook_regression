[
["index.html", "Encyclopedia of Quantitative Methods in R, vol. 4: Multiple Linear Regression Welcome Blocked Notes Code and Output The Authors", " Encyclopedia of Quantitative Methods in R, vol. 4: Multiple Linear Regression Sarah Schwartz &amp; Tyson Barrett Last updated: 2019-11-05 Welcome Backgroup and links to other volumes of this encyclopedia may be found at the Encyclopedia’s Home Website. Blocked Notes Thoughout all the eBooks in this encyclopedia, several small secitons will be blocked out in the following ways: These blocks denote an area UNDER CONSTRUCTION, so check back often. This massive undertaking started during the summer of 2018 and is far from complete. The outline of seven volumes is given above despite any one being complete. Feedback is welcome via either author’s email. These blocks denote something EXTREMELY IMPORTANT. Do NOT skip these notes as they will be used very sparingly. These blocks denote something to DOWNLOAD. This may include software installations, example datasets, or notebook code files. These blocks denote something INTERESTING. These point out information we found of interest or added value. These blocks denote LINKS to other websites. This may include instructional video clips, articles, or blog posts. We are all about NOT re-creating the wheel. If somebody else has described or illustrated a topic well, we celebrate it! Code and Output This is how \\(R\\) code is shown: 1 + 1 This is what the output of the \\(R\\) code above will look: ## [1] 2 The Authors Dr. Sarah Schwartz Dr. Tyson Barrett www.SarahSchwartzStats.com www.TysonBarrett.com Sarah.Schwartz@usu.edu Tyson.Barrett@usu.edu Statistical Consulting Studio Data Science and Discover Unit Why choose R ? Check it out: an article from Fall 2016… No more excuses: R is better than SPSS for psychology undergrads, and students agree FYI This entire encyclopedia is written in \\(R Markdown\\), using \\(R Studio\\) as the text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The book’s source code is hosted on GitHub. If you notice typos or other issues, feel free to email either of the authors. This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. "],
["linear-correlation-example-cancer-experiment.html", "1 Linear Correlation - Example: Cancer Experiment 1.1 Background 1.2 Exploratory Data Analysis: i.e. the eyeball method 1.3 Pearson’s Correlation Coefficient 1.4 Correlation Tables 1.5 Pairs Plots 1.6 Correlation Plots: Corrolagrams", " 1 Linear Correlation - Example: Cancer Experiment 1.1 Background 1.1.1 Required Packages library(tidyverse) # Loads several very helpful &#39;tidy&#39; packages library(haven) # Read in SPSS datasets library(psych) # Lots of nice tid-bits library(GGally) # Extension to &#39;ggplot2&#39; (ggpairs) 1.1.2 Example Dataset - Cancer Experiment The Cancer dataset: cancer_raw &lt;- haven::read_spss(&quot;https://raw.githubusercontent.com/CEHS-research/eBook_ANOVA/master/data/Cancer.sav&quot;) tibble::glimpse(cancer_raw) Observations: 25 Variables: 9 $ ID &lt;dbl&gt; 1, 5, 6, 9, 11, 15, 21, 26, 31, 35, 39, 41, 45, 2, 12... $ TRT &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,... $ AGE &lt;dbl&gt; 52, 77, 60, 61, 59, 69, 67, 56, 61, 51, 46, 65, 67, 4... $ WEIGHIN &lt;dbl&gt; 124.0, 160.0, 136.5, 179.6, 175.8, 167.6, 186.0, 158.... $ STAGE &lt;dbl&gt; 2, 1, 4, 1, 2, 1, 1, 3, 1, 1, 4, 1, 1, 2, 4, 1, 2, 1,... $ TOTALCIN &lt;dbl&gt; 6, 9, 7, 6, 6, 6, 6, 6, 6, 6, 7, 6, 8, 7, 6, 4, 6, 6,... $ TOTALCW2 &lt;dbl&gt; 6, 6, 9, 7, 7, 6, 11, 11, 9, 4, 8, 6, 8, 16, 10, 6, 1... $ TOTALCW4 &lt;dbl&gt; 6, 10, 17, 9, 16, 6, 11, 15, 6, 8, 11, 9, 9, 9, 11, 8... $ TOTALCW6 &lt;dbl&gt; 7, 9, 19, 3, 13, 11, 10, 15, 8, 7, 11, 6, 10, 10, 9, ... cancer_clean &lt;- cancer_raw %&gt;% dplyr::rename_all(tolower) %&gt;% dplyr::mutate(id = factor(id)) %&gt;% dplyr::mutate(trt = factor(trt, labels = c(&quot;Placebo&quot;, &quot;Aloe Juice&quot;))) %&gt;% dplyr::mutate(stage = factor(stage)) tibble::glimpse(cancer_clean) Observations: 25 Variables: 9 $ id &lt;fct&gt; 1, 5, 6, 9, 11, 15, 21, 26, 31, 35, 39, 41, 45, 2, 12... $ trt &lt;fct&gt; Placebo, Placebo, Placebo, Placebo, Placebo, Placebo,... $ age &lt;dbl&gt; 52, 77, 60, 61, 59, 69, 67, 56, 61, 51, 46, 65, 67, 4... $ weighin &lt;dbl&gt; 124.0, 160.0, 136.5, 179.6, 175.8, 167.6, 186.0, 158.... $ stage &lt;fct&gt; 2, 1, 4, 1, 2, 1, 1, 3, 1, 1, 4, 1, 1, 2, 4, 1, 2, 1,... $ totalcin &lt;dbl&gt; 6, 9, 7, 6, 6, 6, 6, 6, 6, 6, 7, 6, 8, 7, 6, 4, 6, 6,... $ totalcw2 &lt;dbl&gt; 6, 6, 9, 7, 7, 6, 11, 11, 9, 4, 8, 6, 8, 16, 10, 6, 1... $ totalcw4 &lt;dbl&gt; 6, 10, 17, 9, 16, 6, 11, 15, 6, 8, 11, 9, 9, 9, 11, 8... $ totalcw6 &lt;dbl&gt; 7, 9, 19, 3, 13, 11, 10, 15, 8, 7, 11, 6, 10, 10, 9, ... psych::headTail(cancer_clean) # A tibble: 9 x 9 id trt age weighin stage totalcin totalcw2 totalcw4 totalcw6 &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1 Placebo 52 124 2 6 6 6 7 2 5 Placebo 77 160 1 9 6 10 9 3 6 Placebo 60 136.5 4 7 9 17 19 4 9 Placebo 61 179.6 1 6 7 9 3 5 &lt;NA&gt; &lt;NA&gt; ... ... &lt;NA&gt; ... ... ... ... 6 42 Aloe Juice 73 181.5 0 8 11 16 &lt;NA&gt; 7 44 Aloe Juice 67 187 1 5 7 7 7 8 50 Aloe Juice 60 164 2 6 8 16 &lt;NA&gt; 9 58 Aloe Juice 54 172.8 4 7 8 10 8 1.2 Exploratory Data Analysis: i.e. the eyeball method 1.2.1 Scatterplot Always plot your data first! cancer_clean %&gt;% ggplot(aes(x = age, y = weighin)) + geom_count() + geom_smooth(method = &quot;lm&quot;) 1.3 Pearson’s Correlation Coefficient 1.3.1 Using the Default Settings The cor.test() function needs at least TWO arguments: formula - The formula specifies the two variabels between which you would like to calcuate the correlation. Note at the two variable names come AFTER the tilda symbol and are separated with a plus sign: ~ continuous_var1 + continuous_var2 data - Since the datset is not the first argument in the function, you must use the period to signify that the datset is being piped from above data = . cancer_clean %&gt;% cor.test(~ age + weighin, # formula: order doesn&#39;t matter data = .) # data piped from above Pearson&#39;s product-moment correlation data: age and weighin t = -1.4401, df = 23, p-value = 0.1633 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.6130537 0.1213316 sample estimates: cor -0.2875868 1.3.2 Additional Arguments alternative - The cor.test() function defaults to the alternative = \"two.sided\". If you would like a one-sided alternative, you must choose which side you would like to test: alternative = \"greater\" to test for POSITIVE correlation or alternative = \"less\" to test for NEGATIVE correlation. method - The default is to calculate the Pearson correlation coefficient (method = \"pearson\"), but you may also specify the Kendall’s tau (method = \"kendall\")or Spearman’s rho (method = \"spearman\"), which are both non-parametric methods. conf.int - It also defaults to testing for the two-sided alternative and computing a 95% confidence interval (conf.level = 0.95), but this may be changed. Since the following code only specifies thedefaults, it Will give the same results as if you did not type out the last three lines (see above). cancer_clean %&gt;% cor.test(~ age + weighin, data = ., alternative = &quot;two.sided&quot;, # or &quot;greater&quot; (positive r) or &quot;less&quot; (negative r) method = &quot;pearson&quot;, # or &quot;kendall&quot; (tau) or &quot;spearman&quot; (rho) conf.level = .95) # or .90 or .99 (ect) Pearson&#39;s product-moment correlation data: age and weighin t = -1.4401, df = 23, p-value = 0.1633 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.6130537 0.1213316 sample estimates: cor -0.2875868 Non-Significant Correlation APA Results: There was no evidence of an association in overall oral condition from baseline to two week follow-up, $r(25) = -0.288 \\(p &lt; .163\\). 1.3.3 Statistical Significance cancer_clean %&gt;% ggplot(aes(x = totalcin, y = totalcw4)) + geom_count() + geom_smooth(method = &quot;lm&quot;) cancer_clean %&gt;% cor.test(~ totalcin + totalcw4, data = .) Pearson&#39;s product-moment correlation data: totalcin and totalcw4 t = 1.0911, df = 23, p-value = 0.2865 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.1899343 0.5672525 sample estimates: cor 0.2218459 Statistically Significant Correlation APA Results: Overall oral condition was positively correlated (\\(r = .763\\)) between weeks two and four, \\(t(21) = 5.409\\), \\(p &lt; .001\\). cancer_clean %&gt;% ggplot(aes(x = totalcw4, y = totalcw6)) + geom_point() + geom_smooth(method = &quot;lm&quot;) cancer_clean %&gt;% cor.test(~ totalcw4 + totalcw6, data = .) Pearson&#39;s product-moment correlation data: totalcw4 and totalcw6 t = 5.4092, df = 21, p-value = 2.296e-05 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.5117459 0.8940223 sample estimates: cor 0.762999 1.4 Correlation Tables The may use the tableC() function from the furniture package to calculate all pair-wise correlations between more than two variables and arrange them all in a table. The table is formatted with the variabels listed on the rows and numbered to show the same variabels across the columns. The cells ON the diagonal are all equal to exactly one, since each variable is perfectly correlated with itself. The cells ABOVE the diagonal are blank as them would just be a mirror image of the values below the diagonal. The cells BELOW the diagonal each contain the Pearson’s correlation coefficients for each pair of variables, \\(r\\), with the \\(p-value\\) showing the significance vs. the null hypothesis for no association (\\(r = 0\\)) to the right. cancer_clean %&gt;% furniture::tableC(age, weighin, totalcin) ----------------------------------------------- [1] [2] [3] [1]age 1.00 [2]weighin -0.288 (0.163) 1.00 [3]totalcin 0.256 (0.217) 0.17 (0.418) 1.00 ----------------------------------------------- 1.4.1 Missing Values - Default Default Behavior na.rm = FALSE (default) If you don’t say otherwise, the correlation \\(r\\) with not be calculated (NA) between any pair of variables for which there is at least one subject with a missing value on at least one of the vairables. This is a nice alert to make you aware of missing values. cancer_clean %&gt;% furniture::tableC(totalcin, totalcw2, totalcw4, totalcw6) ----------------------------------------------------- [1] [2] [3] [4] [1]totalcin 1.00 [2]totalcw2 0.314 (0.126) 1.00 [3]totalcw4 0.222 (0.287) 0.337 (0.099) 1.00 [4]totalcw6 NA NA NA NA NA NA 1.00 ----------------------------------------------------- 1.4.2 Missing Values - Listwise Deletion Listwise Deletion na.rm = TRUE Most of the time you will want to compute the correlation \\(r\\) is the precense of missing values. To do so, you want to remove or exclude subjects with missing data from ALL correlation computation in the table. This is called ‘list-wise deletion’. It ensures that all cells in the table refer to the exact same sub-sample (n = subjects with complete data for all variables in the table), and thus the same degrees of freedom (since \\(df = n - 2\\)). This is done be changing the default to na.rm = TRUE. cancer_clean %&gt;% furniture::tableC(totalcin, totalcw2, totalcw4, totalcw6, na.rm = TRUE) ------------------------------------------------------------- [1] [2] [3] [4] [1]totalcin 1.00 [2]totalcw2 0.282 (0.192) 1.00 [3]totalcw4 0.206 (0.346) 0.314 (0.145) 1.00 [4]totalcw6 0.098 (0.657) 0.378 (0.075) 0.763 (&lt;.001) 1.00 ------------------------------------------------------------- 1.5 Pairs Plots Helpful Website 1.5.1 Using Base R cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% pairs() 1.5.2 Using the psych package cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% psych::pairs.panels() 1.5.3 Using the ggplot2 and GGally packages cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% data.frame %&gt;% ggscatmat() cancer_clean %&gt;% data.frame %&gt;% ggscatmat(columns = c(&quot;age&quot;, &quot;weighin&quot;, &quot;totalcin&quot;, &quot;totalcw2&quot;, &quot;totalcw4&quot;, &quot;totalcw6&quot;), color = &quot;trt&quot;) 1.6 Correlation Plots: Corrolagrams 1.6.1 Simple Correlation Matrix (Base R) cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) age weighin totalcin totalcw2 totalcw4 age 1.00000000 -0.29909121 0.22386540 -0.1613892 0.09918029 weighin -0.29909121 1.00000000 0.16403694 0.2763478 -0.08013506 totalcin 0.22386540 0.16403694 1.00000000 0.2819648 0.20604650 totalcw2 -0.16138924 0.27634783 0.28196479 1.0000000 0.31354250 totalcw4 0.09918029 -0.08013506 0.20604650 0.3135425 1.00000000 totalcw6 0.03015273 -0.07750304 0.09786664 0.3780949 0.76299899 totalcw6 age 0.03015273 weighin -0.07750304 totalcin 0.09786664 totalcw2 0.37809488 totalcw4 0.76299899 totalcw6 1.00000000 1.6.2 Using the Default Settings cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot() 1.6.3 Changing the shape cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot(method = &quot;square&quot;) 1.6.4 Only Displaying Half cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot(method = &quot;ellipse&quot;, type = &quot;lower&quot;) 1.6.5 Mixing it up cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot.mixed() 1.6.6 Getting Fancy cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot.mixed(upper = &quot;number&quot;, lower = &quot;ellipse&quot;) "],
["simple-linear-regression-example-cancer-experiment.html", "2 Simple Linear Regression - Example: Cancer Experiment 2.1 Background 2.2 Exploratory Data Analysis: i.e. the eyeball method", " 2 Simple Linear Regression - Example: Cancer Experiment 2.1 Background 2.1.1 Required Packages library(tidyverse) # Loads several very helpful &#39;tidy&#39; packages library(haven) # Read in SPSS datasets library(psych) # Lots of nice tid-bits library(GGally) # Extension to &#39;ggplot2&#39; (ggpairs) 2.1.2 Example Dataset - Cancer Experiment The Cancer dataset: cancer_raw &lt;- haven::read_spss(&quot;https://raw.githubusercontent.com/CEHS-research/eBook_ANOVA/master/data/Cancer.sav&quot;) tibble::glimpse(cancer_raw) Observations: 25 Variables: 9 $ ID &lt;dbl&gt; 1, 5, 6, 9, 11, 15, 21, 26, 31, 35, 39, 41, 45, 2, 12... $ TRT &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,... $ AGE &lt;dbl&gt; 52, 77, 60, 61, 59, 69, 67, 56, 61, 51, 46, 65, 67, 4... $ WEIGHIN &lt;dbl&gt; 124.0, 160.0, 136.5, 179.6, 175.8, 167.6, 186.0, 158.... $ STAGE &lt;dbl&gt; 2, 1, 4, 1, 2, 1, 1, 3, 1, 1, 4, 1, 1, 2, 4, 1, 2, 1,... $ TOTALCIN &lt;dbl&gt; 6, 9, 7, 6, 6, 6, 6, 6, 6, 6, 7, 6, 8, 7, 6, 4, 6, 6,... $ TOTALCW2 &lt;dbl&gt; 6, 6, 9, 7, 7, 6, 11, 11, 9, 4, 8, 6, 8, 16, 10, 6, 1... $ TOTALCW4 &lt;dbl&gt; 6, 10, 17, 9, 16, 6, 11, 15, 6, 8, 11, 9, 9, 9, 11, 8... $ TOTALCW6 &lt;dbl&gt; 7, 9, 19, 3, 13, 11, 10, 15, 8, 7, 11, 6, 10, 10, 9, ... cancer_clean &lt;- cancer_raw %&gt;% dplyr::rename_all(tolower) %&gt;% dplyr::mutate(id = factor(id)) %&gt;% dplyr::mutate(trt = factor(trt, labels = c(&quot;Placebo&quot;, &quot;Aloe Juice&quot;))) %&gt;% dplyr::mutate(stage = factor(stage)) tibble::glimpse(cancer_clean) Observations: 25 Variables: 9 $ id &lt;fct&gt; 1, 5, 6, 9, 11, 15, 21, 26, 31, 35, 39, 41, 45, 2, 12... $ trt &lt;fct&gt; Placebo, Placebo, Placebo, Placebo, Placebo, Placebo,... $ age &lt;dbl&gt; 52, 77, 60, 61, 59, 69, 67, 56, 61, 51, 46, 65, 67, 4... $ weighin &lt;dbl&gt; 124.0, 160.0, 136.5, 179.6, 175.8, 167.6, 186.0, 158.... $ stage &lt;fct&gt; 2, 1, 4, 1, 2, 1, 1, 3, 1, 1, 4, 1, 1, 2, 4, 1, 2, 1,... $ totalcin &lt;dbl&gt; 6, 9, 7, 6, 6, 6, 6, 6, 6, 6, 7, 6, 8, 7, 6, 4, 6, 6,... $ totalcw2 &lt;dbl&gt; 6, 6, 9, 7, 7, 6, 11, 11, 9, 4, 8, 6, 8, 16, 10, 6, 1... $ totalcw4 &lt;dbl&gt; 6, 10, 17, 9, 16, 6, 11, 15, 6, 8, 11, 9, 9, 9, 11, 8... $ totalcw6 &lt;dbl&gt; 7, 9, 19, 3, 13, 11, 10, 15, 8, 7, 11, 6, 10, 10, 9, ... psych::headTail(cancer_clean) # A tibble: 9 x 9 id trt age weighin stage totalcin totalcw2 totalcw4 totalcw6 &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1 Placebo 52 124 2 6 6 6 7 2 5 Placebo 77 160 1 9 6 10 9 3 6 Placebo 60 136.5 4 7 9 17 19 4 9 Placebo 61 179.6 1 6 7 9 3 5 &lt;NA&gt; &lt;NA&gt; ... ... &lt;NA&gt; ... ... ... ... 6 42 Aloe Juice 73 181.5 0 8 11 16 &lt;NA&gt; 7 44 Aloe Juice 67 187 1 5 7 7 7 8 50 Aloe Juice 60 164 2 6 8 16 &lt;NA&gt; 9 58 Aloe Juice 54 172.8 4 7 8 10 8 2.2 Exploratory Data Analysis: i.e. the eyeball method 2.2.1 Scatterplot Always plot your data first! cancer_clean %&gt;% ggplot(aes(x = age, y = weighin)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE, color = &quot;blue&quot;) + # straight line (linear model) geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;red&quot;) # loess line (moving window) "],
["simple-linear-regression-ex-ventricular-shortening-velocity-single-continuous-iv.html", "3 Simple Linear Regression - Ex: Ventricular Shortening Velocity (single continuous IV) 3.1 Purpose 3.2 Exploratory Data Analysis 3.3 Regression Analysis 3.4 Conclusion", " 3 Simple Linear Regression - Ex: Ventricular Shortening Velocity (single continuous IV) library(tidyverse) # super helpful everything! library(magrittr) # includes other versions of the pipe library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools library(ISwR) # Introduction to Statistics with R (datasets) 3.1 Purpose 3.1.1 Research Question Is there a relationship between fasting blood flucose and shortening of ventricular velocity among type 1 diabetic patiences? If so, what is the nature of the association? 3.1.2 Data Description This dataset is included in the ISwR package (Dalgaard 2015), which was a companion to the texbook “Introductory Statistics with R, 2nd ed.” (Dalgaard 2008), although it was first published by Altman (1991) in table 11.6. The thuesen data frame has 24 rows and 2 columns. It contains ventricular shortening velocity and blood glucose for type 1 diabetic patients. blood.glucose a numeric vector, fasting blood glucose (mmol/l). short.velocity a numeric vector, mean circumferential shortening velocity (%/s). data(thuesen, package = &quot;ISwR&quot;) tibble::glimpse(thuesen) # view the class and 1st few values of each variable Observations: 24 Variables: 2 $ blood.glucose &lt;dbl&gt; 15.3, 10.8, 8.1, 19.5, 7.2, 5.3, 9.3, 11.1, 7.5... $ short.velocity &lt;dbl&gt; 1.76, 1.34, 1.27, 1.47, 1.27, 1.49, 1.31, 1.09,... 3.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 3.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). thuesen %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max blood.glucose 24 10.300 4.338 4.200 7.075 12.700 19.500 short.velocity 23 1.326 0.233 1.030 1.185 1.420 1.950 The stargazer() function has many handy options, should you wish to change the default settings. thuesen %&gt;% stargazer::stargazer(type = &quot;html&quot;, digits = 4, flip = TRUE, summary.stat = c(&quot;n&quot;, &quot;mean&quot;, &quot;sd&quot;, &quot;min&quot;, &quot;median&quot;, &quot;max&quot;), title = &quot;Descriptives&quot;) Descriptives Statistic blood.glucose short.velocity N 24 23 Mean 10.3000 1.3257 St. Dev. 4.3375 0.2329 Min 4.2000 1.0300 Median 9.4000 1.2700 Max 19.5000 1.9500 Although the table1() function from the furniture package creates a nice summary table, it ‘hides’ the nubmer of missing values for each continuous variable (Barrett, Brignone, and Laxman 2019). thuesen %&gt;% furniture::table1(&quot;Fasting Blood Glucose&quot; = blood.glucose, &quot;Circumferential Shortening Velocity&quot; = short.velocity, # defaults to excluding any row missing any variable output = &quot;html&quot;) Mean/Count (SD/%) n = 23 Fasting Blood Glucose 10.4 (4.4) Circumferential Shortening Velocity 1.3 (0.2) thuesen %&gt;% furniture::table1(&quot;Fasting Blood Glucose&quot; = blood.glucose, &quot;Circumferential Shortening Velocity&quot; = short.velocity, na.rm = FALSE, # retains even partial cases output = &quot;html&quot;) Mean/Count (SD/%) n = 24 Fasting Blood Glucose 10.3 (4.3) Circumferential Shortening Velocity 1.3 (0.2) 3.2.2 Univariate Visualizations ggplot(thuesen, aes(blood.glucose)) + # variable of interest (just one) geom_histogram(binwidth = 2) # specify the width of the bars thuesen %&gt;% ggplot() + aes(short.velocity) + # variable of interest (just one) geom_histogram(bins = 10) # specify the number of bars 3.2.3 Bivariate Statistics (Unadjusted Pearson’s correlation) The cor() fucntion in base \\(R\\) doesn’t like NA or missing values thuesen %&gt;% cor() blood.glucose short.velocity blood.glucose 1 NA short.velocity NA 1 You may specify how to handle cases that are missing on at least one of the variables of interest: use = \"everything\" NAs will propagate conceptually, i.e., a resulting value will be NA whenever one of its contributing observations is NA &lt;– DEFAULT use = \"all.obs\" the presence of missing observations will produce an error use = \"complete.obs\" missing values are handled by casewise deletion (and if there are no complete cases, that gives an error). use = \"na.or.complete\" is the same as above unless there are no complete cases, that gives NA use = \"pairwise.complete.obs\" the correlation between each pair of variables is computed using all complete pairs of observations on those variables. This can result in covariance matrices which are not positive semi-definite, as well as NA entries if there are no complete pairs for that pair of variables. Commonly, we want listwise deletion: thuesen %&gt;% cor(use = &quot;complete.obs&quot;) # list-wise deletion blood.glucose short.velocity blood.glucose 1.0000000 0.4167546 short.velocity 0.4167546 1.0000000 It is also handy to specify the number of decimal places desired, but adding a rounding step: thuesen %&gt;% cor(use = &quot;complete.obs&quot;) %&gt;% round(2) # number od decimal places blood.glucose short.velocity blood.glucose 1.00 0.42 short.velocity 0.42 1.00 If you desire a correlation single value of a single PAIR of variables, instead of a matrix, then you must use a magrittr exposition pipe (%$%) thuesen %$% # notice the special kind of pipe cor(blood.glucose, short.velocity, # specify exactly TWO variables use = &quot;complete.obs&quot;) [1] 0.4167546 In addition to the cor() funciton, the base \\(R\\) stats package also includes the cor.test() function to test if the correlation is zero (\\(H_0: R = 0\\)) This TESTS if the cor == 0 thuesen %$% # notice the special kind of pipe cor.test(blood.glucose, short.velocity, # specify exactly TWO variables use=&quot;complete.obs&quot;) Pearson&#39;s product-moment correlation data: blood.glucose and short.velocity t = 2.101, df = 21, p-value = 0.0479 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.005496682 0.707429479 sample estimates: cor 0.4167546 The default correltaion type for cor()is Pearson’s \\(R\\), which assesses linear relationships. Spearman’s correlation assesses monotonic relationships. thuesen %$% # notice the special kind of pipe cor(blood.glucose, short.velocity, # specify exactly TWO variables use = &#39;complete&#39;, method = &#39;spearman&#39;) # spearman&#39;s (rho) [1] 0.318002 thuesen %&gt;% dplyr::select(blood.glucose, short.velocity) %&gt;% furniture::tableC(cor_type = &quot;pearson&quot;, na.rm = TRUE, rounding = 3, output = &quot;markdown&quot;, booktabs = TRUE, caption = &quot;Correlation Table&quot;) [1] [2] [1]blood.glucose 1.00 [2]short.velocity 0.417 (0.048) 1.00 3.2.4 Bivariate Visualization Scatterplots show the relationship between two continuous measures (one on the \\(x-axis\\) and the other on the \\(y-axis\\)), with one point for each observation. ggplot(thuesen, aes(x = blood.glucose, # x-axis variable y = short.velocity)) + # y-axis variable geom_point() + # place a point for each observation theme_bw() # black-and-white theme Both the code chunk above and below produce the same plot. thuesen %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = short.velocity) + # y-axis variable geom_point() + # place a point for each observation theme_bw() # black-and-white theme 3.3 Regression Analysis 3.3.1 Fit A Simple Linear Model \\[ Y = \\beta_0 + \\beta_1 \\times X \\] short.velocity dependent variable or outcome (\\(Y\\)) blood.glucose independent variable or predictor (\\(X\\)) The lm() function must be supplied with at least two options: a formula: Y ~ X a dataset: data = XXXXXXX When a model is fit and directly saved as a named object via the assignment opperator (&lt;-), no output is produced. fit_vel_glu &lt;- lm(short.velocity ~ blood.glucose, data = thuesen) Running the name of the fit object yields very little output: fit_vel_glu Call: lm(formula = short.velocity ~ blood.glucose, data = thuesen) Coefficients: (Intercept) blood.glucose 1.09781 0.02196 Appling the summary() funciton produced a good deal more output: summary(fit_vel_glu) Call: lm(formula = short.velocity ~ blood.glucose, data = thuesen) Residuals: Min 1Q Median 3Q Max -0.40141 -0.14760 -0.02202 0.03001 0.43490 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.09781 0.11748 9.345 6.26e-09 *** blood.glucose 0.02196 0.01045 2.101 0.0479 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2167 on 21 degrees of freedom (1 observation deleted due to missingness) Multiple R-squared: 0.1737, Adjusted R-squared: 0.1343 F-statistic: 4.414 on 1 and 21 DF, p-value: 0.0479 You may request specific pieces of the output: Coefficients or beta estimates: coef(fit_vel_glu) (Intercept) blood.glucose 1.09781488 0.02196252 95% confidence intervals for the coefficients or beta estimates: confint(fit_vel_glu) 2.5 % 97.5 % (Intercept) 0.8534993816 1.34213037 blood.glucose 0.0002231077 0.04370194 The F-test for overall modle fit vs. a \\(null\\) or empty model having only an intercept and no predictors. anova(fit_vel_glu) # A tibble: 2 x 5 Df `Sum Sq` `Mean Sq` `F value` `Pr(&gt;F)` &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0.207 0.207 4.41 0.0479 2 21 0.986 0.0470 NA NA Various other model fit indicies: logLik(fit_vel_glu) &#39;log Lik.&#39; 3.583612 (df=3) AIC(fit_vel_glu) [1] -1.167223 BIC(fit_vel_glu) [1] 2.239259 3.3.2 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_vel_glu, which = 1) plot(fit_vel_glu, which = 2) plot(fit_vel_glu, which = 5) plot(fit_vel_glu, which = 6) Viewing potentially influencial or outlier points based on plots above: thuesen %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::filter(id == c(13, 20, 24)) # A tibble: 3 x 3 blood.glucose short.velocity id &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 19 1.95 13 2 16.1 1.05 20 3 9.5 1.7 24 The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2019). car::residualPlots(fit_vel_glu) Test stat Pr(&gt;|Test stat|) blood.glucose 0.9289 0.3640 Tukey test 0.9289 0.3529 Here is a fancy way to visulaize ‘potential problem cases’ with ggplot2: thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # keep only complete cases ggplot() + # name the FULL dataset aes(x = blood.glucose, # x-axis variable name y = short.velocity) + # y-axis variable name geom_point() + # do a scatterplot stat_smooth(method = &quot;lm&quot;) + # smooth: linear model theme_bw() + # black-and-while theme geom_point(data = thuesen %&gt;% # override the dataset from above filter(row_number() == c(13, 20, 24)), # with a reduced subset of cases size = 4, # make the points bigger in size color = &quot;red&quot;) # give the points a different color 3.3.3 Manually checking residual diagnostics You may extract values from the model in dataset form and then you can maually plot the residuals. thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # keep only complete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) # residual values # A tibble: 23 x 4 blood.glucose short.velocity pred resid &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 15.3 1.76 1.43 0.326 2 10.8 1.34 1.34 0.00499 3 8.1 1.27 1.28 -0.00571 4 19.5 1.47 1.53 -0.0561 5 7.2 1.27 1.26 0.0141 6 5.3 1.49 1.21 0.276 7 9.3 1.31 1.30 0.00793 8 11.1 1.09 1.34 -0.252 9 7.5 1.18 1.26 -0.0825 10 12.2 1.22 1.37 -0.146 # ... with 13 more rows Check for equal spread of points along the \\(y=0\\) horizontal line: thuesen %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # keep only complete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) %&gt;% # residual values ggplot() + aes(x = id, y = resid) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, size = 1, linetype = &quot;dashed&quot;) + theme_classic() + labs(title = &quot;Looking for homogeneity of residuals&quot;, subtitle = &quot;want to see equal spread all across&quot;) Check for normality: thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # keep only complete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) %&gt;% # residual values ggplot() + aes(resid) + geom_histogram(bins = 12, color = &quot;blue&quot;, fill = &quot;blue&quot;, alpha = 0.3) + geom_vline(xintercept = 0, size = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + theme_classic() + labs(title = &quot;Looking for normality of residuals&quot;, subtitle = &quot;want to see roughly a bell curve&quot;) 3.4 Conclusion 3.4.1 Tabulate the Final Model Summary You may also present the output in a table using two different packages: The stargazer package has stargazer() function: stargazer::stargazer(fit_vel_glu, type = &quot;html&quot;) Dependent variable: short.velocity blood.glucose 0.022** (0.010) Constant 1.098*** (0.117) Observations 23 R2 0.174 Adjusted R2 0.134 Residual Std. Error 0.217 (df = 21) F Statistic 4.414** (df = 1; 21) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 The stargazer package can produce the regression table in various output types: type = \"latex Default Use when knitting your .Rmd file to a .pdf via LaTeX type = \"text Default Use when working on a project and viewing tables on your computer screen type = \"html Default Use when knitting your .Rmd file to a .html document The texreg package has the texreg() fucntion: texreg::htmlreg(fit_vel_glu) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Statistical models Model 1 (Intercept) 1.10*** (0.12) blood.glucose 0.02* (0.01) R2 0.17 Adj. R2 0.13 Num. obs. 23 RMSE 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 The texreg package contains three version of the regression table function. screenreg() Use when working on a project and viewing tables on your computer screen htmlreg() Use when knitting your .Rmd file to a .html document texreg() Use when knitting your .Rmd file to a .pdf via LaTeX 3.4.2 Plot the Model When a model only contains main effects, a plot is not important for interpretation, but can help understand the relationship between multiple predictors. The Effect() function from the effects package chooses ‘5 or 6 nice values’ for your continuous independent variable (\\(X\\)) based on the range of values found in the dataset on which the model was fit and plugs them into the regression equation \\(Y = \\beta_0 + \\beta_1 \\times X\\) to compute the predicted mean value of the outcome (\\(Y\\)) (Fox et al. 2019). effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), # IV variable name mod = fit_vel_glu) # fitted model name blood.glucose effect blood.glucose 4.2 8 12 16 20 1.190057 1.273515 1.361365 1.449215 1.537065 You may override the ‘nice values’ using the xlevels = list(var_name = c(#, #, ...#) option. effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = c(5, 10, 15, 20))) blood.glucose effect blood.glucose 5 10 15 20 1.207627 1.317440 1.427253 1.537065 Adding a piped data frame step (%&gt;% data.frame()) will arrange the predicted \\(Y\\) values into a column called fit. This tidy data format is ready for plotting. effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu) %&gt;% data.frame() # A tibble: 5 x 5 blood.glucose fit se lower upper &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4.2 1.19 0.0788 1.03 1.35 2 8 1.27 0.0516 1.17 1.38 3 12 1.36 0.0483 1.26 1.46 4 16 1.45 0.0742 1.29 1.60 5 20 1.54 0.110 1.31 1.77 effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = c(5, 12, 20))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .5) + # ribbon transparency level geom_line() + theme_bw() Notice that although the regression line is smooth, the ribbon is choppy. This is because we are basing it on only THREE values of \\(X\\). c(5, 12, 20) [1] 5 12 20 Use the seq() function in base \\(R\\) to request many values of \\(X\\) seq(from = 5, to = 20, by = 5) [1] 5 10 15 20 seq(from = 5, to = 20, by = 2) [1] 5 7 9 11 13 15 17 19 seq(from = 5, to = 20, by = 1) [1] 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 seq(from = 5, to = 20, by = .5) [1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 11.5 [15] 12.0 12.5 13.0 13.5 14.0 14.5 15.0 15.5 16.0 16.5 17.0 17.5 18.0 18.5 [29] 19.0 19.5 20.0 effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .5) + # ribbon transparency level geom_line() + theme_bw() Now that we are basing our ribbon on MANY more points of \\(X\\), the ribbon is much smoother. For publication, you would of course want to clean up the plot a bit more: effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .3) + # ribbon transparency level geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) # axis labels The above plot has a ribbon that represents a 95% confidence interval (lower toupper) for the MEAN (fit) outcome. Sometimes we would rather display a ribbon for only the MEAN (fit) plus-or-minus ONE STANDARD ERROR (se) for the mean. You would do that by changing the variables that define the min and max edges of the ribbon (notice the range of the y-axis has changed): effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, y = fit) + geom_ribbon(aes(ymin = fit - se, # bottom edge of the ribbon ymax = fit + se), # top edge of the ribbon alpha = .3) + geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) Of course, you could do both ribbons together: effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, y = fit) + geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon = lower of the 95% CI ymax = upper), # top edge of the ribbon = upper of the 95% CI alpha = .3) + geom_ribbon(aes(ymin = fit - se, # bottom edge of the ribbon = mean - SE ymax = fit + se), # top edge of the ribbon = Mean + SE alpha = .3) + geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) # axis labels "],
["multiple-linear-regression-ex-obesity-and-blood-pressure-interaction-between-a-continuous-and-categorical-ivs.html", "4 Multiple Linear Regression - Ex: Obesity and Blood Pressure (interaction between a continuous and categorical IVs) 4.1 Purpose 4.2 Exploratory Data Analysis 4.3 Regression Analysis 4.4 Conclusion", " 4 Multiple Linear Regression - Ex: Obesity and Blood Pressure (interaction between a continuous and categorical IVs) library(tidyverse) # super helpful everything! library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools library(GGally) # extensions to ggplot2 library(ISwR) # Introduction to Statistics with R (datasets) 4.1 Purpose 4.1.1 Research Question Is obsesity associated with higher blood pressure and is that relationship the same among men and women? 4.1.2 Data Description This dataset is included in the ISwR package (Dalgaard 2015), which was a companion to the texbook “Introductory Statistics with R, 2nd ed.” (Dalgaard 2008), although it was first published by Brown and Hollander (1977). To view the documentation for the dataset, type ?bp.obese in the console and enter or search the help tab for `bp.obese’. The bp.obese data frame has 102 rows and 3 columns. It contains data from a random sample of Mexican-American adults in a small California town. This data frame contains the following columns: sex a numeric vector code, 0: male, 1: female obese a numeric vector, ratio of actual weight to ideal weight from New York Metropolitan Life Tables bp a numeric vector,systolic blood pressure (mm Hg) data(bp.obese, package = &quot;ISwR&quot;) bp.obese &lt;- bp.obese %&gt;% dplyr::mutate(sex = factor(sex, labels = c(&quot;Male&quot;, &quot;Female&quot;))) tibble::glimpse(bp.obese) Observations: 102 Variables: 3 $ sex &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Ma... $ obese &lt;dbl&gt; 1.31, 1.31, 1.19, 1.11, 1.34, 1.17, 1.56, 1.18, 1.04, 1.... $ bp &lt;int&gt; 130, 148, 146, 122, 140, 146, 132, 110, 124, 150, 120, 1... 4.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 4.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). bp.obese %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max obese 102 1.313 0.258 0.810 1.143 1.430 2.390 bp 102 127.020 18.184 94 116 137.5 208 4.2.2 Bivariate Relationships The furniture package’s table1() function is a clean way to create a descriptive table that compares distinct subgroups of your sample (Barrett, Brignone, and Laxman 2019). bp.obese %&gt;% furniture::table1(obese, bp, splitby = ~ sex, test = TRUE, output = &quot;html&quot;) Male Female P-Value n = 44 n = 58 obese &lt;.001 1.2 (0.2) 1.4 (0.3) bp 0.653 128.0 (16.6) 126.3 (19.4) The ggpairs() function in the GGally package is helpful for showing all pairwise relationships in raw data, especially seperating out two or three groups (Schloerke et al. 2018). GGally::ggpairs(bp.obese, mapping = aes(fill = sex, col = sex, alpha = 0.1), upper = list(continuous = &quot;smooth&quot;, combo = &quot;facethist&quot;, discrete = &quot;ratio&quot;), lower = list(continuous = &quot;cor&quot;, combo = &quot;box&quot;, discrete = &quot;facetbar&quot;), title = &quot;Very Useful for Exploring Data&quot;) bp.obese %&gt;% ggplot() + aes(x = sex, y = bp, fill = sex) + geom_boxplot(alpha = 0.6) + scale_fill_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;)) + labs(x = &quot;Gender&quot;, y = &quot;Blood Pressure (mmHg)&quot;) + guides(fill = FALSE) + theme_bw() Visual inspection for an interaction (is gender a moderator?) bp.obese %&gt;% ggplot(aes(x = obese, y = bp, color = sex)) + geom_point(size = 3) + geom_smooth(aes(fill = sex), alpha = 0.2, method = &quot;lm&quot;) + scale_color_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;), breaks = c(&quot;male&quot;, &quot;female&quot;), labels = c(&quot;Men&quot;, &quot;Women&quot;)) + scale_fill_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;), breaks = c(&quot;male&quot;, &quot;female&quot;), labels = c(&quot;Men&quot;, &quot;Women&quot;)) + labs(title = &quot;Does Gender Moderate the Association Between Obesity and Blood Pressure?&quot;, x = &quot;Ratio: Actual Weight vs. Ideal Weight (NYM Life Tables)&quot;, y = &quot;Systolic Blood Pressure (mmHg)&quot;) + theme_bw() + scale_x_continuous(breaks = seq(from = 0, to = 3, by = 0.25 )) + scale_y_continuous(breaks = seq(from = 75, to = 300, by = 25)) + theme(legend.title = element_blank(), legend.key = element_rect(fill = &quot;white&quot;), legend.background = element_rect(color = &quot;black&quot;), legend.justification = c(1, 0), legend.position = c(1, 0)) bp.obese %&gt;% dplyr::mutate(sex = as.numeric(sex)) %&gt;% # cor needs only numeric cor() %&gt;% round(3) sex obese bp sex 1.000 0.405 -0.045 obese 0.405 1.000 0.326 bp -0.045 0.326 1.000 Often it is easier to digest a correlation matrix if it is visually presented, instead of just given as a table of many numbers. The corrplot package has a useful function called corrplot.mixed() for doing just that (Wei and Simko 2017). bp.obese %&gt;% dplyr::mutate(sex = as.numeric(sex)) %&gt;% # cor needs only numeric cor() %&gt;% corrplot::corrplot.mixed(lower = &quot;ellipse&quot;, upper = &quot;number&quot;, tl.col = &quot;black&quot;) 4.3 Regression Analysis 4.3.1 Fit Nested Models The bottom-up approach consists of starting with an initial NULL model with only an intercept term and them building additional models that are nested. Two models are considered nested if one is conains a subset of the terms (predictors or IV) compared to the other. fit_bp_null &lt;- lm(bp ~ 1, data = bp.obese) # intercept only or NULL model fit_bp_sex &lt;- lm(bp ~ sex, data = bp.obese) fit_bp_obe &lt;- lm(bp ~ obese, data = bp.obese) fit_bp_obesex &lt;- lm(bp ~ obese + sex, data = bp.obese) fit_bp_inter &lt;- lm(bp ~ obese*sex, data = bp.obese) 4.3.2 Comparing Nested Models 4.3.2.1 Model Comparison Table In single level, multiple linear regression significance of predictors (independent variables, IV) is usually based on both the Wald tests of significance for each beta estimate (shown with stars here) and comparisons in the model fit via the \\(R^2\\) values. Again the texreg package comes in handy to display several models in the same tal e (Leifeld 2017). texreg::htmlreg(list(fit_bp_null, fit_bp_sex, fit_bp_obe, fit_bp_obesex, fit_bp_inter), custom.model.names = c(&quot;No Predictors&quot;, &quot;Only Sex Quiz&quot;, &quot;Only Obesity&quot;, &quot;Both IVs&quot;, &quot;Add Interaction&quot;)) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Statistical models No Predictors Only Sex Quiz Only Obesity Both IVs Add Interaction (Intercept) 127.02*** 127.95*** 96.82*** 93.29*** 102.11*** (1.80) (2.75) (8.92) (8.94) (18.23) sexFemale -1.64 -7.73* -19.60 (3.65) (3.72) (21.66) obese 23.00*** 29.04*** 21.65 (6.67) (7.17) (15.12) obese:sexFemale 9.56 (17.19) R2 0.00 0.00 0.11 0.14 0.15 Adj. R2 0.00 -0.01 0.10 0.13 0.12 Num. obs. 102 102 102 102 102 RMSE 18.18 18.26 17.28 17.00 17.05 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.3.2.2 Likelihood Ratio Test of Nested Models An alternative method for determing model fit and variable importance is the likelihood ratio test. This involves comparing the \\(-2LL\\) or inverse of twice the log of the likelihood value for the model. The difference in these values follows a Chi Squared distribution with degrees of freedom equal to the difference in the number of parameters estimated (number of betas). Test the main effect of math quiz: anova(fit_bp_null, fit_bp_sex) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 101 33398. NA NA NA NA 2 100 33330. 1 67.6 0.203 0.653 Test the main effect of math phobia anova(fit_bp_null, fit_bp_obe) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 101 33398. NA NA NA NA 2 100 29846. 1 3552. 11.9 0.000822 Test the main effect of math phobia, after controlling for math test anova(fit_bp_obe, fit_bp_obesex) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 100 29846. NA NA NA NA 2 99 28595. 1 1250. 4.33 0.0401 Test the interaction between math test and math phobia (i.e. moderation) anova(fit_bp_obesex, fit_bp_inter) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 99 28595. NA NA NA NA 2 98 28505. 1 89.9 0.309 0.579 4.3.3 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_bp_obesex, which = 1) plot(fit_bp_obesex, which = 4, id.n = 10) # Change the number labeled The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2019). car::residualPlots(fit_bp_obesex) Test stat Pr(&gt;|Test stat|) obese -0.2759 0.7832 sex Tukey test -0.6141 0.5391 you can adjust any part of a ggplot bp.obese %&gt;% dplyr::mutate(e_bp = resid(fit_bp_obesex)) %&gt;% # add the resid to the dataset ggplot(aes(x = sex, # x-axis variable name y = e_bp, # y-axis variable name color = sex, # color is the outline fill = sex)) + # fill is the inside geom_hline(yintercept = 0, # set at a meaningful value size = 1, # adjust line thickness linetype = &quot;dashed&quot;, # set type of line color = &quot;purple&quot;) + # color of line geom_boxplot(alpha = 0.5) + # level of transparency theme_bw() + # my favorite theme labs(title = &quot;Check Assumptions&quot;, # main title&#39;s text x = &quot;Gender&quot;, # x-axis text label y = &quot;Blood Pressure, Residual (bpm)&quot;) + # y-axis text label scale_y_continuous(breaks = seq(from = -40, # declare a sequence of to = 80, # values to make the by = 20)) + # tick marks at guides(color = FALSE, fill = FALSE) # no legends included bp.obese %&gt;% dplyr::mutate(e_bp = resid(fit_bp_obesex)) %&gt;% # add the resid to the dataset ggplot(aes(x = e_bp, # y-axis variable name color = sex, # color is the outline fill = sex)) + # fill is the inside geom_density(alpha = 0.5) + geom_vline(xintercept = 0, # set at a meaningful value size = 1, # adjust line thickness linetype = &quot;dashed&quot;, # set type of line color = &quot;purple&quot;) + # color of line theme_bw() + # my favorite theme labs(title = &quot;Check Assumptions&quot;, # main title&#39;s text x = &quot;Blood Pressure, Residual (bpm)&quot;) + # y-axis text label scale_x_continuous(breaks = seq(from = -40, # declare a sequence of to = 80, # values to make the by = 20)) # tick marks at 4.4 Conclusion Violations to the assumtions call the reliabity of the regression results into question. The data should be further investigated, specifically the \\(102^{nd}\\) case. "],
["multiple-linear-regression-ex-ihnos-experiment-interaction-between-two-continuous-ivs.html", "5 Multiple Linear Regression - Ex: Ihno’s Experiment (interaction between two continuous IVs) 5.1 Purpose 5.2 Exploratory Data Analysis 5.3 Regression Analysis 5.4 Conclusion 5.5 Write-up", " 5 Multiple Linear Regression - Ex: Ihno’s Experiment (interaction between two continuous IVs) library(tidyverse) # super helpful everything! library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools 5.1 Purpose 5.1.1 Research Question Does math phobia moderate the relationship between math and statistics performance? That is, does the assocation between math and stat quiz performance differ at variaous levels of math phobia? 5.1.2 Data Description Inho’s dataset is included in the textbook “Explaining Psychological Statistics” (Cohen 2013) and details regarding the sample and measures is describe in this Encyclopedia’s Vol. 2 - Ihno’s Dataset. data_ihno &lt;- haven::read_spss(&quot;http://www.psych.nyu.edu/cohen/Ihno_dataset.sav&quot;) %&gt;% dplyr::rename_all(tolower) %&gt;% dplyr::mutate(gender = factor(gender, levels = c(1, 2), labels = c(&quot;Female&quot;, &quot;Male&quot;))) %&gt;% dplyr::mutate(major = factor(major, levels = c(1, 2, 3, 4,5), labels = c(&quot;Psychology&quot;, &quot;Premed&quot;, &quot;Biology&quot;, &quot;Sociology&quot;, &quot;Economics&quot;))) %&gt;% dplyr::mutate(reason = factor(reason, levels = c(1, 2, 3), labels = c(&quot;Program requirement&quot;, &quot;Personal interest&quot;, &quot;Advisor recommendation&quot;))) %&gt;% dplyr::mutate(exp_cond = factor(exp_cond, levels = c(1, 2, 3, 4), labels = c(&quot;Easy&quot;, &quot;Moderate&quot;, &quot;Difficult&quot;, &quot;Impossible&quot;))) %&gt;% dplyr::mutate(coffee = factor(coffee, levels = c(0, 1), labels = c(&quot;Not a regular coffee drinker&quot;, &quot;Regularly drinks coffee&quot;))) 5.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 5.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% data.frame() %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max phobia 100 3.310 2.444 0 1 4 10 mathquiz 85 29.071 9.480 9.000 22.000 35.000 49.000 statquiz 100 6.860 1.700 1 6 8 10 5.2.2 Bivariate Relationships The furniture package’s table1() function is a clean way to create a descriptive table that compares distinct subgroups of your sample (Barrett, Brignone, and Laxman 2019). Although categorizing continuous variables results in a loss of information (possible signal or noise), it is often done to investigate relationships in an exploratory way. data_ihno %&gt;% dplyr::mutate(phobia_cut3 = cut(phobia, breaks = c(0, 2, 4, 10), include.lowest = TRUE)) %&gt;% furniture::table1(mathquiz, statquiz, splitby = ~ phobia_cut3, na.rm = FALSE, test = TRUE, output = &quot;html&quot;) [0,2] (2,4] (4,10] P-Value n = 39 n = 37 n = 24 mathquiz 0.014 32.6 (8.5) 26.5 (9.8) 26.8 (8.9) statquiz 0.001 7.6 (1.3) 6.6 (1.6) 6.1 (2.0) One of the quickest ways to get a feel for all the pairwise relationships in your dataset (provided there aren’t too many variables) is with the pairs.panels() function in the psych package (Revelle 2019). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% data.frame() %&gt;% psych::pairs.panels(lm = TRUE, ci = TRUE, stars = TRUE) When two variables are both continuous, correlations (Pearson’s \\(R\\)) are an important measure of association. Notice the discrepincy between the correlation between statquiz and phobia. Above, the psych::pairs.panels() function uses pairwise complete cases by default, so \\(r=-.39\\) is computed on all \\(n=100\\) subjects. Below, we specified use = \"complete.obs\" in the cor() fucntion, so all correlations will be based on the same \\(n=85\\) students, making it listwise complete. The choice of which method to you will vary by situation. Often it is easier to digest a correlation matrix if it is visually presented, instead of just given as a table of many numbers. The corrplot package has a useful function called corrplot.mixed() for doing just that (Wei and Simko 2017). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% cor(use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot.mixed(lower = &quot;ellipse&quot;, upper = &quot;number&quot;, tl.col = &quot;black&quot;) 5.3 Regression Analysis 5.3.1 Subset the Sample All regression models can only be fit to complete observations regarding the variables included in the model (dependent and independent). Removing any case that is incomplete with respect to even one variables is called “list-wise deletion”. In this analysis, models including the mathquiz variable will be fit on only 85 students (sincle 15 students did not take the math quiz), where as models not including this variable will be fit to all 100 studnets. This complicates model comparisons, which require nested models be fit to the same data (exactly). For this reason, the dataset has been reduced to the subset of students that are complete regarding the three variables utilized throughout the set of five nested models. data_ihno_fitting &lt;- data_ihno %&gt;% dplyr::filter(complete.cases(mathquiz, statquiz, phobia)) dim(data_ihno_fitting) [1] 85 18 5.3.2 Fit Nested Models The bottom-up approach consists of starting with an initial NULL model with only an intercept term and them building additional models that are nested. Two models are considered nested if one is conains a subset of the terms (predictors or IV) compared to the other. fit_ihno_lm_0 &lt;- lm(statquiz ~ 1, # null model: intercept only data = data_ihno_fitting) fit_ihno_lm_1 &lt;- lm(statquiz ~ mathquiz, # only main effect of mathquiz data = data_ihno_fitting) fit_ihno_lm_2 &lt;- lm(statquiz ~ phobia, # only mian effect of phobia data = data_ihno_fitting) fit_ihno_lm_3 &lt;- lm(statquiz ~ mathquiz + phobia, # both main effects data = data_ihno_fitting) fit_ihno_lm_4 &lt;- lm(statquiz ~ mathquiz*phobia, # additional interaction data = data_ihno_fitting) 5.3.3 Comparing Nested Models 5.3.3.1 Model Comparison Table In single level, multiple linear regression significance of predictors (independent variables, IV) is usually based on both the Wald tests of significance for each beta estimate (shown with stars here) and comparisons in the model fit via the \\(R^2\\) values. Again the texreg package comes in handy to display several models in the same tal e (Leifeld 2017). texreg::htmlreg(list(fit_ihno_lm_0, fit_ihno_lm_1, fit_ihno_lm_2, fit_ihno_lm_3, fit_ihno_lm_4), custom.model.names = c(&quot;No Predictors&quot;, &quot;Only Math Quiz&quot;, &quot;Only Phobia&quot;, &quot;Both IVs&quot;, &quot;Add Interaction&quot;)) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Statistical models No Predictors Only Math Quiz Only Phobia Both IVs Add Interaction (Intercept) 6.85*** 4.14*** 7.65*** 5.02*** 5.60*** (0.19) (0.53) (0.29) (0.63) (0.91) mathquiz 0.09*** 0.08*** 0.06* (0.02) (0.02) (0.03) phobia -0.25*** -0.16* -0.34 (0.07) (0.07) (0.21) mathquiz:phobia 0.01 (0.01) R2 0.00 0.26 0.13 0.31 0.31 Adj. R2 0.00 0.25 0.12 0.29 0.29 Num. obs. 85 85 85 85 85 RMSE 1.74 1.50 1.63 1.46 1.46 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 5.3.3.2 Likelihood Ratio Test of Nested Models An alternative method for determing model fit and variable importance is the likelihood ratio test. This involves comparing the \\(-2LL\\) or inverse of twice the log of the likelihood value for the model. The difference in these values follows a Chi Squared distribution with degrees of freedom equal to the difference in the number of parameters estimated (number of betas). Test the main effect of math quiz: anova(fit_ihno_lm_0, fit_ihno_lm_1) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 84 253. NA NA NA NA 2 83 188. 1 65.3 28.8 0.000000700 Test the main effect of math phobia anova(fit_ihno_lm_0, fit_ihno_lm_2) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 84 253. NA NA NA NA 2 83 221. 1 32.3 12.1 0.000791 Test the main effect of math phobia, after controlling for math test anova(fit_ihno_lm_1, fit_ihno_lm_3) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 83 188. NA NA NA NA 2 82 175. 1 12.6 5.88 0.0175 Test the interaction between math test and math phobia (i.e. moderation) anova(fit_ihno_lm_3, fit_ihno_lm_4) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 82 175. NA NA NA NA 2 81 173. 1 1.69 0.789 0.377 5.3.4 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_ihno_lm_3, which = 1) plot(fit_ihno_lm_3, which = 2) The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2019). car::residualPlots(fit_ihno_lm_3) Test stat Pr(&gt;|Test stat|) mathquiz -1.7778 0.07918 . phobia 0.5004 0.61813 Tukey test -1.5749 0.11527 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 While the model tables give starts to denote significance, you may print the actual p-values with the summary() function applied to the model name. summary(fit_ihno_lm_3) Call: lm(formula = statquiz ~ mathquiz + phobia, data = data_ihno_fitting) Residuals: Min 1Q Median 3Q Max -4.3436 -0.8527 0.2805 0.9857 2.7370 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.01860 0.62791 7.993 7.23e-12 *** mathquiz 0.08097 0.01754 4.617 1.42e-05 *** phobia -0.16176 0.06670 -2.425 0.0175 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.462 on 82 degrees of freedom Multiple R-squared: 0.3076, Adjusted R-squared: 0.2907 F-statistic: 18.21 on 2 and 82 DF, p-value: 2.849e-07 summary(fit_ihno_lm_4) Call: lm(formula = statquiz ~ mathquiz * phobia, data = data_ihno_fitting) Residuals: Min 1Q Median 3Q Max -4.1634 -0.8433 0.2832 0.9685 2.9434 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.600183 0.907824 6.169 2.57e-08 *** mathquiz 0.061216 0.028334 2.161 0.0337 * phobia -0.339426 0.210907 -1.609 0.1114 mathquiz:phobia 0.006485 0.007303 0.888 0.3771 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.464 on 81 degrees of freedom Multiple R-squared: 0.3143, Adjusted R-squared: 0.2889 F-statistic: 12.37 on 3 and 81 DF, p-value: 9.637e-07 5.4 Conclusion 5.4.1 Tabulate the Final Model Summary Many journals prefer that regression tables include 95% confidence intervals, rater than standard errors for the beta estimates. The texreg package contains three version of the regression table function (Leifeld 2017). screenreg() Use when working on a project and viewing tables on your computer screen htmlreg() Use when knitting your .Rmd file to a .html document texreg() Use when knitting your .Rmd file to a .pdf via LaTeX texreg::htmlreg(fit_ihno_lm_3, custom.model.names = &quot;Main Effects Model&quot;, ci.force = TRUE, # request 95% conf interv caption = &quot;Final Model for Stat&#39;s Quiz&quot;, single.row = TRUE) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Final Model for Stat’s Quiz Main Effects Model (Intercept) 5.02 [3.79; 6.25]* mathquiz 0.08 [0.05; 0.12]* phobia -0.16 [-0.29; -0.03]* R2 0.31 Adj. R2 0.29 Num. obs. 85 RMSE 1.46 * 0 outside the confidence interval 5.4.2 Plot the Model When a model only contains main effects, a plot is not important for interpretation, but can help understand the relationship between multiple predictors. The Effect() function from the effects package chooses ‘5 or 6 nice values’ for each of your continuous independent variable (\\(X&#39;s\\)) based on the range of values found in the dataset on which the model and plugs all possible combinations of them into the regression equation \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 \\dots \\beta_k X_k\\) to compute the predicted mean value of the outcome (\\(Y\\)) (Fox et al. 2019). When plotting a regression model the outcome (dependent variable) is always on the y-axis (fit) and only one predictor (independent variable) may be used on the x-axis. You may incorporate additional predictor using colors, shapes, linetypes, or facets. For these predictors, you will want to specify only 2-4 values for illustration and then declare them as factors prior to plotting. effects::Effect(focal.predictors = c(&quot;mathquiz&quot;, &quot;phobia&quot;), mod = fit_ihno_lm_3, xlevels = list(phobia = c(0, 5, 10))) %&gt;% # values for illustration data.frame %&gt;% dplyr::mutate(phobia = factor(phobia)) %&gt;% # factor for illustration ggplot() + aes(x = mathquiz, y = fit, fill = phobia) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se), alpha = .3) + geom_line(aes(color = phobia)) + theme_bw() + labs(x = &quot;Score on Math Quiz&quot;, y = &quot;Estimated Marginal Mean\\nScore on Stat Quiz&quot;, fill = &quot;Self Rated\\nMath Phobia&quot;, color = &quot;Self Rated\\nMath Phobia&quot;) + theme(legend.background = element_rect(color = &quot;black&quot;), legend.position = c(0, 1), legend.key.width = unit(1.5, &quot;cm&quot;), legend.justification = c(-0.1, 1.1)) 5.5 Write-up There is evidence both mathquiz and phobia are associated with statquiz and that the relationship is addative (i.e. no interaction). There is a strong association between math and stats quiz scores, \\(r = .51\\). Math phobia is associated with lower math, \\(r = -.28\\), and stats quiz scores, \\(r = -.36\\). When considered togehter, the combined effects of math phobia and math score account for 31% of the variance in statistical achievement. Not surprizingly, while higher self-reported math phobia was associated with lower statists scores, \\(b = -0.162\\), \\(p=.018\\), \\(95CI = [-0.29, -0.03]\\), higher math quiz scores were associated with higher stats score, \\(b = -0.081\\), \\(p&lt;.001\\), \\(95CI = [0.05, 0.12]\\). There was no evidence that math phobia moderated the relationship between math and quiz performance, \\(p=.377\\). "],
["logistic-regression-ex-bronchopulmonary-dysplasia-in-premature-infants.html", "6 Logistic Regression - Ex: Bronchopulmonary Dysplasia in Premature Infants 6.1 Background 6.2 Logistic Regresion", " 6 Logistic Regression - Ex: Bronchopulmonary Dysplasia in Premature Infants example walk through: https://stats.idre.ucla.edu/r/dae/logit-regression/ info: https://onlinecourses.science.psu.edu/stat504/node/216/ sjPlot::tab_model (HTML only) http://www.strengejacke.de/sjPlot/articles/sjtlm.html#changing-summary-style-and-content finafit https://www.r-bloggers.com/elegant-regression-results-tables-and-plots-in-r-the-finalfit-package/ library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(pscl) # psudo R-squared function 6.1 Background Simple example demonstrating basic modeling approach: Data on Bronchopulmonary Dysplasia (BPD) from 223 low birth weight infants (weighing less than 1750 grams). 6.1.1 Source Data courtesy of Dr. Linda Van Marter. 6.1.2 Reference Van Marter, L.J., Leviton, A., Kuban, K.C.K., Pagano, M. &amp; Allred, E.N. (1990). Maternal glucocorticoid therapy and reduced risk of bronchopulmonary dysplasia. Pediatrics, 86, 331-336. The data are from a study of low birth weight infants in a neonatal intensive care unit. The study was designed to examine the development of bronchopulmonary dysplasia (BPD), a chronic lung disease, in a sample of 223 infants weighing less than 1750 grams. The response variable is binary, denoting whether an infant develops BPD by day 28 of life (where BPD is defined by both oxygen requirement and compatible chest radiograph). 6.1.3 Variables bpd(0 [N],1 [Y]) brthwght (grams) gestage (weeks) toxemia (0 [N] ,1 [Y]) in mother bpd_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/Regression/VanMarter_%20BPD.txt&quot;, header = TRUE, strip.white = TRUE) n &lt;- nrow(bpd_raw) n [1] 223 str(bpd_raw) &#39;data.frame&#39;: 223 obs. of 4 variables: $ bpd : int 1 0 1 0 0 0 1 0 1 1 ... $ brthwght: int 850 1500 1360 960 1560 1120 810 1620 1000 700 ... $ gestage : int 27 33 32 35 33 29 28 32 30 26 ... $ toxemia : int 0 0 0 1 0 0 0 0 0 0 ... head(bpd_raw) # A tibble: 6 x 4 bpd brthwght gestage toxemia &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 850 27 0 2 0 1500 33 0 3 1 1360 32 0 4 0 960 35 1 5 0 1560 33 0 6 0 1120 29 0 bpd_clean &lt;- bpd_raw %&gt;% dplyr::mutate(toxemia = factor(toxemia, levels = c(0, 1), labels = c(&quot;No&quot;, &quot;Yes&quot;))) summary(bpd_clean) bpd brthwght gestage toxemia Min. :0.0000 Min. : 450 Min. :25.00 No :194 1st Qu.:0.0000 1st Qu.: 895 1st Qu.:28.00 Yes: 29 Median :0.0000 Median :1140 Median :30.00 Mean :0.3408 Mean :1173 Mean :30.09 3rd Qu.:1.0000 3rd Qu.:1465 3rd Qu.:32.00 Max. :1.0000 Max. :1730 Max. :37.00 6.2 Logistic Regresion 6.2.1 Fit the Models fit_glm_0 &lt;- glm(bpd ~ 1, data = bpd_clean, family = binomial(link = &quot;logit&quot;)) fit_glm_1 &lt;- glm(bpd ~ I(brthwght/100) + gestage + toxemia, data = bpd_clean, family = binomial(link = &quot;logit&quot;)) 6.2.1.1 Log Likelihood logLik(fit_glm_0) &#39;log Lik.&#39; -143.07 (df=1) logLik(fit_glm_1) &#39;log Lik.&#39; -101.8538 (df=4) 6.2.1.2 Deviance = -1 times the Log Likelihood deviance(fit_glm_0) [1] 286.14 deviance(fit_glm_1) [1] 203.7075 6.2.2 GoF Measures 6.2.2.1 AIC AIC(fit_glm_0) [1] 288.14 AIC(fit_glm_1) [1] 211.7075 6.2.2.2 Logistic R^2 http://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/ Technically, \\(R^2\\) cannot be computed the same way in logistic regression as it is in OLS regression. The \\(pseudo-R^2\\), in logistic regression, is defined as \\(1−\\frac{L_1}{L_0}\\), where \\(L_0\\) represents the log likelihood for the “constant-only” or NULL model and \\(L_1\\) is the log likelihood for the full model with constant and predictors. 6.2.2.3 McFadden’s pseud- R^2 \\[ R^2_{McF} = 1 - \\frac{L_1}{L_0} \\] MFR2 &lt;- 1 - (logLik(fit_glm_1)/logLik(fit_glm_0)) MFR2 &#39;log Lik.&#39; 0.2880843 (df=4) 6.2.2.4 Cox &amp; Snell \\(l = e^{L}\\), sinc \\(L\\) is the log of the likelihood and \\(l\\) is the likelihood…\\(log(l) = L\\) \\[ R^2_{CS} = 1 - \\Bigg( \\frac{l_0}{l_1} \\Bigg) ^{2 \\backslash n} \\\\ n = \\text{sample size} \\] CSR2 &lt;- 1 - (exp(logLik(fit_glm_0))/exp(logLik(fit_glm_1)))^(2/n) CSR2 &#39;log Lik.&#39; 0.3090253 (df=1) 6.2.2.5 Nagelkerke or Cragg and Uhler’s \\[ R^2_{Nag} = \\frac{1 - \\Bigg( \\frac{l_0}{l_1} \\Bigg) ^{2 \\backslash n}} {1 - \\Big( l_0 \\Big) ^{2 \\backslash n}} \\] NR2 &lt;- CSR2 / (1 - exp(logLik(fit_glm_0))^(2/n)) NR2 &#39;log Lik.&#39; 0.4275191 (df=1) 6.2.2.6 Several with the pscl::pR2() function pscl::pR2(fit_glm_1) llh llhNull G2 McFadden r2ML -101.8537711 -143.0699809 82.4324196 0.2880843 0.3090253 r2CU 0.4275191 6.2.3 Parameter Estimates 6.2.3.1 Logit Scale fit_glm_1 %&gt;% coef() (Intercept) I(brthwght/100) gestage toxemiaYes 13.9360826 -0.2643578 -0.3885357 -1.3437865 6.2.3.2 Odds Ratio Scale fit_glm_1 %&gt;% coef() %&gt;% exp() (Intercept) I(brthwght/100) gestage toxemiaYes 1.128142e+06 7.676988e-01 6.780490e-01 2.608561e-01 6.2.3.3 Confidence Intervals - OR sclae fit_glm_1 %&gt;% confint() %&gt;% exp() 2.5 % 97.5 % (Intercept) 4.402379e+03 5.591330e+08 I(brthwght/100) 6.511832e-01 8.967757e-01 gestage 5.351280e-01 8.414808e-01 toxemiaYes 7.314875e-02 8.078916e-01 6.2.4 Significance of Terms 6.2.4.1 Likelihood Ratio Test of all Nested Models anova(fit_glm_0, fit_glm_1) # A tibble: 2 x 4 `Resid. Df` `Resid. Dev` Df Deviance &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 222 286. NA NA 2 219 204. 3 82.4 6.2.4.2 Sequential LRTs: for adding one variable at a time anova(fit_glm_1, test = &quot;Chisq&quot;) # A tibble: 4 x 5 Df Deviance `Resid. Df` `Resid. Dev` `Pr(&gt;Chi)` &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 NA NA 222 286. NA 2 1 62.4 221 224. 2.78e-15 3 1 14.5 220 209. 1.41e- 4 4 1 5.52 219 204. 1.88e- 2 6.2.5 Parameter Estimates 6.2.5.1 Raw Output summary(fit_glm_1) Call: glm(formula = bpd ~ I(brthwght/100) + gestage + toxemia, family = binomial(link = &quot;logit&quot;), data = bpd_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.8400 -0.7029 -0.3352 0.7261 2.9902 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 13.93608 2.98255 4.673 2.98e-06 *** I(brthwght/100) -0.26436 0.08123 -3.254 0.00114 ** gestage -0.38854 0.11489 -3.382 0.00072 *** toxemiaYes -1.34379 0.60750 -2.212 0.02697 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 286.14 on 222 degrees of freedom Residual deviance: 203.71 on 219 degrees of freedom AIC: 211.71 Number of Fisher Scoring iterations: 5 6.2.5.2 sjPlot - HTML tables JUST HTML for now… Parameters Exponentiated: sjPlot::tab_model(fit_glm_1) bpd Predictors Odds Ratios CI p (Intercept) 1128141.99 3262.96 – 390045835.72 &lt;0.001 I(brthwght/100) 0.77 0.65 – 0.90 0.001 gestage 0.68 0.54 – 0.85 0.001 Yes 0.26 0.08 – 0.86 0.027 Observations 223 R2 Tjur 0.346 sjPlot::tab_model(fit_glm_1, emph.p = TRUE, pred.labels = c(&quot;(Intercept)&quot;, &quot;Birthweight, 100 grams&quot;, &quot;Gestational Age, week&quot;, &quot;Mother had Toxemia&quot;)) bpd Predictors Odds Ratios CI p (Intercept) 1128141.99 3262.96 – 390045835.72 &lt;0.001 Birthweight, 100 grams 0.77 0.65 – 0.90 0.001 Gestational Age, week 0.68 0.54 – 0.85 0.001 Mother had Toxemia 0.26 0.08 – 0.86 0.027 Observations 223 R2 Tjur 0.346 6.2.5.3 texreg default texreg::screenreg(fit_glm_1) ============================ Model 1 ---------------------------- (Intercept) 13.94 *** (2.98) I(brthwght/100) -0.26 ** (0.08) gestage -0.39 *** (0.11) toxemiaYes -1.34 * (0.61) ---------------------------- AIC 211.71 BIC 225.34 Log Likelihood -101.85 Deviance 203.71 Num. obs. 223 ============================ *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 6.2.5.4 texreg Confidence Intervals on Logit Scale texreg::screenreg(fit_glm_1, ci.force = TRUE) =============================== Model 1 ------------------------------- (Intercept) 13.94 * [ 8.09; 19.78] I(brthwght/100) -0.26 * [-0.42; -0.11] gestage -0.39 * [-0.61; -0.16] toxemiaYes -1.34 * [-2.53; -0.15] ------------------------------- AIC 211.71 BIC 225.34 Log Likelihood -101.85 Deviance 203.71 Num. obs. 223 =============================== * 0 outside the confidence interval 6.2.5.5 texreg exponentiate the betas (SE are not exp) texreg::screenreg(fit_glm_1, override.coef = list(fit_glm_1 %&gt;% coef() %&gt;% exp())) =============================== Model 1 ------------------------------- (Intercept) 1128141.99 *** (2.98) I(brthwght/100) 0.77 ** (0.08) gestage 0.68 *** (0.11) toxemiaYes 0.26 * (0.61) ------------------------------- AIC 211.71 BIC 225.34 Log Likelihood -101.85 Deviance 203.71 Num. obs. 223 =============================== *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 6.2.6 Marginal Model Plot 6.2.6.1 Manually Specified summary(bpd_clean) bpd brthwght gestage toxemia Min. :0.0000 Min. : 450 Min. :25.00 No :194 1st Qu.:0.0000 1st Qu.: 895 1st Qu.:28.00 Yes: 29 Median :0.0000 Median :1140 Median :30.00 Mean :0.3408 Mean :1173 Mean :30.09 3rd Qu.:1.0000 3rd Qu.:1465 3rd Qu.:32.00 Max. :1.0000 Max. :1730 Max. :37.00 effects::Effect(focal.predictors = c(&quot;brthwght&quot;, &quot;toxemia&quot;, &quot;gestage&quot;), mod = fit_glm_1, xlevels = list(brthwght = seq(from = 450, to = 1730, by = 10), gestage = c(28, 30, 32))) %&gt;% data.frame() %&gt;% dplyr::mutate(gestage = factor(gestage)) %&gt;% ggplot(aes(x = brthwght, y = fit)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = toxemia), alpha = .2) + geom_line(aes(linetype = toxemia, color = toxemia), size = 1) + facet_grid(. ~ gestage, labeller = label_both) + theme_bw() effects::Effect(focal.predictors = c(&quot;brthwght&quot;, &quot;toxemia&quot;, &quot;gestage&quot;), mod = fit_glm_1, xlevels = list(brthwght = seq(from = 450, to = 1730, by = 10), gestage = c(28, 30, 32))) %&gt;% data.frame() %&gt;% dplyr::mutate(gestage = factor(gestage)) %&gt;% ggplot(aes(x = brthwght, y = fit)) + geom_line(aes(linetype = toxemia, color = toxemia), size = 1) + facet_grid(. ~ gestage, labeller = label_both) + theme_bw() 6.2.7 Residual Diagnostics 6.2.7.1 sjPlot sjPlot::plot_model(fit_glm_1, type = &quot;diag&quot;) NULL 6.2.7.2 base R graphics plot(fit_glm_1) "],
["logistic-regression-ex-maternal-risk-factor-for-low-birth-weight-delivery.html", "7 Logistic Regression - Ex: Maternal Risk Factor for Low Birth Weight Delivery 7.1 Background 7.2 Exploratory Data Analysis 7.3 Logistic Regression - Simple, un-adjusted 7.4 Logistic Regression - Multivariate, with Main Effects Only 7.5 Logistic Regression - Multivariate, with Interactions 7.6 Logistic Regression - Multivariate, Simplify 7.7 Logistic Regression - Multivariate, Final Model", " 7 Logistic Regression - Ex: Maternal Risk Factor for Low Birth Weight Delivery library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(sjPlot) # Quick plots and tables for models library(pscl) # psudo R-squared function library(glue) # Interpreted String Literals 7.1 Background More complex example demonstrating modeling decisions Another set of data from a study investigating predictors of low birth weight id infant’s unique identification number Dependent variable (DV) or outcome low Low birth weight (outcome) (0 = birth weight &gt;2500 g (normal), 1 = birth weight &lt; 2500 g (low)) bwt actual infant birth weight in grams Indepdentend variables (IV) or predictors age Age of mother, in years lwt Mother’s weight at last menstrual period, in pounds race Race: 1 = White, 2 = Black, 3 = Other smoke Smoking status during pregnancy:1 = Yes, 0 = No ptl History of premature labor: 0 = None, 1 = One, 2 = two, 3 = three ht History of hypertension: 1 = Yes, 0 = No ui Uterine irritability: 1 = Yes, 0 = No ftv Number of physician visits in 1st trimester: 0 = None, 1 = One, … 6 = six 7.1.1 Raw Dataset The data is saved in a text file (.txt) without any labels. lowbwt_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/Regression/lowbwt.txt&quot;, header = TRUE, sep = &quot;&quot;, na.strings = &quot;NA&quot;, dec = &quot;.&quot;, strip.white = TRUE) tibble::glimpse(lowbwt_raw) Observations: 189 Variables: 11 $ id &lt;int&gt; 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, ... $ low &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... $ age &lt;int&gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, ... $ lwt &lt;int&gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 15... $ race &lt;int&gt; 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3,... $ smoke &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,... $ ptl &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,... $ ht &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,... $ ui &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,... $ ftv &lt;int&gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0,... $ bwt &lt;int&gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 26... 7.1.2 Declare Factors lowbwt_clean &lt;- lowbwt_raw %&gt;% dplyr::mutate(id = factor(id)) %&gt;% dplyr::mutate(low = factor(low, levels = c(0, 1), labels = c(&quot;birth weight &gt;2500 g (normal)&quot;, &quot;birth weight &lt; 2500 g (low)&quot;))) %&gt;% dplyr::mutate(race = factor(race, levels = 1:3, labels = c(&quot;White&quot;, &quot;Black&quot;, &quot;Other&quot;))) %&gt;% dplyr::mutate(ptl_any = as.numeric(ptl &gt; 0)) %&gt;% # colapse into 0 = none vs. 1 = at least one dplyr::mutate(ptl = factor(ptl)) %&gt;% # declare the number of pre-term labors to be a factor: 0, 1, 2, 3 dplyr::mutate_at(vars(smoke, ht, ui, ptl_any), # declare all there variables to be factors with the same two levels factor, levels = 0:1, labels = c(&quot;No&quot;, &quot;Yes&quot;)) Display the structure of the ‘clean’ version of the dataset str(lowbwt_clean) &#39;data.frame&#39;: 189 obs. of 12 variables: $ id : Factor w/ 189 levels &quot;4&quot;,&quot;10&quot;,&quot;11&quot;,..: 60 61 62 63 64 65 66 67 68 69 ... $ low : Factor w/ 2 levels &quot;birth weight &gt;2500 g (normal)&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ age : int 19 33 20 21 18 21 22 17 29 26 ... $ lwt : int 182 155 105 108 107 124 118 103 123 113 ... $ race : Factor w/ 3 levels &quot;White&quot;,&quot;Black&quot;,..: 2 3 1 1 1 3 1 3 1 1 ... $ smoke : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 2 2 2 1 1 1 2 2 ... $ ptl : Factor w/ 4 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ ht : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ ui : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 2 2 1 1 1 1 1 ... $ ftv : int 0 3 1 2 0 0 1 1 1 0 ... $ bwt : int 2523 2551 2557 2594 2600 2622 2637 2637 2663 2665 ... $ ptl_any: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... 7.2 Exploratory Data Analysis # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; lowbwt_clean %&gt;% furniture::table1(&quot;Age, years&quot; = age, &quot;Weight, pounds&quot; = lwt, &quot;Race&quot; = race, &quot;Smoking During pregnancy&quot; = smoke, &quot;History of Premature Labor, any&quot; = ptl_any, &quot;History of Premature Labor, number&quot; = ptl, &quot;History of Hypertension&quot; = ht, &quot;Uterince Irritability&quot; = ui, &quot;1st Tri Dr Visits&quot; = ftv, splitby = ~ low, test = TRUE, output = &quot;html&quot;) birth weight &gt;2500 g (normal) birth weight &lt; 2500 g (low) P-Value n = 130 n = 59 Age, years 0.103 23.7 (5.6) 22.3 (4.5) Weight, pounds 0.02 133.3 (31.7) 122.1 (26.6) Race 0.082 White 73 (56.2%) 23 (39%) Black 15 (11.5%) 11 (18.6%) Other 42 (32.3%) 25 (42.4%) Smoking During pregnancy 0.04 No 86 (66.2%) 29 (49.2%) Yes 44 (33.8%) 30 (50.8%) History of Premature Labor, any &lt;.001 No 118 (90.8%) 41 (69.5%) Yes 12 (9.2%) 18 (30.5%) History of Premature Labor, number &lt;.001 0 118 (90.8%) 41 (69.5%) 1 8 (6.2%) 16 (27.1%) 2 3 (2.3%) 2 (3.4%) 3 1 (0.8%) 0 (0%) History of Hypertension 0.076 No 125 (96.2%) 52 (88.1%) Yes 5 (3.8%) 7 (11.9%) Uterince Irritability 0.035 No 116 (89.2%) 45 (76.3%) Yes 14 (10.8%) 14 (23.7%) 1st Tri Dr Visits 0.389 0.8 (1.1) 0.7 (1.0) 7.3 Logistic Regression - Simple, un-adjusted low1.age &lt;- glm(low ~ age, family=binomial(logit), data=lowbwt_clean) low1.lwt &lt;- glm(low ~ lwt, family=binomial(logit), data=lowbwt_clean) low1.race &lt;- glm(low ~ race, family=binomial(logit), data=lowbwt_clean) low1.smoke &lt;- glm(low ~ smoke, family=binomial(logit), data=lowbwt_clean) low1.ptl &lt;- glm(low ~ ptl_any, family=binomial(logit), data=lowbwt_clean) low1.ht &lt;- glm(low ~ ht, family=binomial(logit), data=lowbwt_clean) low1.ui &lt;- glm(low ~ ui, family=binomial(logit), data=lowbwt_clean) low1.ftv &lt;- glm(low ~ ftv, family=binomial(logit), data=lowbwt_clean) Note: the parameter estimates here are for the LOGIT scale, not the odds ration (OR) or even the probability. # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(low1.age, low1.lwt, low1.race, low1.smoke), custom.model.names = c(&quot;Age&quot;, &quot;Weight&quot;, &quot;Race&quot;, &quot;Smoker&quot;), caption = &quot;Simple, Unadjusted Logistic Regression: Models 1-4&quot;, caption.above = TRUE) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Simple, Unadjusted Logistic Regression: Models 1-4 Age Weight Race Smoker (Intercept) 0.38 1.00 -1.15*** -1.09*** (0.73) (0.79) (0.24) (0.21) age -0.05 (0.03) lwt -0.01* (0.01) raceBlack 0.84 (0.46) raceOther 0.64 (0.35) smokeYes 0.70* (0.32) AIC 235.91 232.69 235.66 233.80 BIC 242.40 239.17 245.39 240.29 Log Likelihood -115.96 -114.35 -114.83 -114.90 Deviance 231.91 228.69 229.66 229.80 Num. obs. 189 189 189 189 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(low1.ptl, low1.ht, low1.ui, low1.ftv), custom.model.names = c(&quot;Pre-Labor&quot;, &quot;Hypertension&quot;, &quot;Uterine&quot;, &quot;Visits&quot;), caption = &quot;Simple, Unadjusted Logistic Regression: Models 5-8&quot;, caption.above = TRUE) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Simple, Unadjusted Logistic Regression: Models 5-8 Pre-Labor Hypertension Uterine Visits (Intercept) -1.06*** -0.88*** -0.95*** -0.69*** (0.18) (0.17) (0.18) (0.19) ptl_anyYes 1.46*** (0.41) htYes 1.21* (0.61) uiYes 0.95* (0.42) ftv -0.14 (0.16) AIC 225.90 234.65 233.60 237.90 BIC 232.38 241.13 240.08 244.38 Log Likelihood -110.95 -115.32 -114.80 -116.95 Deviance 221.90 230.65 229.60 233.90 Num. obs. 189 189 189 189 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 7.4 Logistic Regression - Multivariate, with Main Effects Only Main-effects multiple logistic regression model low1_1 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui, family = binomial(logit), data = lowbwt_clean) summary(low1_1) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6459 -0.7992 -0.5103 0.9388 2.2018 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.63691 1.23028 0.518 0.60467 age -0.03775 0.03781 -0.998 0.31808 lwt -0.01491 0.00704 -2.118 0.03419 * raceBlack 1.21274 0.53248 2.278 0.02275 * raceOther 0.80412 0.44843 1.793 0.07294 . smokeYes 0.84640 0.40806 2.074 0.03806 * ptl_anyYes 1.22175 0.46301 2.639 0.00832 ** htYes 1.83869 0.70324 2.615 0.00893 ** uiYes 0.71113 0.46311 1.536 0.12465 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 196.83 on 180 degrees of freedom AIC: 214.83 Number of Fisher Scoring iterations: 4 7.5 Logistic Regression - Multivariate, with Interactions Before removing non-significant main effects, test plausible interactions Try interactions between age and lwt, age and smoke, lwt and smoke, 1 at a time 7.5.1 Age and Weight low1_2 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:lwt, family = binomial(logit), data = lowbwt_clean) summary(low1_2) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:lwt, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6426 -0.8004 -0.5163 0.9400 2.1989 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.1396293 4.0443041 0.282 0.77811 age -0.0597273 0.1724339 -0.346 0.72906 lwt -0.0188377 0.0309225 -0.609 0.54240 raceBlack 1.2071359 0.5341716 2.260 0.02383 * raceOther 0.7977750 0.4505381 1.771 0.07661 . smokeYes 0.8433655 0.4083953 2.065 0.03892 * ptl_anyYes 1.2260463 0.4642795 2.641 0.00827 ** htYes 1.8389418 0.7034162 2.614 0.00894 ** uiYes 0.7142688 0.4642468 1.539 0.12391 age:lwt 0.0001718 0.0013146 0.131 0.89600 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 196.82 on 179 degrees of freedom AIC: 216.82 Number of Fisher Scoring iterations: 5 7.5.1.1 Compare Model Fits vs. Likelihood Ratio Test anova(low1_1, low1_2, test = &#39;Chi&#39;) # A tibble: 2 x 5 `Resid. Df` `Resid. Dev` Df Deviance `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 180 197. NA NA NA 2 179 197. 1 0.0170 0.896 7.5.1.2 Type II Analysis of Deviance Table Anova(low1_2, test = &#39;LR&#39;) # A tibble: 8 x 3 `LR Chisq` Df `Pr(&gt;Chisq)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1.02 1 0.313 2 5.00 1 0.0254 3 6.27 2 0.0436 4 4.38 1 0.0364 5 7.13 1 0.00758 6 7.18 1 0.00738 7 2.33 1 0.127 8 0.0170 1 0.896 7.5.1.3 Type III Analysis of Deviance Table Anova(low1_2, test = &#39;LR&#39;, type = &#39;III&#39;) # A tibble: 8 x 3 `LR Chisq` Df `Pr(&gt;Chisq)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.119 1 0.730 2 0.372 1 0.542 3 6.27 2 0.0436 4 4.38 1 0.0364 5 7.13 1 0.00758 6 7.18 1 0.00738 7 2.33 1 0.127 8 0.0170 1 0.896 7.5.2 Age and Smoking low1_3 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:smoke, family = binomial(logit), data = lowbwt_clean) summary(low1_3) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:smoke, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.7020 -0.8016 -0.4970 0.9055 2.2124 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.311029 1.483078 0.884 0.37670 age -0.068293 0.053865 -1.268 0.20485 lwt -0.014701 0.007001 -2.100 0.03575 * raceBlack 1.126482 0.543684 2.072 0.03827 * raceOther 0.768241 0.451199 1.703 0.08863 . smokeYes -0.601286 1.782234 -0.337 0.73583 ptl_anyYes 1.197295 0.460992 2.597 0.00940 ** htYes 1.860851 0.705311 2.638 0.00833 ** uiYes 0.783999 0.472451 1.659 0.09703 . age:smokeYes 0.063744 0.076634 0.832 0.40552 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 196.14 on 179 degrees of freedom AIC: 216.14 Number of Fisher Scoring iterations: 5 7.5.2.1 Compare Model Fits vs. Likelihood Ratio Test anova(low1_1, low1_3, test = &#39;Chi&#39;) # A tibble: 2 x 5 `Resid. Df` `Resid. Dev` Df Deviance `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 180 197. NA NA NA 2 179 196. 1 0.699 0.403 7.5.3 Weight and Smoking low1_4 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui + lwt:smoke, family = binomial(logit), data = lowbwt_clean) summary(low1_4) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui + lwt:smoke, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6816 -0.7874 -0.5251 0.8876 2.2365 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.74840 1.59439 1.097 0.27282 age -0.03768 0.03809 -0.989 0.32259 lwt -0.02362 0.01077 -2.193 0.02828 * raceBlack 1.23110 0.53358 2.307 0.02104 * raceOther 0.71646 0.45183 1.586 0.11281 smokeYes -1.13566 1.75557 -0.647 0.51770 ptl_anyYes 1.26472 0.46488 2.721 0.00652 ** htYes 1.74326 0.70738 2.464 0.01373 * uiYes 0.80121 0.47044 1.703 0.08855 . lwt:smokeYes 0.01555 0.01344 1.157 0.24740 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 195.44 on 179 degrees of freedom AIC: 215.44 Number of Fisher Scoring iterations: 5 7.5.3.1 Compare Model Fits vs. Likelihood Ratio Test anova(low1_1, low1_4, test = &#39;Chi&#39;) # A tibble: 2 x 5 `Resid. Df` `Resid. Dev` Df Deviance `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 180 197. NA NA NA 2 179 195. 1 1.39 0.238 7.6 Logistic Regression - Multivariate, Simplify No interactions are significant Remove non-significant main effects 7.6.1 Remove the least significant perdictor: ui low1_5 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht, family = binomial(logit), data = lowbwt_clean) summary(low1_5) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6533 -0.8202 -0.5299 0.9709 2.1982 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.924910 1.202693 0.769 0.44187 age -0.042784 0.037567 -1.139 0.25475 lwt -0.015436 0.007044 -2.191 0.02843 * raceBlack 1.168452 0.532577 2.194 0.02824 * raceOther 0.814620 0.442740 1.840 0.06578 . smokeYes 0.858332 0.404787 2.120 0.03397 * ptl_anyYes 1.333970 0.457573 2.915 0.00355 ** htYes 1.740511 0.703104 2.475 0.01331 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 199.15 on 181 degrees of freedom AIC: 215.15 Number of Fisher Scoring iterations: 4 7.7 Logistic Regression - Multivariate, Final Model Since the mother’s age is theoretically a meaningful variable, it should probably be retained. Revise so that age is interpreted in 5-year and lwt in 20 lb increments and the intercept has meaning. low1_6 &lt;- glm(low ~ I((age - 20)/5) + I((lwt - 125)/20) + race + smoke + ptl_any + ht, family = binomial(logit), data = lowbwt_clean) summary(low1_6) Call: glm(formula = low ~ I((age - 20)/5) + I((lwt - 125)/20) + race + smoke + ptl_any + ht, family = binomial(logit), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6533 -0.8202 -0.5299 0.9709 2.1982 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.8603 0.4092 -4.546 5.48e-06 *** I((age - 20)/5) -0.2139 0.1878 -1.139 0.25475 I((lwt - 125)/20) -0.3087 0.1409 -2.191 0.02843 * raceBlack 1.1685 0.5326 2.194 0.02824 * raceOther 0.8146 0.4427 1.840 0.06578 . smokeYes 0.8583 0.4048 2.120 0.03397 * ptl_anyYes 1.3340 0.4576 2.915 0.00355 ** htYes 1.7405 0.7031 2.475 0.01331 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 199.15 on 181 degrees of freedom AIC: 215.15 Number of Fisher Scoring iterations: 4 7.7.1 Several \\(R^2\\) measures with the pscl::pR2() function pscl::pR2(low1_6) llh llhNull G2 McFadden r2ML -99.5757045 -117.3359981 35.5205871 0.1513627 0.1713353 r2CU 0.2409463 7.7.2 Parameter Estiamtes Table 7.7.2.1 Using texreg::screenreg() Default: parameters are in terms of the ‘logit’ or log odds ratio # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(low1_6) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Statistical models Model 1 (Intercept) -1.86*** (0.41) I((age - 20)/5) -0.21 (0.19) I((lwt - 125)/20) -0.31* (0.14) raceBlack 1.17* (0.53) raceOther 0.81 (0.44) smokeYes 0.86* (0.40) ptl_anyYes 1.33** (0.46) htYes 1.74* (0.70) AIC 215.15 BIC 241.09 Log Likelihood -99.58 Deviance 199.15 Num. obs. 189 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 The texreg package uses an intermediate function called extract() to extract information for the model and then put it in the right places in the table. We can invervene by writing our own extract_exp() function to use instead. extract_exp &lt;- function(fit_glm){ beta = coef(fit_glm) betaci = confint(fit_glm) fit_glm_exp = texreg::extract(fit_glm) fit_glm_exp@coef = exp(beta) fit_glm_exp@ci.low = exp(betaci[, 1]) fit_glm_exp@ci.up = exp(betaci[, 2]) return(fit_glm_exp) } # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(extract_exp(low1_6), custom.coef.names = c(&quot;BL: 125 lb, 20 yr old White Mother&quot;, &quot;Additional 5 years older&quot;, &quot;Additional 20 lbs pre-pregnancy&quot;, &quot;Race: Black vs. White&quot;, &quot;Race: Other vs. White&quot;, &quot;Smoking During pregnancy&quot;, &quot;History of Any Premature Labor&quot;, &quot;History of Hypertension&quot;), custom.model.names = &quot;OR, Low Birth Weight&quot;, single.row = TRUE, custom.note = &quot;* The value of &#39;1&#39; is outside the confidence interval for the OR&quot;) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Statistical models OR, Low Birth Weight BL: 125 lb, 20 yr old White Mother 0.16 [0.07; 0.33]* Additional 5 years older 0.81 [0.55; 1.16]* Additional 20 lbs pre-pregnancy 0.73 [0.55; 0.95]* Race: Black vs. White 3.22 [1.13; 9.31]* Race: Other vs. White 2.26 [0.96; 5.50]* Smoking During pregnancy 2.36 [1.08; 5.32]* History of Any Premature Labor 3.80 [1.57; 9.53]* History of Hypertension 5.70 [1.49; 24.76]* AIC 215.15 BIC 241.09 Log Likelihood -99.58 Deviance 199.15 Num. obs. 189 * The value of ‘1’ is outside the confidence interval for the OR 7.7.2.2 Using sjPlot::tab_model() Parameters Exponentiated: sjPlot::tab_model(low1_6, emph.p = TRUE, pred.labels = c(&quot;BL: 125 lb, 20 yr old White Mother&quot;, &quot;Additional 5 years old&quot;, &quot;Additional 20 lbs pre-pregnancy&quot;, &quot;Race: Black vs. White&quot;, &quot;Race: Other vs. White&quot;, &quot;Smoking During pregnancy&quot;, &quot;History of Any Premature Labor&quot;, &quot;History of Hypertension&quot;)) low Predictors Odds Ratios CI p BL: 125 lb, 20 yr old White Mother 0.16 0.07 – 0.35 &lt;0.001 Additional 5 years old 0.81 0.56 – 1.17 0.255 Additional 20 lbs pre-pregnancy 0.73 0.56 – 0.97 0.028 Race: Black vs. White 3.22 1.13 – 9.14 0.028 Race: Other vs. White 2.26 0.95 – 5.38 0.066 Smoking During pregnancy 2.36 1.07 – 5.22 0.034 History of Any Premature Labor 3.80 1.55 – 9.31 0.004 History of Hypertension 5.70 1.44 – 22.61 0.013 Observations 189 R2 Tjur 0.180 7.7.3 Marginal Model Plot 7.7.3.1 Focus on: Mother’s Age, weight, and race effects::Effect(focal.predictors = c(&quot;age&quot;, &quot;lwt&quot;, &quot;race&quot;), mod = low1_6, xlevels = list(age = c(20, 30, 40), lwt = seq(from = 80, to = 250, by = 5))) %&gt;% data.frame() %&gt;% dplyr::mutate(age_labels = glue(&quot;Mother Age: {age}&quot;)) %&gt;% ggplot(aes(x = lwt, y = fit)) + geom_line(aes(color = race, linetype = race), size = 1) + theme_bw() + facet_grid(.~ age_labels) + labs(title = &quot;Risk of Low Birth Weight&quot;, subtitle = &quot;Illustates risk given mother is a non-smoker, without a history of pre-term labor or hypertension&quot;, x = &quot;Mother&#39;s Weight Pre-Pregnancy, pounds&quot;, y = &quot;Predicted Probability\\nBaby has Low Birth Weight (&lt; 2500 grams)&quot;, color = &quot;Mother&#39;s Race&quot;, linetype = &quot;Mother&#39;s Race&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + scale_linetype_manual(values = c(&quot;longdash&quot;, &quot;dotted&quot;, &quot;solid&quot;)) + scale_color_manual(values = c( &quot;coral2&quot;, &quot;dodger blue&quot;, &quot;gray50&quot;)) 7.7.3.2 Focus on: Mother’s weight and smothing status during pregnancy, as well as history of any per-term labor and hypertension effects::Effect(focal.predictors = c(&quot;lwt&quot;, &quot;smoke&quot;, &quot;ptl_any&quot;, &quot;ht&quot;), fixed.predictors = list(age = 20), mod = low1_6, xlevels = list(lwt = seq(from = 80, to = 250, by = 5))) %&gt;% data.frame() %&gt;% dplyr::mutate(smoke = forcats::fct_rev(smoke)) %&gt;% dplyr::mutate(ptl_any_labels = glue(&quot;History of Preterm Labor: {ptl_any}&quot;)) %&gt;% dplyr::mutate(ht_labels = glue(&quot;History of Hypertension: {ht}&quot;) %&gt;% forcats::fct_rev()) %&gt;% ggplot(aes(x = lwt, y = fit)) + geom_line(aes(color = smoke, linetype = smoke), size = 1) + theme_bw() + facet_grid(ht_labels ~ ptl_any_labels) + labs(title = &quot;Risk of Low Birth Weight&quot;, subtitle = &quot;Illustates risk given the mother is 20 years old and white&quot;, x = &quot;Mother&#39;s Weight Pre-Pregnancy, pounds&quot;, y = &quot;Predicted Probability\\nBaby has Low Birth Weight (&lt; 2500 grams)&quot;, color = &quot;Mother Smoked&quot;, linetype = &quot;Mother Smoked&quot;) + theme(legend.position = c(1, .5), legend.justification = c(1.1, 1.15), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + scale_linetype_manual(values = c(&quot;longdash&quot;, &quot;solid&quot;)) + scale_color_manual(values = c( &quot;coral2&quot;, &quot;dodger blue&quot;)) "],
["count-outcome-regression-ex-possion-and-negative-binomial.html", "8 Count Outcome Regression - Ex: POssion and Negative Binomial 8.1 Background 8.2 Exploratory Data Analysis 8.3 Poisson Reression 8.4 Negative Binomial Regression 8.5 Zero Inflated Poisson 8.6 Zero Inflated Negative Binomial", " 8 Count Outcome Regression - Ex: POssion and Negative Binomial library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(sjPlot) # Quick plots and tables for models library(glue) # Interpreted String Literals library(DescTools) # Tools for Descriptive Statistics library(texreghelpr) # Helper Functions for generalized models library(pscl) # Political Science Computational Laboratory (ZIP) 8.1 Background 8.1.1 Raw Dataset data_gss &lt;- haven::read_spss(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/Hoffmann_datasets/gss.sav&quot;) %&gt;% haven::as_factor() data_gss # A tibble: 2,903 x 20 id marital divorce childs age income polviews fund attend spanking &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 402 divorc~ yes 2 54 10 middle ~ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2 1473 widowed no 0 24 2 slight ~ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3 1909 widowed no 7 75 NA extreme~ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4 334 widowed yes 2 41 NA middle ~ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5 1751 married no 2 37 12 extreme~ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 6 456 divorc~ yes 0 40 NA extreme~ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 7 292 never ~ no 2 36 9 &lt;NA&gt; &lt;NA&gt; nearl~ &lt;NA&gt; 8 2817 married yes 3 33 NA middle ~ &lt;NA&gt; sever~ &lt;NA&gt; 9 2810 never ~ no 0 18 NA middle ~ &lt;NA&gt; once ~ &lt;NA&gt; 10 2232 widowed no 2 35 6 middle ~ &lt;NA&gt; sever~ &lt;NA&gt; # ... with 2,893 more rows, and 10 more variables: totrelig &lt;dbl&gt;, # sei &lt;dbl&gt;, pasei &lt;dbl&gt;, volteer &lt;dbl&gt;, female &lt;fct&gt;, nonwhite &lt;fct&gt;, # prayer &lt;fct&gt;, educate &lt;dbl&gt;, volrelig &lt;fct&gt;, polview1 &lt;fct&gt; 8.2 Exploratory Data Analysis 8.2.1 Entire Sample data_gss %&gt;% furniture::tableF(volteer) -------------------------------------- volteer Freq CumFreq Percent CumPerc 0 2376 2376 81.85% 81.85% 1 286 2662 9.85% 91.70% 2 133 2795 4.58% 96.28% 3 64 2859 2.20% 98.48% 4 19 2878 0.65% 99.14% 5 11 2889 0.38% 99.52% 6 7 2896 0.24% 99.76% 7 6 2902 0.21% 99.97% 9 1 2903 0.03% 100.00% -------------------------------------- data_gss %&gt;% dplyr::select(volteer) %&gt;% summary() volteer Min. :0.0000 1st Qu.:0.0000 Median :0.0000 Mean :0.3334 3rd Qu.:0.0000 Max. :9.0000 data_gss %&gt;% ggplot(aes(volteer)) + geom_histogram() 8.2.2 By Sex data_gss %&gt;% dplyr::group_by(female) %&gt;% furniture::table1(factor(volteer), digits = 4, total = TRUE) -------------------------------------------------------- female Total male female n = 2903 n = 1285 n = 1618 factor(volteer) 0 2376 (81.8%) 1057 (82.3%) 1319 (81.5%) 1 286 (9.9%) 132 (10.3%) 154 (9.5%) 2 133 (4.6%) 50 (3.9%) 83 (5.1%) 3 64 (2.2%) 26 (2%) 38 (2.3%) 4 19 (0.7%) 10 (0.8%) 9 (0.6%) 5 11 (0.4%) 4 (0.3%) 7 (0.4%) 6 7 (0.2%) 5 (0.4%) 2 (0.1%) 7 6 (0.2%) 1 (0.1%) 5 (0.3%) 9 1 (0%) 0 (0%) 1 (0.1%) -------------------------------------------------------- data_gss %&gt;% dplyr::group_by(female) %&gt;% furniture::table1(volteer, digits = 4, total = TRUE, test = TRUE) ----------------------------------------------------------------- female Total male female P-Value n = 2903 n = 1285 n = 1618 volteer 0.365 0.3334 (0.8858) 0.3167 (0.8493) 0.3467 (0.9139) ----------------------------------------------------------------- data_gss %&gt;% ggplot(aes(volteer, fill = female)) + geom_histogram(position = &quot;dodge&quot;) 8.3 Poisson Reression 8.3.1 Single Predictor: Sex 8.3.1.1 Fit the model glm_possion_1 &lt;- glm(volteer ~ female, data = data_gss, family = poisson(link = &quot;log&quot;)) summary(glm_possion_1) Call: glm(formula = volteer ~ female, family = poisson(link = &quot;log&quot;), data = data_gss) Deviance Residuals: Min 1Q Median 3Q Max -0.8327 -0.8327 -0.7959 -0.7959 6.4273 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.14970 0.04957 -23.19 &lt;2e-16 *** femalefemale 0.09048 0.06511 1.39 0.165 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 3658.1 on 2902 degrees of freedom Residual deviance: 3656.2 on 2901 degrees of freedom AIC: 4924.1 Number of Fisher Scoring iterations: 6 Note: The deviance residuals range as high as 6.47!!! 8.3.1.2 Marginal Estimates Note: Results are given on the log (not the response) scale glm_possion_1 %&gt;% emmeans::emmeans(~ female) female emmean SE df asymp.LCL asymp.UCL male -1.15 0.0496 Inf -1.25 -1.053 female -1.06 0.0422 Inf -1.14 -0.976 Results are given on the log (not the response) scale. Confidence level used: 0.95 Note: These means are on the original scale (number of volunteer activities in the past year). These standard errors are called “delta-method standard errors” effects::Effect(focal.predictors = c(&quot;female&quot;), mod = glm_possion_1) %&gt;% data.frame() # A tibble: 2 x 5 female fit se lower upper &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 male 0.317 0.0157 0.287 0.349 2 female 0.347 0.0146 0.319 0.377 8.3.1.3 Pairwise Post Hoc Test glm_possion_1 %&gt;% emmeans::emmeans(~ female) %&gt;% pairs() contrast estimate SE df z.ratio p.value male - female -0.0905 0.0651 Inf -1.390 0.1647 Results are given on the log (not the response) scale. 8.3.1.4 Parameter Estimates Coefficients are in terms of the LOG of the number of times a person volunteers per year. glm_possion_1 %&gt;% coef() (Intercept) femalefemale -1.14970081 0.09047562 Exponentiating the coefficients (betas) returns the values to the original scale (number of times a person volunteers per year) and is refered to as the incident rate ratio IRR. glm_possion_1 %&gt;% coef() %&gt;% exp() (Intercept) femalefemale 0.3167315 1.0946948 texreg::screenreg(list(glm_possion_1, texreghelpr::extract_glm_exp(glm_possion_1, include.aic = FALSE, include.bic = FALSE, include.loglik = FALSE, include.deviance = FALSE, include.nobs = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;RR [95% CI]&quot;), custom.coef.map = list(&quot;(Intercept)&quot; =&quot;Intercept&quot;, femalefemale = &quot;Female vs. Male&quot;), caption = &quot;GLM: Simple Possion Regression&quot;, single.row = TRUE, digits = 3) ============================================================== b (SE) RR [95% CI] -------------------------------------------------------------- Intercept -1.150 (0.050) *** 0.317 [0.287; 0.349] * Female vs. Male 0.090 (0.065) 1.095 [0.964; 1.244] * -------------------------------------------------------------- AIC 4924.135 BIC 4936.082 Log Likelihood -2460.068 Deviance 3656.198 Num. obs. 2903 ============================================================== *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 (or 0 outside the confidence interval). 8.3.2 Multiple Predictors 8.3.2.1 Fit the model glm_possion_2 &lt;- glm(volteer ~ female + nonwhite + educate + income, data = data_gss, family = poisson(link = &quot;log&quot;)) summary(glm_possion_2) Call: glm(formula = volteer ~ female + nonwhite + educate + income, family = poisson(link = &quot;log&quot;), data = data_gss) Deviance Residuals: Min 1Q Median 3Q Max -1.3061 -0.8864 -0.7597 -0.6064 5.8489 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.15830 0.24479 -12.902 &lt; 2e-16 *** femalefemale 0.26132 0.07785 3.357 0.000789 *** nonwhitenon-white -0.28038 0.10838 -2.587 0.009681 ** educate 0.10280 0.01443 7.123 1.05e-12 *** income 0.05683 0.01566 3.628 0.000286 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 2566.8 on 1943 degrees of freedom Residual deviance: 2465.5 on 1939 degrees of freedom (959 observations deleted due to missingness) AIC: 3380.9 Number of Fisher Scoring iterations: 6 8.3.2.2 Marginal Estimates Note: These means are on the original scale (number of volunteer activities in the past year). These standard errors are called “delta-method standard errors” effects::Effect(focal.predictors = c(&quot;female&quot;), mod = glm_possion_2, xlevels = list(nonwhite = &quot;non-white&quot;, educate = 5, income = 12)) %&gt;% data.frame() # A tibble: 2 x 5 female fit se lower upper &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 male 0.294 0.0175 0.262 0.330 2 female 0.382 0.0201 0.344 0.423 8.3.2.3 Parameter Estimates Coefficients are in terms of the LOG of the number of times a person volunteers per year. glm_possion_2 %&gt;% coef() (Intercept) femalefemale nonwhitenon-white educate -3.15830174 0.26132182 -0.28037733 0.10280183 income 0.05682778 Exponentiating the coefficients (betas) returns the values to the original scale (number of times a person volunteers per year) and is refered to as the incident rate ratio IRR. glm_possion_2 %&gt;% coef() %&gt;% exp() (Intercept) femalefemale nonwhitenon-white educate 0.04249785 1.29864553 0.75549861 1.10827176 income 1.05847350 texreg::screenreg(list(glm_possion_2, texreghelpr::extract_glm_exp(glm_possion_2, include.aic = FALSE, include.bic = FALSE, include.loglik = FALSE, include.deviance = FALSE, include.nobs = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;RR [95% CI]&quot;), custom.coef.map = list(&quot;(Intercept)&quot; =&quot;Intercept&quot;, femalefemale = &quot;Female vs. Male&quot;, &quot;nonwhitenon-white&quot; = &quot;Non-white vs. White&quot;, educate = &quot;Education, Years&quot;, income = &quot;Income&quot;), caption = &quot;GLM: Multiple Possion Regression&quot;, single.row = TRUE, digits = 3) ================================================================== b (SE) RR [95% CI] ------------------------------------------------------------------ Intercept -3.158 (0.245) *** 0.042 [0.026; 0.068] * Female vs. Male 0.261 (0.078) *** 1.299 [1.115; 1.513] * Non-white vs. White -0.280 (0.108) ** 0.755 [0.608; 0.930] * Education, Years 0.103 (0.014) *** 1.108 [1.077; 1.140] * Income 0.057 (0.016) *** 1.058 [1.027; 1.092] * ------------------------------------------------------------------ AIC 3380.860 BIC 3408.722 Log Likelihood -1685.430 Deviance 2465.514 Num. obs. 1944 ================================================================== *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 (or 0 outside the confidence interval). 8.3.2.4 Assess Model Fit DescTools::PseudoR2(glm_possion_2) McFadden 0.3151547 DescTools::PseudoR2(glm_possion_2, which = &quot;all&quot;) %&gt;% round(3) McFadden McFaddenAdj CoxSnell Nagelkerke 0.315 0.313 0.550 0.597 AldrichNelson VeallZimmermann Efron McKelveyZavoina 0.444 0.619 0.020 NA Tjur AIC BIC logLik NA 3380.860 3408.722 -1685.430 logLik0 G2 -2461.037 1551.215 8.3.2.5 Interpretation female: Adjusting for the effects of rate/ethnicity, education, and income, FEMALES are expected to volunteer about 30% MORE activities per year than males. nonwhite: Adjusting for the effects of sex, education, and income, NON-WHITES are expected to volunteer for about 24% LESS activities per year than males. educate: Each one-year increase in education is associated with an 11% increase in the number of volunteer activities per year, adjusting for the effects of sex, race/ethnicity, and income. 8.3.2.6 Residual Diagnostics par(mfrow = c(2, 2)) plot(glm_possion_2) par(mfrow = c(1, 1)) These residuals do NOT look good, especially the Q-Q plot for normality. 8.3.2.7 Marginal Plot data_gss %&gt;% dplyr::select(educate, income) %&gt;% psych::describe(skew = FALSE) # A tibble: 2 x 8 vars n mean sd min max range se &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 2894 13.4 2.93 0 20 20 0.0544 2 2 1947 9.86 2.99 1 12 11 0.0677 data_gss %&gt;% dplyr::select(educate, income) %&gt;% summary() educate income Min. : 0.00 Min. : 1.000 1st Qu.:12.00 1st Qu.: 9.000 Median :13.00 Median :11.000 Mean :13.36 Mean : 9.862 3rd Qu.:16.00 3rd Qu.:12.000 Max. :20.00 Max. :12.000 NA&#39;s :9 NA&#39;s :956 effects::Effect(focal.predictors = c(&quot;female&quot;, &quot;nonwhite&quot;, &quot;educate&quot;, &quot;income&quot;), mod = glm_possion_2, xlevels = list(educate = seq(from = 0, to = 20, by = .1), income = c(8, 10, 12))) %&gt;% data.frame() %&gt;% dplyr::mutate(income = factor(income)) %&gt;% ggplot(aes(x = educate, y = fit)) + geom_ribbon(aes(ymin = fit - se, # bands = +/- 1 SEM ymax = fit + se, fill = female), alpha = .2) + geom_line(aes(linetype = female, color = female), size = 1) + theme_bw() + labs(x = &quot;Education, Years&quot;, y = &quot;Predicted Mean Number of Volunteer Activities&quot;, color = NULL, fill = NULL, linetype = NULL) + theme(legend.position = c(0, 1), legend.justification = c(-.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + facet_grid(nonwhite ~ income) effects::Effect(focal.predictors = c(&quot;female&quot;, &quot;nonwhite&quot;, &quot;educate&quot;, &quot;income&quot;), mod = glm_possion_2, xlevels = list(educate = seq(from = 0, to = 20, by = .1), income = c(8, 10, 12))) %&gt;% data.frame() %&gt;% dplyr::mutate(income = factor(income)) %&gt;% ggplot(aes(x = educate, y = fit)) + geom_line(aes(linetype = fct_rev(income), color = fct_rev(income)), size = 1) + theme_bw() + labs(x = &quot;Education, Years&quot;, y = &quot;Predicted Mean Number of Volunteer Activities&quot;, color = &quot;Income:&quot;, fill = &quot;Income:&quot;, linetype = &quot;Income:&quot;) + theme(legend.position = c(0, 1), legend.justification = c(-.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + facet_grid(nonwhite ~ female) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) effects::Effect(focal.predictors = c(&quot;female&quot;, &quot;nonwhite&quot;, &quot;educate&quot;, &quot;income&quot;), mod = glm_possion_2, xlevels = list(educate = seq(from = 0, to = 20, by = .1), income = c(8, 10, 12))) %&gt;% data.frame() %&gt;% dplyr::mutate(income = factor(income)) %&gt;% ggplot(aes(x = educate, y = fit)) + geom_ribbon(aes(ymin = fit - se, # bands = +/- 1 SEM ymax = fit + se, fill = nonwhite), alpha = .2) + geom_line(aes(linetype = nonwhite, color = nonwhite), size = 1) + theme_bw() + labs(x = &quot;Education, Years&quot;, y = &quot;Predicted Mean Number of Volunteer Activities&quot;, color = NULL, fill = NULL, linetype = NULL) + theme(legend.position = c(0, .5), legend.justification = c(-.05, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + facet_grid(female ~ income) + scale_color_manual(values = c(&quot;darkgreen&quot;, &quot;orange&quot;)) + scale_fill_manual(values = c(&quot;darkgreen&quot;, &quot;orange&quot;)) effects::Effect(focal.predictors = c(&quot;female&quot;, &quot;educate&quot;), mod = glm_possion_2, xlevels = list(educate = seq(from = 0, to = 20, by = .1), income = 12)) %&gt;% data.frame() %&gt;% ggplot(aes(x = educate, y = fit)) + geom_ribbon(aes(ymin = fit - se, # bands = +/- 1 SEM ymax = fit + se, fill = female), alpha = .2) + geom_line(aes(linetype = female, color = female), size = 1) + theme_bw() + labs(x = &quot;Education, Years&quot;, y = &quot;Predicted Mean Number of Volunteer Activities&quot;, color = NULL, fill = NULL, linetype = NULL) + theme(legend.position = c(0, 1), legend.justification = c(-.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) 8.4 Negative Binomial Regression 8.4.1 Multiple Predictors 8.4.1.1 Fit the model glm_negbin_1 &lt;- MASS::glm.nb(volteer ~ female + nonwhite + educate + income, data = data_gss) summary(glm_negbin_1) Call: MASS::glm.nb(formula = volteer ~ female + nonwhite + educate + income, data = data_gss, init.theta = 0.2559648877, link = log) Deviance Residuals: Min 1Q Median 3Q Max -0.8798 -0.6897 -0.6141 -0.5211 2.7019 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.24738 0.37283 -8.710 &lt; 2e-16 *** femalefemale 0.28441 0.12312 2.310 0.0209 * nonwhitenon-white -0.31107 0.16203 -1.920 0.0549 . educate 0.11200 0.02321 4.825 1.4e-06 *** income 0.05193 0.02264 2.293 0.0218 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Negative Binomial(0.256) family taken to be 1) Null deviance: 1068.5 on 1943 degrees of freedom Residual deviance: 1024.3 on 1939 degrees of freedom (959 observations deleted due to missingness) AIC: 2851.6 Number of Fisher Scoring iterations: 1 Theta: 0.2560 Std. Err.: 0.0251 2 x log-likelihood: -2839.5640 Note: the deviance residuals all have absolute values less than 3-4’ish…better than before 8.4.1.2 Marginal Estimates Note: These means are on the original scale (number of volunteer activities in the past year). These standard errors are called “delta-method standard errors” effects::Effect(focal.predictors = c(&quot;female&quot;), mod = glm_negbin_1, xlevels = list(nonwhite = &quot;non-white&quot;, educate = 5, income = 12)) %&gt;% data.frame() # A tibble: 2 x 5 female fit se lower upper &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 male 0.289 0.0257 0.243 0.344 2 female 0.384 0.0322 0.326 0.453 8.4.1.3 Parameter Estimates Coefficients are in terms of the LOG of the number of times a person volunteers per year. glm_negbin_1 %&gt;% coef() (Intercept) femalefemale nonwhitenon-white educate -3.24738340 0.28440826 -0.31107286 0.11199528 income 0.05193102 Exponentiating the coefficients (betas) returns the values to the original scale (number of times a person volunteers per year) and is refered to as the incident rate ratio IRR. glm_negbin_1 %&gt;% coef() %&gt;% exp() (Intercept) femalefemale nonwhitenon-white educate 0.0388758 1.3289754 0.7326605 1.1185076 income 1.0533031 texreg::screenreg(list(glm_negbin_1, texreghelpr::extract_glm_exp(glm_negbin_1, include.aic = FALSE, include.bic = FALSE, include.loglik = FALSE, include.deviance = FALSE, include.nobs = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;RR [95% CI]&quot;), custom.coef.map = list(&quot;(Intercept)&quot; =&quot;Intercept&quot;, femalefemale = &quot;Female vs. Male&quot;, &quot;nonwhitenon-white&quot; = &quot;Non-white vs. White&quot;, educate = &quot;Education, Years&quot;, income = &quot;Income&quot;), caption = &quot;GLM: Negitive Binomial Regression&quot;, single.row = TRUE, digits = 3) ================================================================== b (SE) RR [95% CI] ------------------------------------------------------------------ Intercept -3.247 (0.373) *** 0.039 [0.018; 0.081] * Female vs. Male 0.284 (0.123) * 1.329 [1.043; 1.696] * Non-white vs. White -0.311 (0.162) 0.733 [0.533; 1.008] * Education, Years 0.112 (0.023) *** 1.119 [1.067; 1.173] * Income 0.052 (0.023) * 1.053 [1.008; 1.101] * ------------------------------------------------------------------ AIC 2851.564 BIC 2884.999 Log Likelihood -1419.782 Deviance 1024.343 Num. obs. 1944 ================================================================== *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 (or 0 outside the confidence interval). 8.4.1.4 Residual Diagnostics par(mfrow = c(2, 2)) plot(glm_negbin_1) par(mfrow = c(1, 1)) These still don’t look very good :( 8.5 Zero Inflated Poisson 8.5.0.1 Fit the model glm_zip_1 &lt;- pscl::zeroinfl(volteer ~ female + nonwhite + educate + income | educate, data = data_gss) summary(glm_zip_1) Call: pscl::zeroinfl(formula = volteer ~ female + nonwhite + educate + income | educate, data = data_gss) Pearson residuals: Min 1Q Median 3Q Max -0.5778 -0.4416 -0.3908 -0.3383 9.4777 Count model coefficients (poisson with log link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.09660 0.34808 -3.150 0.00163 ** femalefemale 0.20615 0.09452 2.181 0.02918 * nonwhitenon-white -0.20101 0.13444 -1.495 0.13488 educate 0.05648 0.02140 2.639 0.00833 ** income 0.04888 0.01882 2.597 0.00940 ** Zero-inflation model coefficients (binomial with logit link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 2.01161 0.41192 4.883 1.04e-06 *** educate -0.07179 0.02768 -2.593 0.00951 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Number of iterations in BFGS optimization: 18 Log-likelihood: -1434 on 7 Df glm_zip_1 %&gt;% coef() %&gt;% exp() count_(Intercept) count_femalefemale count_nonwhitenon-white 0.3340048 1.2289390 0.8179079 count_educate count_income zero_(Intercept) 1.0581037 1.0500947 7.4753687 zero_educate 0.9307240 Compares two models fit to the same data that do not nest via Vuong’s non-nested test. pscl::vuong(glm_zip_1, glm_possion_2) Vuong Non-Nested Hypothesis Test-Statistic: (test-statistic is asymptotically distributed N(0,1) under the null that the models are indistinguishible) ------------------------------------------------------------- Vuong z-statistic H_A p-value Raw 8.000302 model1 &gt; model2 6.6613e-16 AIC-corrected 7.936568 model1 &gt; model2 9.9920e-16 BIC-corrected 7.758989 model1 &gt; model2 4.3299e-15 8.6 Zero Inflated Negative Binomial 8.6.0.1 Fit the model glm_zinb_1 &lt;- pscl::zeroinfl(volteer ~ female + nonwhite + educate + income | educate, data = data_gss, dist = &quot;negbin&quot;) summary(glm_zinb_1) Call: pscl::zeroinfl(formula = volteer ~ female + nonwhite + educate + income | educate, data = data_gss, dist = &quot;negbin&quot;) Pearson residuals: Min 1Q Median 3Q Max -0.5146 -0.4122 -0.3704 -0.3222 8.8088 Count model coefficients (negbin with log link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.79278 0.54745 -3.275 0.00106 ** femalefemale 0.26245 0.11745 2.235 0.02545 * nonwhitenon-white -0.28519 0.15603 -1.828 0.06758 . educate 0.06817 0.03317 2.055 0.03987 * income 0.05292 0.02159 2.451 0.01426 * Log(theta) 0.05056 0.46242 0.109 0.91293 Zero-inflation model coefficients (binomial with logit link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.28322 0.71435 1.796 0.0724 . educate -0.07208 0.04684 -1.539 0.1238 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Theta = 1.0519 Number of iterations in BFGS optimization: 33 Log-likelihood: -1416 on 8 Df glm_zinb_1 %&gt;% coef() %&gt;% exp() count_(Intercept) count_femalefemale count_nonwhitenon-white 0.1664962 1.3001110 0.7518708 count_educate count_income zero_(Intercept) 1.0705424 1.0543416 3.6082563 zero_educate 0.9304550 pscl::vuong(glm_zinb_1, glm_negbin_1) Vuong Non-Nested Hypothesis Test-Statistic: (test-statistic is asymptotically distributed N(0,1) under the null that the models are indistinguishible) ------------------------------------------------------------- Vuong z-statistic H_A p-value Raw 1.3486325 model1 &gt; model2 0.088728 AIC-corrected 0.5592035 model1 &gt; model2 0.288011 BIC-corrected -1.6403440 model2 &gt; model1 0.050467 pscl::vuong(glm_zip_1, glm_zinb_1) Vuong Non-Nested Hypothesis Test-Statistic: (test-statistic is asymptotically distributed N(0,1) under the null that the models are indistinguishible) ------------------------------------------------------------- Vuong z-statistic H_A p-value Raw -2.428612 model2 &gt; model1 0.0075784 AIC-corrected -2.428612 model2 &gt; model1 0.0075784 BIC-corrected -2.428612 model2 &gt; model1 0.0075784 The ‘best’ model is the zero-inflated negative binomial "],
["extensions-of-logistic-regression-ex-canadian-womens-labour-force-participation.html", "9 Extensions of Logistic Regression - Ex: Canadian Women’s Labour-Force Participation 9.1 Background 9.2 Exploratory Data Analysis 9.3 Hierarchical (nested) Logistic Regression 9.4 Multinomial (nominal) Logistic Regression 9.5 Proportional-odds (ordinal) Logistic Regression", " 9 Extensions of Logistic Regression - Ex: Canadian Women’s Labour-Force Participation library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(sjPlot) # Quick plots and tables for models library(car) # Companion to Applied Regression (a text book - includes datasets) library(MASS) # Support Functions and Datasets library(nnet) # Multinomial Log-Linear Models 9.1 Background The Womenlf data frame has 263 rows and 4 columns. The data are from a 1977 survey of the Canadian population. Dependent variable (DV) or outcome partic Labour-Force Participation, a factor with levels: fulltime Working full-time not.work Not working outside the home parttime Working part-time Indepdentend variables (IV) or predictors hincome Husband’s income, in $1000’s children Presence of children in the household, a factor with levels: absent no children in the home present at least one child at home region A factor with levels: Atlantic Atlantic Canada BC British Columbia Ontario Prairie Prairie provinces Quebec 9.1.1 Raw Dataset The data is included in the carData package which installs and loads with the car package. data(Womenlf, package = &quot;carData&quot;) # load the internal data tibble::glimpse(Womenlf) # glimpse a bit of the data Observations: 263 Variables: 4 $ partic &lt;fct&gt; not.work, not.work, not.work, not.work, not.work, not... $ hincome &lt;int&gt; 15, 13, 45, 23, 19, 7, 15, 7, 15, 23, 23, 13, 9, 9, 4... $ children &lt;fct&gt; present, present, present, present, present, present,... $ region &lt;fct&gt; Ontario, Ontario, Ontario, Ontario, Ontario, Ontario,... Womenlf %&gt;% dplyr:: filter(row_number() %in% sample(1:nrow(.), size = 10)) # select a random sample of 10 rows # A tibble: 10 x 4 partic hincome children region &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; 1 not.work 9 present Prairie 2 not.work 45 present Atlantic 3 not.work 9 present Ontario 4 parttime 9 present Ontario 5 parttime 13 present Prairie 6 not.work 28 present Ontario 7 not.work 23 present BC 8 fulltime 9 absent Ontario 9 fulltime 5 absent Quebec 10 not.work 23 absent Quebec Notice the order of the factor levels, especially for the partic factor str(Womenlf) # view the structure of the data &#39;data.frame&#39;: 263 obs. of 4 variables: $ partic : Factor w/ 3 levels &quot;fulltime&quot;,&quot;not.work&quot;,..: 2 2 2 2 2 2 2 1 2 2 ... $ hincome : int 15 13 45 23 19 7 15 7 15 23 ... $ children: Factor w/ 2 levels &quot;absent&quot;,&quot;present&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ region : Factor w/ 5 levels &quot;Atlantic&quot;,&quot;BC&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... We can view the order of the factors levels Womenlf$partic %&gt;% levels() # view the levels (in order) of the variable [1] &quot;fulltime&quot; &quot;not.work&quot; &quot;parttime&quot; 9.1.2 Declare Factors Womenlf_clean &lt;- Womenlf %&gt;% dplyr::mutate(working_ord = dplyr::case_when(partic == &quot;fulltime&quot; ~ 2, # manually re-code a factor partic == &quot;not.work&quot; ~ 0, # into a numeric variable partic == &quot;parttime&quot; ~ 1) %&gt;% factor(levels = c(0, 1, 2), # maually declare the level order labels = c(&quot;Not at All&quot;, # and labels of the levels) &quot;Part Time&quot;, &quot;Full Time&quot;))) %&gt;% dplyr::mutate(working_any = dplyr::case_when(partic == &quot;fulltime&quot; ~ 1, # manually re-code a factor partic == &quot;not.work&quot; ~ 0, # into a numeric variable partic == &quot;parttime&quot; ~ 1) %&gt;% factor(levels = c(0, 1), # maually declare the level order labels = c(&quot;Not at All&quot;, # and labels of the levels) &quot;At Least Part Time&quot;))) %&gt;% dplyr::mutate(working_full = dplyr::case_when(partic == &quot;fulltime&quot; ~ 1, # manually re-code a factor partic == &quot;not.work&quot; ~ 0, # into a numeric variable partic == &quot;parttime&quot; ~ 0)%&gt;% factor(levels = c(0, 1), # maually declare the level order labels = c(&quot;Less Than Full Time&quot;, # and labels of the levels) &quot;Full Time&quot;))) %&gt;% dplyr::mutate(working_type = dplyr::case_when(partic == &quot;fulltime&quot; ~ 1, # manually re-code a factor partic == &quot;parttime&quot; ~ 0)%&gt;% # into a numeric variable factor(levels = c(0, 1), # maually declare the level order labels = c(&quot;Part Time&quot;, # and labels of the levels) &quot;Full Time&quot;))) Display the structure of the ‘clean’ version of the dataset str(Womenlf_clean) # view the structure of the data &#39;data.frame&#39;: 263 obs. of 8 variables: $ partic : Factor w/ 3 levels &quot;fulltime&quot;,&quot;not.work&quot;,..: 2 2 2 2 2 2 2 1 2 2 ... $ hincome : int 15 13 45 23 19 7 15 7 15 23 ... $ children : Factor w/ 2 levels &quot;absent&quot;,&quot;present&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ region : Factor w/ 5 levels &quot;Atlantic&quot;,&quot;BC&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... $ working_ord : Factor w/ 3 levels &quot;Not at All&quot;,&quot;Part Time&quot;,..: 1 1 1 1 1 1 1 3 1 1 ... $ working_any : Factor w/ 2 levels &quot;Not at All&quot;,&quot;At Least Part Time&quot;: 1 1 1 1 1 1 1 2 1 1 ... $ working_full: Factor w/ 2 levels &quot;Less Than Full Time&quot;,..: 1 1 1 1 1 1 1 2 1 1 ... $ working_type: Factor w/ 2 levels &quot;Part Time&quot;,&quot;Full Time&quot;: NA NA NA NA NA NA NA 2 NA NA ... 9.2 Exploratory Data Analysis Three versions of the outcome # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; Womenlf_clean %&gt;% furniture::table1(working_ord, working_any, working_full, working_type, na.rm = FALSE, # do NOT restrict to complete cases!!! output = &quot;html&quot;) Mean/Count (SD/%) n = 263 working_ord Not at All 155 (58.9%) Part Time 42 (16%) Full Time 66 (25.1%) NA 0 (0%) working_any Not at All 155 (58.9%) At Least Part Time 108 (41.1%) NA 0 (0%) working_full Less Than Full Time 197 (74.9%) Full Time 66 (25.1%) NA 0 (0%) working_type Part Time 42 (16%) Full Time 66 (25.1%) NA 155 (58.9%) Other Predisctors, univariate # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; Womenlf_clean %&gt;% furniture::table1(&quot;Husband&#39;s Income, $1000&#39;s&quot; = hincome, &quot;Children In the Home&quot; = children, &quot;Region of Canada&quot; = region, output = &quot;html&quot;) Mean/Count (SD/%) n = 263 Husband’s Income, $1000’s 14.8 (7.2) Children In the Home absent 79 (30%) present 184 (70%) Region of Canada Atlantic 30 (11.4%) BC 29 (11%) Ontario 108 (41.1%) Prairie 31 (11.8%) Quebec 65 (24.7%) # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; Womenlf_clean %&gt;% furniture::table1(&quot;Husband&#39;s Income, $1000&#39;s&quot; = hincome, &quot;Children In the Home&quot; = children, &quot;Region of Canada&quot; = region, splitby = ~ working_ord, row_wise = TRUE, # show row %s rather than default column %s test = TRUE, output = &quot;html&quot;) Not at All Part Time Full Time P-Value n = 155 n = 42 n = 66 Husband’s Income, $1000’s 0.002 15.6 (7.2) 16.0 (8.1) 12.1 (6.1) Children In the Home &lt;.001 absent 26 (32.9%) 7 (8.9%) 46 (58.2%) present 129 (70.1%) 35 (19%) 20 (10.9%) Region of Canada 0.71 Atlantic 20 (66.7%) 4 (13.3%) 6 (20%) BC 14 (48.3%) 8 (27.6%) 7 (24.1%) Ontario 64 (59.3%) 17 (15.7%) 27 (25%) Prairie 17 (54.8%) 6 (19.4%) 8 (25.8%) Quebec 40 (61.5%) 7 (10.8%) 18 (27.7%) 9.2.1 Husband’s Income Womenlf_clean %&gt;% ggplot(aes(hincome, fill = working_ord)) + geom_density(alpha = .3) Womenlf_clean %&gt;% ggplot(aes(x = working_ord, y = hincome)) + geom_jitter(position=position_jitter(0.2)) + stat_summary(fun.y = mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y..), width = .75, color = &quot;red&quot;, size = 1) Womenlf_clean %&gt;% ggplot(aes(hincome, x = working_ord, fill = working_ord)) + geom_boxplot(alpha = .3) Womenlf_clean %&gt;% ggplot(aes(hincome, x = working_ord, fill = working_ord)) + geom_violin(alpha = .3) 9.2.2 Children in Home Womenlf_clean %&gt;% ggplot(aes(x = children, fill = working_ord)) + geom_bar() Womenlf_clean %&gt;% ggplot(aes(x = children, fill = working_ord %&gt;% fct_rev)) + geom_bar(position=&quot;fill&quot;) + labs(x = &quot;Children in the Home&quot;, y = &quot;Proportion of Women&quot;, fill = &quot;Working&quot;) + theme_bw() + scale_fill_manual(values = c(&quot;gray25&quot;, &quot;gray50&quot;, &quot;gray75&quot;)) 9.2.3 Region of Canada Womenlf_clean %&gt;% ggplot(aes(x = region, fill = working_ord)) + geom_bar() Womenlf_clean %&gt;% ggplot(aes(x = region, fill = working_ord %&gt;% fct_rev)) + geom_bar(position=&quot;fill&quot;) + labs(x = &quot;Region of Canada&quot;, y = &quot;Proportion of Women&quot;, fill = &quot;Working&quot;) + theme_bw() + scale_fill_manual(values = c(&quot;gray25&quot;, &quot;gray50&quot;, &quot;gray75&quot;)) 9.3 Hierarchical (nested) Logistic Regression For an \\(m-\\)category polytomy dependent variable is respecified as a series of \\(m – 1\\) nested dichotomies. A single or combined levels of outcome compared to another single or combination of levels. Then they are analyzed using a series of binary logistic regressions, such that: Dichotomies selected based on theory Avoid redundancy Similar to contrast coding, but for outcome For this dataset example, the outcome (partic) has \\(3-\\)categories, so we will investigate TWO nested dichotomies outcome = working_any outcome = working_type 9.3.1 Role of Predictors on ANY working Fit a regular logistic model with all three predictors regressed on the binary indicator for any working. Use the glm() function in the base \\(R\\) stats package. fit_glm_1 &lt;- glm(working_any ~ hincome + children + region, data = Womenlf_clean, family = binomial(link = &quot;logit&quot;)) summary(fit_glm_1) Call: glm(formula = working_any ~ hincome + children + region, family = binomial(link = &quot;logit&quot;), data = Womenlf_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.7927 -0.8828 -0.7283 0.9562 2.0074 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.26771 0.55296 2.293 0.0219 * hincome -0.04534 0.02057 -2.204 0.0275 * childrenpresent -1.60434 0.30187 -5.315 1.07e-07 *** regionBC 0.34196 0.58504 0.585 0.5589 regionOntario 0.18778 0.46762 0.402 0.6880 regionPrairie 0.47186 0.55680 0.847 0.3967 regionQuebec -0.17310 0.49957 -0.347 0.7290 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 356.15 on 262 degrees of freedom Residual deviance: 317.30 on 256 degrees of freedom AIC: 331.3 Number of Fisher Scoring iterations: 4 Check if region is statistically significant with the drop1() function from the base \\(R\\) stats package. This may be done with a Likelihood Ratio Test (test = \"LRT\", which is the same as test = \"Chisq\" for glm models). drop1(fit_glm_1, test = &quot;LRT&quot;) # A tibble: 4 x 5 Df Deviance AIC LRT `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 NA 317. 331. NA NA 2 1 322. 334. 5.13 0.0236 3 1 348. 360. 30.5 0.0000000326 4 4 320. 326. 2.43 0.657 Since the region doesn’t have exhibit any effect on odds a women is in the labor force, remove that predictor in the model to simplify to a ‘best’ final model. Also, center husband’s income at a value near the mean so the intercept has meaning. fit_glm_2 &lt;- glm(working_any ~ I(hincome - 14) + children, data = Womenlf_clean, family = binomial(link = &quot;logit&quot;)) summary(fit_glm_2) Call: glm(formula = working_any ~ I(hincome - 14) + children, family = binomial(link = &quot;logit&quot;), data = Womenlf_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6767 -0.8652 -0.7768 0.9292 1.9970 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.74351 0.24312 3.058 0.00223 ** I(hincome - 14) -0.04231 0.01978 -2.139 0.03244 * childrenpresent -1.57565 0.29226 -5.391 7e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 356.15 on 262 degrees of freedom Residual deviance: 319.73 on 260 degrees of freedom AIC: 325.73 Number of Fisher Scoring iterations: 4 The texreg package uses an intermediate function called extract() to extract information for the model and then put it in the right places in the table. We can invervene by writing our own extract_exp() function to use instead. extract_exp &lt;- function(fit_glm){ beta = coef(fit_glm) betaci = confint(fit_glm) fit_glm_exp = texreg::extract(fit_glm) fit_glm_exp@coef = exp(beta) fit_glm_exp@ci.low = exp(betaci[, 1]) fit_glm_exp@ci.up = exp(betaci[, 2]) return(fit_glm_exp) } # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(extract_exp(fit_glm_2), custom.coef.names = c(&quot;BL: No children, Husband Earns $14,000/yr&quot;, &quot;Husband&#39;s Income, $1000&#39;s&quot;, &quot;Children in the Home&quot;), custom.model.names = &quot;OR, Women is in the Workforce at All&quot;, single.row = TRUE, custom.note = &quot;* The value of &#39;1&#39; is outside the confidence interval for the OR&quot;) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Statistical models OR, Women is in the Workforce at All BL: No children, Husband Earns $14,000/yr 2.10 [1.32; 3.44]* Husband’s Income, $1000’s 0.96 [0.92; 1.00]* Children in the Home 0.21 [0.12; 0.36]* AIC 325.73 BIC 336.45 Log Likelihood -159.87 Deviance 319.73 Num. obs. 263 * The value of ‘1’ is outside the confidence interval for the OR Interpretation: Among women without children in the home and a husband making $14,000 annually, there is about a 2:1 odds she is in the workforce. For each additional thousand dollars the husband makes, the odds ratio decreases by about 4 percent. If there are children in the home, the odds of being in the workforce is nearly a fifth as large. effects::allEffects(fit_glm_2) model: working_any ~ I(hincome - 14) + children hincome effect hincome 1 10 20 30 40 0.5476466 0.4527392 0.3514450 0.2619655 0.1886414 children effect children absent present 0.6707323 0.2964731 effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_glm_2) %&gt;% data.frame() %&gt;% ggplot(aes(x = hincome, y = fit, color = children, linetype = children)) + geom_vline(xintercept = 14, color = &quot;gray25&quot;) + # reference line for intercept geom_line(size = 1) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability of\\nWomen Being in the Workforce&quot;, color = &quot;Children in\\nthe Home:&quot;, linetype = &quot;Children in\\nthe Home:&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;)) + coord_cartesian(ylim = c(0, 1)) 9.3.2 Role of Predictors on TYPE of work Fit a regular logistic model with all three predictors regressed on the binary indicator for level/type of working. fit_glm_3 &lt;- glm(working_type ~ hincome + children + region, data = Womenlf_clean, family = binomial(link = &quot;logit&quot;)) summary(fit_glm_3) Call: glm(formula = working_type ~ hincome + children + region, family = binomial(link = &quot;logit&quot;), data = Womenlf_clean) Deviance Residuals: Min 1Q Median 3Q Max -2.5202 -0.8048 0.3583 0.7201 1.9957 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 3.76164 1.05718 3.558 0.000373 *** hincome -0.10475 0.04032 -2.598 0.009383 ** childrenpresent -2.74781 0.56893 -4.830 1.37e-06 *** regionBC -1.18248 1.02764 -1.151 0.249865 regionOntario -0.14876 0.84703 -0.176 0.860589 regionPrairie -0.39173 0.96310 -0.407 0.684200 regionQuebec 0.14842 0.93300 0.159 0.873612 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 144.34 on 107 degrees of freedom Residual deviance: 101.84 on 101 degrees of freedom (155 observations deleted due to missingness) AIC: 115.84 Number of Fisher Scoring iterations: 5 Check if region is statistically significant with the drop1() function from the base \\(R\\) stats package. This may be done with a Likelihood Ratio Test (test = \"LRT\", which is the same as test = \"Chisq\" for glm models). drop1(fit_glm_3, test = &quot;LRT&quot;) # A tibble: 4 x 5 Df Deviance AIC LRT `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 NA 102. 116. NA NA 2 1 110. 122. 7.84 0.00512 3 1 134. 146. 31.9 0.0000000162 4 4 104. 110. 2.65 0.618 Since the region doesn’t have exhibit any effect on odds a working women is in the labor force full time, remove that predictor in the model to simplify to a ‘best’ final model. Also, center husband’s income at a value near the mean so the intercept has meaning. fit_glm_4 &lt;- glm(working_type ~ I(hincome - 14) + children, data = Womenlf_clean, family = binomial(link = &quot;logit&quot;)) summary(fit_glm_4) Call: glm(formula = working_type ~ I(hincome - 14) + children, family = binomial(link = &quot;logit&quot;), data = Womenlf_clean) Deviance Residuals: Min 1Q Median 3Q Max -2.4047 -0.8678 0.3949 0.6213 1.7641 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.97602 0.43024 4.593 4.37e-06 *** I(hincome - 14) -0.10727 0.03915 -2.740 0.00615 ** childrenpresent -2.65146 0.54108 -4.900 9.57e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 144.34 on 107 degrees of freedom Residual deviance: 104.49 on 105 degrees of freedom (155 observations deleted due to missingness) AIC: 110.49 Number of Fisher Scoring iterations: 5 The texreg package uses an intermediate function called extract() to extract information for the model and then put it in the right places in the table. We can invervene by writing our own extract_exp() function to use instead. # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(extract_exp(fit_glm_2), extract_exp(fit_glm_4)), custom.coef.names = c(&quot;BL: No children, Husband Earns $14,000/yr&quot;, &quot;Husband&#39;s Income, $1000&#39;s&quot;, &quot;Children in the Home&quot;), custom.model.names = c(&quot;Working at All&quot;, &quot;Full vs. Part-Time&quot;), single.row = TRUE, custom.note = &quot;* The value of &#39;1&#39; is outside the confidence interval for the OR&quot;) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Statistical models Working at All Full vs. Part-Time BL: No children, Husband Earns $14,000/yr 2.10 [1.32; 3.44]* 7.21 [3.34; 18.45]* Husband’s Income, $1000’s 0.96 [0.92; 1.00]* 0.90 [0.83; 0.97]* Children in the Home 0.21 [0.12; 0.36]* 0.07 [0.02; 0.19]* AIC 325.73 110.49 BIC 336.45 118.54 Log Likelihood -159.87 -52.25 Deviance 319.73 104.49 Num. obs. 263 108 * The value of ‘1’ is outside the confidence interval for the OR Interpretation: Among working women without children in the home and a husband making $14,000 annually, there is more than 7:1 odds she is working full time verses part time. For each additional thousand dollars the husband makes, the odds ratio decreases by about 10 percent. If there are children in the home, the odds of being in the workforce is drastically reduced. effects::allEffects(fit_glm_4) model: working_type ~ I(hincome - 14) + children hincome effect hincome 1 10 20 30 40 0.8829045 0.7416987 0.4955346 0.2515165 0.1031024 children effect children absent present 0.8830579 0.3475686 effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_glm_4) %&gt;% data.frame() %&gt;% ggplot(aes(x = hincome, y = fit, color = children, linetype = children)) + geom_vline(xintercept = 14, color = &quot;gray25&quot;) + # reference line for intercept geom_line(size = 1) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability of\\nEmployed Women Work Full Time vs. Part Time&quot;, color = &quot;Children in\\nthe Home:&quot;, linetype = &quot;Children in\\nthe Home:&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;)) + coord_cartesian(ylim = c(0, 1)) 9.4 Multinomial (nominal) Logistic Regression Multinomial Logistic Regression fits a single model by specifing a reference level of the outcome and comparing each additional level to it. In our case we will choose not working as the reference category adn get a set of parameter estimates (betas) for each of the two options part time and full time. Use multinom() function in the base \\(R\\) nnet package. You will also need the MASS and \\(R\\) package (only to compute MLEs). Make sure to remove cases with missing data on predictors before modeling or use the na.action = na.omit optin in the multinom() model command. fit_multnom_1 &lt;- nnet::multinom(working_ord ~ I(hincome - 14) + children + region, data = Womenlf_clean) # weights: 24 (14 variable) initial value 288.935032 iter 10 value 208.470682 iter 20 value 207.732796 iter 20 value 207.732796 iter 20 value 207.732796 final value 207.732796 converged summary(fit_multnom_1, corr = FALSE, wald = TRUE) Call: nnet::multinom(formula = working_ord ~ I(hincome - 14) + children + region, data = Womenlf_clean) Coefficients: (Intercept) I(hincome - 14) childrenpresent regionBC Part Time -1.7521373 0.005261435 0.1462009 1.0863549 Full Time 0.7240428 -0.100034170 -2.6977927 -0.4599247 regionOntario regionPrairie regionQuebec Part Time 0.2856917 0.5747258 -0.1105184 Full Time 0.1135573 0.4681016 -0.3116829 Std. Errors: (Intercept) I(hincome - 14) childrenpresent regionBC Part Time 0.7204798 0.02468887 0.4901621 0.7193077 Full Time 0.6102008 0.02901623 0.3876731 0.7837044 regionOntario regionPrairie regionQuebec Part Time 0.6175050 0.7259118 0.6873048 Full Time 0.6175128 0.7332449 0.6515172 Residual Deviance: 415.4656 AIC: 443.4656 Reduce the model by removing the region variable. fit_multnom_2 &lt;- nnet::multinom(working_ord ~ I(hincome - 14) + children, data = Womenlf_clean) # weights: 12 (6 variable) initial value 288.935032 iter 10 value 211.456740 final value 211.440963 converged summary(fit_multnom_2, corr = FALSE, wald = TRUE) Call: nnet::multinom(formula = working_ord ~ I(hincome - 14) + children, data = Womenlf_clean) Coefficients: (Intercept) I(hincome - 14) childrenpresent Part Time -1.3357927 0.00688932 0.02149927 Full Time 0.6215469 -0.09723492 -2.55867912 Std. Errors: (Intercept) I(hincome - 14) childrenpresent Part Time 0.4340062 0.02345463 0.4690285 Full Time 0.2585136 0.02809670 0.3622077 Residual Deviance: 422.8819 AIC: 434.8819 Check if we need to keep the region variable in our model. anova(fit_multnom_1, fit_multnom_2) # A tibble: 2 x 7 Model `Resid. df` `Resid. Dev` Test ` Df` `LR stat.` `Pr(Chi)` &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 I(hincome - ~ 520 423. &quot;&quot; NA NA NA 2 I(hincome - ~ 512 415. 1 vs~ 8 7.42 0.492 Here is one way to extract the parameter estimates, but recall they are in terms of the logit or log-odds, not probability. broom::tidy(fit_multnom_2) %&gt;% dplyr::mutate(p.value = round(p.value, 4)) # A tibble: 6 x 6 y.level term estimate std.error statistic p.value &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Part Time (Intercept) 0.263 0.434 -3.08 0.0021 2 Part Time I(hincome - 14) 1.01 0.0235 0.294 0.769 3 Part Time childrenpresent 1.02 0.469 0.0458 0.963 4 Full Time (Intercept) 1.86 0.259 2.40 0.0162 5 Full Time I(hincome - 14) 0.907 0.0281 -3.46 0.0005 6 Full Time childrenpresent 0.0774 0.362 -7.06 0 The effects::allEffects() function provides probability estimates for each outcome level for different levels of the predictors. effects::allEffects(fit_multnom_2) model: working_ord ~ I(hincome - 14) + children hincome effect (probability) for Not at All hincome 1 10 20 30 40 0.4265713 0.5819837 0.6888795 0.7333353 0.7439897 hincome effect (probability) for Part Time hincome 1 10 20 30 40 0.1041120 0.1511290 0.1916462 0.2185644 0.2375547 hincome effect (probability) for Full Time hincome 1 10 20 30 40 0.46931674 0.26688731 0.11947430 0.04810031 0.01845552 children effect (probability) for Not at All children absent present 0.3339937 0.7122698 children effect (probability) for Part Time children absent present 0.08828253 0.19236144 children effect (probability) for Full Time children absent present 0.57772376 0.09536878 The texreg package know how to handle this type of model and displays the parameteri estimates in two seperate columns. It is unable to use the extract_exp() helper function to exponentiate the betas. # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(fit_multnom_2, custom.coef.names = c(&quot;BL: No children, Husband Earns $14,000/yr&quot;, &quot;Husband&#39;s Income, $1000&#39;s&quot;, &quot;Children in the Home&quot;), single.row = TRUE) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Statistical models Part Time Full Time BL: No children, Husband Earns $14,000/yr -1.34 (0.43)** 0.62 (0.26)* Husband’s Income, $1000’s 0.01 (0.02) -0.10 (0.03)*** Children in the Home 0.02 (0.47) -2.56 (0.36)*** AIC 434.88 434.88 BIC 456.31 456.31 Log Likelihood -211.44 -211.44 Deviance 422.88 422.88 Num. obs. 263 263 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 fit_multnom_2 %&gt;% coef() %&gt;% exp() (Intercept) I(hincome - 14) childrenpresent Part Time 0.2629496 1.0069131 1.02173205 Full Time 1.8618058 0.9073428 0.07740692 Interpretation: Among women without children in the home and a husband making $14,000 annually, there is about 1:4 odds she is working part time verses not at all. and a 1.8:1 odds she is working full time. For each additional thousand dollars the husband makes, the odds ratio decreases by about 10 percent that she is working full time, yet stay the same that she works part time. If there are children in the home, the odds of working part time increase by 2 percent and there is a very unlikely change she works full time. effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_multnom_2) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;)) %&gt;% dplyr::rename(none = prob.Not.at.All, part = prob.Part.Time, full = prob.Full.Time) %&gt;% tidyr::gather(key = work_level, value = fit, none, part, full) %&gt;% dplyr::mutate(work_level = factor(work_level, levels = c(&quot;none&quot;, &quot;part&quot;, &quot;full&quot;), labels = c(&quot;Not at All&quot;, &quot;Part Time&quot;, &quot;Full Time&quot;))) %&gt;% ggplot(aes(x = hincome, y = fit, color = work_level %&gt;% fct_rev, linetype = work_level %&gt;% fct_rev)) + geom_vline(xintercept = 14, color = &quot;gray25&quot;) + # reference line for intercept geom_line(size = 1) + facet_grid(. ~ children, labeller = label_both) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability&quot;, color = &quot;Woman Works:&quot;, linetype = &quot;Woman Works:&quot;) + theme(legend.key.width = unit(1, &quot;cm&quot;)) + coord_cartesian(ylim = c(0, 1)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_multnom_2) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;)) %&gt;% dplyr::rename(none = prob.Not.at.All, part = prob.Part.Time, full = prob.Full.Time) %&gt;% tidyr::gather(key = work_level, value = fit, none, part, full) %&gt;% dplyr::mutate(work_level = factor(work_level, levels = c(&quot;none&quot;, &quot;part&quot;, &quot;full&quot;), labels = c(&quot;Not at All&quot;, &quot;Part Time&quot;, &quot;Full Time&quot;))) %&gt;% ggplot(aes(x = hincome, y = fit, color = children %&gt;% fct_rev, linetype = children %&gt;% fct_rev)) + geom_vline(xintercept = 14, color = &quot;gray25&quot;) + # reference line for intercept geom_line(size = 1) + facet_grid(. ~ work_level) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability&quot;, color = &quot;Children in\\nthe Home:&quot;, linetype = &quot;Children in\\nthe Home:&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1, &quot;cm&quot;)) + coord_cartesian(ylim = c(0, 1)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) 9.5 Proportional-odds (ordinal) Logistic Regression This type of logisit regression model forces the predictors to have similar relationship with the outcome (slopes), but different means (intercepts). This is called the proportional odds assumption. Use polr() function in the base \\(R\\) MASS package. While outcome variable may be a regualr factor, it is prefereable to specify it as an ordered factor. fit_polr_1 &lt;- MASS::polr(working_ord ~ hincome + children, data = Womenlf_clean) summary(fit_polr_1) Call: MASS::polr(formula = working_ord ~ hincome + children, data = Womenlf_clean) Coefficients: Value Std. Error t value hincome -0.0539 0.01949 -2.766 childrenpresent -1.9720 0.28695 -6.872 Intercepts: Value Std. Error t value Not at All|Part Time -1.8520 0.3863 -4.7943 Part Time|Full Time -0.9409 0.3699 -2.5435 Residual Deviance: 441.663 AIC: 449.663 fit_polr_1$zeta %&gt;% exp() Not at All|Part Time Part Time|Full Time 0.1569171 0.3902663 fit_polr_1 %&gt;% coef() %&gt;% exp() hincome childrenpresent 0.9475262 0.1391841 fit_polr_1 %&gt;% confint() %&gt;% exp() 2.5 % 97.5 % hincome 0.9109815 0.9834852 childrenpresent 0.0784888 0.2422588 effects::allEffects(fit_polr_1) model: working_ord ~ hincome + children hincome effect (probability) for Not at All hincome 1 10 20 30 40 0.3968718 0.5166413 0.6469356 0.7585238 0.8433820 hincome effect (probability) for Part Time hincome 1 10 20 30 40 0.22384583 0.21001062 0.17311759 0.12800001 0.08713914 hincome effect (probability) for Full Time hincome 1 10 20 30 40 0.37928237 0.27334810 0.17994676 0.11347618 0.06947888 children effect (probability) for Not at All children absent present 0.2579513 0.7140863 children effect (probability) for Part Time children absent present 0.2057297 0.1472491 children effect (probability) for Full Time children absent present 0.5363190 0.1386646 # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(fit_polr_1, custom.coef.names = c(&quot;Husband&#39;s Income, $1000&#39;s&quot;, &quot;Children in the Home&quot;), single.row = TRUE) &lt;!DOCTYPE HTML PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd”&gt; Statistical models Model 1 Husband’s Income, $1000’s -0.05 (0.02)** Children in the Home -1.97 (0.29)*** AIC 449.66 BIC 463.95 Log Likelihood -220.83 Deviance 441.66 Num. obs. 263 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Interpretation: Among women without children in the home and a husband making $14,000 annually, there is a 26% chance she is not working, 21% change she is working part time and just over a 53% change she is working full time. For each additional thousand dollars the husband makes, the odds ratio decreases by about 5 percent that she is working part time vs not at all and 5% that she is working full time vs part time. If there are children in the home, the odds ratio of working part time vs not at all decreases by 86% and similartly the odds ratio fo working full time vs part time also decreases by 86%. effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_polr_1) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;)) %&gt;% dplyr::rename(none = prob.Not.at.All, part = prob.Part.Time, full = prob.Full.Time) %&gt;% tidyr::gather(key = work_level, value = fit, none, part, full) %&gt;% dplyr::mutate(work_level = factor(work_level, levels = c(&quot;none&quot;, &quot;part&quot;, &quot;full&quot;), labels = c(&quot;Not at All&quot;, &quot;Part Time&quot;, &quot;Full Time&quot;))) %&gt;% ggplot(aes(x = hincome, y = fit, color = work_level %&gt;% fct_rev, linetype = work_level %&gt;% fct_rev)) + geom_vline(xintercept = 14, color = &quot;gray25&quot;) + # reference line for intercept geom_line(size = 1) + facet_grid(. ~ children, labeller = label_both) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability&quot;, color = &quot;Woman Works:&quot;, linetype = &quot;Woman Works:&quot;) + theme(legend.key.width = unit(1, &quot;cm&quot;)) + coord_cartesian(ylim = c(0, 1)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_polr_1) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;)) %&gt;% dplyr::rename(none = prob.Not.at.All, part = prob.Part.Time, full = prob.Full.Time) %&gt;% tidyr::gather(key = work_level, value = fit, none, part, full) %&gt;% dplyr::mutate(work_level = factor(work_level, levels = c(&quot;none&quot;, &quot;part&quot;, &quot;full&quot;), labels = c(&quot;Not at All&quot;, &quot;Part Time&quot;, &quot;Full Time&quot;))) %&gt;% ggplot(aes(x = hincome, y = fit, color = children %&gt;% fct_rev, linetype = children %&gt;% fct_rev)) + geom_vline(xintercept = 14, color = &quot;gray25&quot;) + # reference line for intercept geom_line(size = 1) + facet_grid(. ~ work_level) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability&quot;, color = &quot;Children in\\nthe Home:&quot;, linetype = &quot;Children in\\nthe Home:&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1, &quot;cm&quot;)) + coord_cartesian(ylim = c(0, 1)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) "],
["references.html", "References", " References "]
]
