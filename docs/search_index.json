[["index.html", "Encyclopedia of Quantitative Methods in R, vol. 4: Multiple Linear Regression Welcome Blocked Notes Code and Output The Authors", " Encyclopedia of Quantitative Methods in R, vol. 4: Multiple Linear Regression Sarah Schwartz &amp; Tyson Barrett Last updated: 2021-09-02 Welcome Backgroup and links to other volumes of this encyclopedia may be found at the Encyclopedias Home Website. Blocked Notes Thoughout all the eBooks in this encyclopedia, several small secitons will be blocked out in the following ways: These blocks denote an area UNDER CONSTRUCTION, so check back often. This massive undertaking started during the summer of 2018 and is far from complete. The outline of seven volumes is given above despite any one being complete. Feedback is welcome via either authors email. These blocks denote something EXTREMELY IMPORTANT. Do NOT skip these notes as they will be used very sparingly. These blocks denote something to DOWNLOAD. This may include software installations, example datasets, or notebook code files. These blocks denote something INTERESTING. These point out information we found of interest or added value. These blocks denote LINKS to other websites. This may include instructional video clips, articles, or blog posts. We are all about NOT re-creating the wheel. If somebody else has described or illustrated a topic well, we celebrate it! Code and Output This is how \\(R\\) code is shown: 1 + 1 This is what the output of the \\(R\\) code above will look: ## [1] 2 The Authors Dr. Sarah Schwartz Dr. Tyson Barrett www.SarahSchwartzStats.com www.TysonBarrett.com Sarah.Schwartz@usu.edu Tyson.Barrett@usu.edu Statistical Consulting Studio Data Science and Discover Unit Why choose R ? Check it out: an article from Fall 2016 No more excuses: R is better than SPSS for psychology undergrads, and students agree FYI This entire encyclopedia is written in \\(R Markdown\\), using \\(R Studio\\) as the text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The books source code is hosted on GitHub. If you notice typos or other issues, feel free to email either of the authors. This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. "],["linear-correlation---example-cancer-experiment.html", "1 Linear Correlation - Example: Cancer Experiment 1.1 Background 1.2 Exploratory Data Analysis: i.e. the eyeball method 1.3 Pearsons Correlation Coefficient 1.4 Correlation Tables 1.5 Pairs Plots 1.6 Correlation Plots: Corrolagrams", " 1 Linear Correlation - Example: Cancer Experiment 1.1 Background 1.1.1 Required Packages library(tidyverse) # Loads several very helpful &#39;tidy&#39; packages library(haven) # Read in SPSS datasets library(psych) # Lots of nice tid-bits library(GGally) # Extension to &#39;ggplot2&#39; (ggpairs) 1.1.2 Example Dataset - Cancer Experiment The Cancer dataset: cancer_raw &lt;- haven::read_spss(&quot;https://raw.githubusercontent.com/CEHS-research/eBook_ANOVA/master/data/Cancer.sav&quot;) tibble::glimpse(cancer_raw) Observations: 25 Variables: 9 $ ID &lt;dbl&gt; 1, 5, 6, 9, 11, 15, 21, 26, 31, 35, 39, 41, 45, 2, 12... $ TRT &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,... $ AGE &lt;dbl&gt; 52, 77, 60, 61, 59, 69, 67, 56, 61, 51, 46, 65, 67, 4... $ WEIGHIN &lt;dbl&gt; 124.0, 160.0, 136.5, 179.6, 175.8, 167.6, 186.0, 158.... $ STAGE &lt;dbl&gt; 2, 1, 4, 1, 2, 1, 1, 3, 1, 1, 4, 1, 1, 2, 4, 1, 2, 1,... $ TOTALCIN &lt;dbl&gt; 6, 9, 7, 6, 6, 6, 6, 6, 6, 6, 7, 6, 8, 7, 6, 4, 6, 6,... $ TOTALCW2 &lt;dbl&gt; 6, 6, 9, 7, 7, 6, 11, 11, 9, 4, 8, 6, 8, 16, 10, 6, 1... $ TOTALCW4 &lt;dbl&gt; 6, 10, 17, 9, 16, 6, 11, 15, 6, 8, 11, 9, 9, 9, 11, 8... $ TOTALCW6 &lt;dbl&gt; 7, 9, 19, 3, 13, 11, 10, 15, 8, 7, 11, 6, 10, 10, 9, ... cancer_clean &lt;- cancer_raw %&gt;% dplyr::rename_all(tolower) %&gt;% dplyr::mutate(id = factor(id)) %&gt;% dplyr::mutate(trt = factor(trt, labels = c(&quot;Placebo&quot;, &quot;Aloe Juice&quot;))) %&gt;% dplyr::mutate(stage = factor(stage)) tibble::glimpse(cancer_clean) Observations: 25 Variables: 9 $ id &lt;fct&gt; 1, 5, 6, 9, 11, 15, 21, 26, 31, 35, 39, 41, 45, 2, 12... $ trt &lt;fct&gt; Placebo, Placebo, Placebo, Placebo, Placebo, Placebo,... $ age &lt;dbl&gt; 52, 77, 60, 61, 59, 69, 67, 56, 61, 51, 46, 65, 67, 4... $ weighin &lt;dbl&gt; 124.0, 160.0, 136.5, 179.6, 175.8, 167.6, 186.0, 158.... $ stage &lt;fct&gt; 2, 1, 4, 1, 2, 1, 1, 3, 1, 1, 4, 1, 1, 2, 4, 1, 2, 1,... $ totalcin &lt;dbl&gt; 6, 9, 7, 6, 6, 6, 6, 6, 6, 6, 7, 6, 8, 7, 6, 4, 6, 6,... $ totalcw2 &lt;dbl&gt; 6, 6, 9, 7, 7, 6, 11, 11, 9, 4, 8, 6, 8, 16, 10, 6, 1... $ totalcw4 &lt;dbl&gt; 6, 10, 17, 9, 16, 6, 11, 15, 6, 8, 11, 9, 9, 9, 11, 8... $ totalcw6 &lt;dbl&gt; 7, 9, 19, 3, 13, 11, 10, 15, 8, 7, 11, 6, 10, 10, 9, ... psych::headTail(cancer_clean) # A tibble: 9 x 9 id trt age weighin stage totalcin totalcw2 totalcw4 totalcw6 &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1 Placebo 52 124 2 6 6 6 7 2 5 Placebo 77 160 1 9 6 10 9 3 6 Placebo 60 136.5 4 7 9 17 19 4 9 Placebo 61 179.6 1 6 7 9 3 5 &lt;NA&gt; &lt;NA&gt; ... ... &lt;NA&gt; ... ... ... ... 6 42 Aloe Juice 73 181.5 0 8 11 16 &lt;NA&gt; 7 44 Aloe Juice 67 187 1 5 7 7 7 8 50 Aloe Juice 60 164 2 6 8 16 &lt;NA&gt; 9 58 Aloe Juice 54 172.8 4 7 8 10 8 1.2 Exploratory Data Analysis: i.e. the eyeball method 1.2.1 Scatterplot Always plot your data first! cancer_clean %&gt;% ggplot(aes(x = age, y = weighin)) + geom_count() + geom_smooth(method = &quot;lm&quot;) 1.3 Pearsons Correlation Coefficient 1.3.1 Using the Default Settings The cor.test() function needs at least TWO arguments: formula - The formula specifies the two variabels between which you would like to calcuate the correlation. Note at the two variable names come AFTER the tilda symbol and are separated with a plus sign: ~ continuous_var1 + continuous_var2 data - Since the datset is not the first argument in the function, you must use the period to signify that the datset is being piped from above data = . cancer_clean %&gt;% cor.test(~ age + weighin, # formula: order doesn&#39;t matter data = .) # data piped from above Pearson&#39;s product-moment correlation data: age and weighin t = -1.4401, df = 23, p-value = 0.1633 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.6130537 0.1213316 sample estimates: cor -0.2875868 1.3.2 Additional Arguments alternative - The cor.test() function defaults to the alternative = \"two.sided\". If you would like a one-sided alternative, you must choose which side you would like to test: alternative = \"greater\" to test for POSITIVE correlation or alternative = \"less\" to test for NEGATIVE correlation. method - The default is to calculate the Pearson correlation coefficient (method = \"pearson\"), but you may also specify the Kendalls tau (method = \"kendall\")or Spearmans rho (method = \"spearman\"), which are both non-parametric methods. conf.int - It also defaults to testing for the two-sided alternative and computing a 95% confidence interval (conf.level = 0.95), but this may be changed. Since the following code only specifies thedefaults, it Will give the same results as if you did not type out the last three lines (see above). cancer_clean %&gt;% cor.test(~ age + weighin, data = ., alternative = &quot;two.sided&quot;, # or &quot;greater&quot; (positive r) or &quot;less&quot; (negative r) method = &quot;pearson&quot;, # or &quot;kendall&quot; (tau) or &quot;spearman&quot; (rho) conf.level = .95) # or .90 or .99 (ect) Pearson&#39;s product-moment correlation data: age and weighin t = -1.4401, df = 23, p-value = 0.1633 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.6130537 0.1213316 sample estimates: cor -0.2875868 Non-Significant Correlation APA Results: There was no evidence of an association in overall oral condition from baseline to two week follow-up, $r(25) = -0.288 \\(p &lt; .163\\). 1.3.3 Statistical Significance cancer_clean %&gt;% ggplot(aes(x = totalcin, y = totalcw4)) + geom_count() + geom_smooth(method = &quot;lm&quot;) cancer_clean %&gt;% cor.test(~ totalcin + totalcw4, data = .) Pearson&#39;s product-moment correlation data: totalcin and totalcw4 t = 1.0911, df = 23, p-value = 0.2865 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.1899343 0.5672525 sample estimates: cor 0.2218459 Statistically Significant Correlation APA Results: Overall oral condition was positively correlated (\\(r = .763\\)) between weeks two and four, \\(t(21) = 5.409\\), \\(p &lt; .001\\). cancer_clean %&gt;% ggplot(aes(x = totalcw4, y = totalcw6)) + geom_point() + geom_smooth(method = &quot;lm&quot;) cancer_clean %&gt;% cor.test(~ totalcw4 + totalcw6, data = .) Pearson&#39;s product-moment correlation data: totalcw4 and totalcw6 t = 5.4092, df = 21, p-value = 2.296e-05 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.5117459 0.8940223 sample estimates: cor 0.762999 1.4 Correlation Tables The may use the tableC() function from the furniture package to calculate all pair-wise correlations between more than two variables and arrange them all in a table. The table is formatted with the variabels listed on the rows and numbered to show the same variabels across the columns. The cells ON the diagonal are all equal to exactly one, since each variable is perfectly correlated with itself. The cells ABOVE the diagonal are blank as them would just be a mirror image of the values below the diagonal. The cells BELOW the diagonal each contain the Pearsons correlation coefficients for each pair of variables, \\(r\\), with the \\(p-value\\) showing the significance vs. the null hypothesis for no association (\\(r = 0\\)) to the right. cancer_clean %&gt;% furniture::tableC(age, weighin, totalcin) ----------------------------------------------- [1] [2] [3] [1]age 1.00 [2]weighin -0.288 (0.163) 1.00 [3]totalcin 0.256 (0.217) 0.17 (0.418) 1.00 ----------------------------------------------- 1.4.1 Missing Values - Default Default Behavior na.rm = FALSE (default) If you dont say otherwise, the correlation \\(r\\) with not be calculated (NA) between any pair of variables for which there is at least one subject with a missing value on at least one of the vairables. This is a nice alert to make you aware of missing values. cancer_clean %&gt;% furniture::tableC(totalcin, totalcw2, totalcw4, totalcw6) ----------------------------------------------------- [1] [2] [3] [4] [1]totalcin 1.00 [2]totalcw2 0.314 (0.126) 1.00 [3]totalcw4 0.222 (0.287) 0.337 (0.099) 1.00 [4]totalcw6 NA NA NA NA NA NA 1.00 ----------------------------------------------------- 1.4.2 Missing Values - Listwise Deletion Listwise Deletion na.rm = TRUE Most of the time you will want to compute the correlation \\(r\\) is the precense of missing values. To do so, you want to remove or exclude subjects with missing data from ALL correlation computation in the table. This is called list-wise deletion. It ensures that all cells in the table refer to the exact same sub-sample (n = subjects with complete data for all variables in the table), and thus the same degrees of freedom (since \\(df = n - 2\\)). This is done be changing the default to na.rm = TRUE. cancer_clean %&gt;% furniture::tableC(totalcin, totalcw2, totalcw4, totalcw6, na.rm = TRUE) ------------------------------------------------------------- [1] [2] [3] [4] [1]totalcin 1.00 [2]totalcw2 0.282 (0.192) 1.00 [3]totalcw4 0.206 (0.346) 0.314 (0.145) 1.00 [4]totalcw6 0.098 (0.657) 0.378 (0.075) 0.763 (&lt;.001) 1.00 ------------------------------------------------------------- 1.5 Pairs Plots Helpful Website 1.5.1 Using Base R cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% pairs() 1.5.2 Using the psych package cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% psych::pairs.panels() 1.5.3 Using the ggplot2 and GGally packages cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% data.frame %&gt;% ggscatmat() cancer_clean %&gt;% data.frame %&gt;% ggscatmat(columns = c(&quot;age&quot;, &quot;weighin&quot;, &quot;totalcin&quot;, &quot;totalcw2&quot;, &quot;totalcw4&quot;, &quot;totalcw6&quot;), color = &quot;trt&quot;) 1.6 Correlation Plots: Corrolagrams 1.6.1 Simple Correlation Matrix (Base R) cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) age weighin totalcin totalcw2 totalcw4 age 1.00000000 -0.29909121 0.22386540 -0.1613892 0.09918029 weighin -0.29909121 1.00000000 0.16403694 0.2763478 -0.08013506 totalcin 0.22386540 0.16403694 1.00000000 0.2819648 0.20604650 totalcw2 -0.16138924 0.27634783 0.28196479 1.0000000 0.31354250 totalcw4 0.09918029 -0.08013506 0.20604650 0.3135425 1.00000000 totalcw6 0.03015273 -0.07750304 0.09786664 0.3780949 0.76299899 totalcw6 age 0.03015273 weighin -0.07750304 totalcin 0.09786664 totalcw2 0.37809488 totalcw4 0.76299899 totalcw6 1.00000000 1.6.2 Using the Default Settings cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot() 1.6.3 Changing the shape cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot(method = &quot;square&quot;) 1.6.4 Only Displaying Half cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot(method = &quot;ellipse&quot;, type = &quot;lower&quot;) 1.6.5 Mixing it up cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot.mixed() 1.6.6 Getting Fancy cancer_clean %&gt;% dplyr::select(age, weighin, totalcin, totalcw2, totalcw4, totalcw6) %&gt;% cor(method = &quot;pearson&quot;, use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot.mixed(upper = &quot;number&quot;, lower = &quot;ellipse&quot;) "],["simple-linear-regression---example-cancer-experiment.html", "2 Simple Linear Regression - Example: Cancer Experiment 2.1 Background 2.2 Exploratory Data Analysis: i.e. the eyeball method", " 2 Simple Linear Regression - Example: Cancer Experiment 2.1 Background 2.1.1 Required Packages library(tidyverse) # Loads several very helpful &#39;tidy&#39; packages library(haven) # Read in SPSS datasets library(psych) # Lots of nice tid-bits library(GGally) # Extension to &#39;ggplot2&#39; (ggpairs) 2.1.2 Example Dataset - Cancer Experiment The Cancer dataset: cancer_raw &lt;- haven::read_spss(&quot;https://raw.githubusercontent.com/CEHS-research/eBook_ANOVA/master/data/Cancer.sav&quot;) tibble::glimpse(cancer_raw) Observations: 25 Variables: 9 $ ID &lt;dbl&gt; 1, 5, 6, 9, 11, 15, 21, 26, 31, 35, 39, 41, 45, 2, 12... $ TRT &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,... $ AGE &lt;dbl&gt; 52, 77, 60, 61, 59, 69, 67, 56, 61, 51, 46, 65, 67, 4... $ WEIGHIN &lt;dbl&gt; 124.0, 160.0, 136.5, 179.6, 175.8, 167.6, 186.0, 158.... $ STAGE &lt;dbl&gt; 2, 1, 4, 1, 2, 1, 1, 3, 1, 1, 4, 1, 1, 2, 4, 1, 2, 1,... $ TOTALCIN &lt;dbl&gt; 6, 9, 7, 6, 6, 6, 6, 6, 6, 6, 7, 6, 8, 7, 6, 4, 6, 6,... $ TOTALCW2 &lt;dbl&gt; 6, 6, 9, 7, 7, 6, 11, 11, 9, 4, 8, 6, 8, 16, 10, 6, 1... $ TOTALCW4 &lt;dbl&gt; 6, 10, 17, 9, 16, 6, 11, 15, 6, 8, 11, 9, 9, 9, 11, 8... $ TOTALCW6 &lt;dbl&gt; 7, 9, 19, 3, 13, 11, 10, 15, 8, 7, 11, 6, 10, 10, 9, ... cancer_clean &lt;- cancer_raw %&gt;% dplyr::rename_all(tolower) %&gt;% dplyr::mutate(id = factor(id)) %&gt;% dplyr::mutate(trt = factor(trt, labels = c(&quot;Placebo&quot;, &quot;Aloe Juice&quot;))) %&gt;% dplyr::mutate(stage = factor(stage)) tibble::glimpse(cancer_clean) Observations: 25 Variables: 9 $ id &lt;fct&gt; 1, 5, 6, 9, 11, 15, 21, 26, 31, 35, 39, 41, 45, 2, 12... $ trt &lt;fct&gt; Placebo, Placebo, Placebo, Placebo, Placebo, Placebo,... $ age &lt;dbl&gt; 52, 77, 60, 61, 59, 69, 67, 56, 61, 51, 46, 65, 67, 4... $ weighin &lt;dbl&gt; 124.0, 160.0, 136.5, 179.6, 175.8, 167.6, 186.0, 158.... $ stage &lt;fct&gt; 2, 1, 4, 1, 2, 1, 1, 3, 1, 1, 4, 1, 1, 2, 4, 1, 2, 1,... $ totalcin &lt;dbl&gt; 6, 9, 7, 6, 6, 6, 6, 6, 6, 6, 7, 6, 8, 7, 6, 4, 6, 6,... $ totalcw2 &lt;dbl&gt; 6, 6, 9, 7, 7, 6, 11, 11, 9, 4, 8, 6, 8, 16, 10, 6, 1... $ totalcw4 &lt;dbl&gt; 6, 10, 17, 9, 16, 6, 11, 15, 6, 8, 11, 9, 9, 9, 11, 8... $ totalcw6 &lt;dbl&gt; 7, 9, 19, 3, 13, 11, 10, 15, 8, 7, 11, 6, 10, 10, 9, ... psych::headTail(cancer_clean) # A tibble: 9 x 9 id trt age weighin stage totalcin totalcw2 totalcw4 totalcw6 &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1 Placebo 52 124 2 6 6 6 7 2 5 Placebo 77 160 1 9 6 10 9 3 6 Placebo 60 136.5 4 7 9 17 19 4 9 Placebo 61 179.6 1 6 7 9 3 5 &lt;NA&gt; &lt;NA&gt; ... ... &lt;NA&gt; ... ... ... ... 6 42 Aloe Juice 73 181.5 0 8 11 16 &lt;NA&gt; 7 44 Aloe Juice 67 187 1 5 7 7 7 8 50 Aloe Juice 60 164 2 6 8 16 &lt;NA&gt; 9 58 Aloe Juice 54 172.8 4 7 8 10 8 2.2 Exploratory Data Analysis: i.e. the eyeball method 2.2.1 Scatterplot Always plot your data first! cancer_clean %&gt;% ggplot(aes(x = age, y = weighin)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE, color = &quot;blue&quot;) + # straight line (linear model) geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;red&quot;) # loess line (moving window) "],["simple-linear-regression---ex-ventricular-shortening-velocity-single-continuous-iv.html", "3 Simple Linear Regression - Ex: Ventricular Shortening Velocity (single continuous IV) 3.1 Purpose 3.2 Exploratory Data Analysis 3.3 Regression Analysis 3.4 Conclusion", " 3 Simple Linear Regression - Ex: Ventricular Shortening Velocity (single continuous IV) library(tidyverse) # super helpful everything! library(magrittr) # includes other versions of the pipe library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools library(ISwR) # Introduction to Statistics with R (datasets) 3.1 Purpose 3.1.1 Research Question Is there a relationship between fasting blood flucose and shortening of ventricular velocity among type 1 diabetic patiences? If so, what is the nature of the association? 3.1.2 Data Description This dataset is included in the ISwR package (Dalgaard 2020), which was a companion to the texbook Introductory Statistics with R, 2nd ed. (Dalgaard 2008), although it was first published by Altman (1991) in table 11.6. The thuesen data frame has 24 rows and 2 columns. It contains ventricular shortening velocity and blood glucose for type 1 diabetic patients. blood.glucose a numeric vector, fasting blood glucose (mmol/l). short.velocity a numeric vector, mean circumferential shortening velocity (%/s). data(thuesen, package = &quot;ISwR&quot;) tibble::glimpse(thuesen) # view the class and 1st few values of each variable Observations: 24 Variables: 2 $ blood.glucose &lt;dbl&gt; 15.3, 10.8, 8.1, 19.5, 7.2, 5.3, 9.3, 11.1, 7.5... $ short.velocity &lt;dbl&gt; 1.76, 1.34, 1.27, 1.47, 1.27, 1.49, 1.31, 1.09,... 3.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 3.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). thuesen %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max blood.glucose 24 10.300 4.338 4.200 7.075 12.700 19.500 short.velocity 23 1.326 0.233 1.030 1.185 1.420 1.950 The stargazer() function has many handy options, should you wish to change the default settings. thuesen %&gt;% stargazer::stargazer(type = &quot;html&quot;, digits = 4, flip = TRUE, summary.stat = c(&quot;n&quot;, &quot;mean&quot;, &quot;sd&quot;, &quot;min&quot;, &quot;median&quot;, &quot;max&quot;), title = &quot;Descriptives&quot;) Descriptives Statistic blood.glucose short.velocity N 24 23 Mean 10.3000 1.3257 St. Dev. 4.3375 0.2329 Min 4.2000 1.0300 Median 9.4000 1.2700 Max 19.5000 1.9500 Although the table1() function from the furniture package creates a nice summary table, it hides the nubmer of missing values for each continuous variable (Barrett, Brignone, and Laxman 2021). thuesen %&gt;% furniture::table1(&quot;Fasting Blood Glucose&quot; = blood.glucose, &quot;Circumferential Shortening Velocity&quot; = short.velocity, # defaults to excluding any row missing any variable output = &quot;html&quot;) Mean/Count (SD/%) n = 23 Fasting Blood Glucose 10.4 (4.4) Circumferential Shortening Velocity 1.3 (0.2) thuesen %&gt;% furniture::table1(&quot;Fasting Blood Glucose&quot; = blood.glucose, &quot;Circumferential Shortening Velocity&quot; = short.velocity, na.rm = FALSE, # retains even partial cases output = &quot;html&quot;) Mean/Count (SD/%) n = 24 Fasting Blood Glucose 10.3 (4.3) Circumferential Shortening Velocity 1.3 (0.2) 3.2.2 Univariate Visualizations ggplot(thuesen, aes(blood.glucose)) + # variable of interest (just one) geom_histogram(binwidth = 2) # specify the width of the bars thuesen %&gt;% ggplot() + aes(short.velocity) + # variable of interest (just one) geom_histogram(bins = 10) # specify the number of bars 3.2.3 Bivariate Statistics (Unadjusted Pearsons correlation) The cor() fucntion in base \\(R\\) doesnt like NA or missing values thuesen %&gt;% cor() blood.glucose short.velocity blood.glucose 1 NA short.velocity NA 1 You may specify how to handle cases that are missing on at least one of the variables of interest: use = \"everything\" NAs will propagate conceptually, i.e., a resulting value will be NA whenever one of its contributing observations is NA &lt; DEFAULT use = \"all.obs\" the presence of missing observations will produce an error use = \"complete.obs\" missing values are handled by casewise deletion (and if there are no complete cases, that gives an error). use = \"na.or.complete\" is the same as above unless there are no complete cases, that gives NA use = \"pairwise.complete.obs\" the correlation between each pair of variables is computed using all complete pairs of observations on those variables. This can result in covariance matrices which are not positive semi-definite, as well as NA entries if there are no complete pairs for that pair of variables. Commonly, we want listwise deletion: thuesen %&gt;% cor(use = &quot;complete.obs&quot;) # list-wise deletion blood.glucose short.velocity blood.glucose 1.0000000 0.4167546 short.velocity 0.4167546 1.0000000 It is also handy to specify the number of decimal places desired, but adding a rounding step: thuesen %&gt;% cor(use = &quot;complete.obs&quot;) %&gt;% round(2) # number od decimal places blood.glucose short.velocity blood.glucose 1.00 0.42 short.velocity 0.42 1.00 If you desire a correlation single value of a single PAIR of variables, instead of a matrix, then you must use a magrittr exposition pipe (%$%) thuesen %$% # notice the special kind of pipe cor(blood.glucose, short.velocity, # specify exactly TWO variables use = &quot;complete.obs&quot;) [1] 0.4167546 In addition to the cor() funciton, the base \\(R\\) stats package also includes the cor.test() function to test if the correlation is zero (\\(H_0: R = 0\\)) This TESTS if the cor == 0 thuesen %$% # notice the special kind of pipe cor.test(blood.glucose, short.velocity, # specify exactly TWO variables use=&quot;complete.obs&quot;) Pearson&#39;s product-moment correlation data: blood.glucose and short.velocity t = 2.101, df = 21, p-value = 0.0479 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.005496682 0.707429479 sample estimates: cor 0.4167546 The default correltaion type for cor()is Pearsons \\(R\\), which assesses linear relationships. Spearmans correlation assesses monotonic relationships. thuesen %$% # notice the special kind of pipe cor(blood.glucose, short.velocity, # specify exactly TWO variables use = &#39;complete&#39;, method = &#39;spearman&#39;) # spearman&#39;s (rho) [1] 0.318002 thuesen %&gt;% dplyr::select(blood.glucose, short.velocity) %&gt;% furniture::tableC(cor_type = &quot;pearson&quot;, na.rm = TRUE, rounding = 3, output = &quot;markdown&quot;, booktabs = TRUE, caption = &quot;Correlation Table&quot;) [1] [2] [1]blood.glucose 1.00 [2]short.velocity 0.417 (0.048) 1.00 3.2.4 Bivariate Visualization Scatterplots show the relationship between two continuous measures (one on the \\(x-axis\\) and the other on the \\(y-axis\\)), with one point for each observation. ggplot(thuesen, aes(x = blood.glucose, # x-axis variable y = short.velocity)) + # y-axis variable geom_point() + # place a point for each observation theme_bw() # black-and-white theme Both the code chunk above and below produce the same plot. thuesen %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = short.velocity) + # y-axis variable geom_point() + # place a point for each observation theme_bw() # black-and-white theme 3.3 Regression Analysis 3.3.1 Fit A Simple Linear Model \\[ Y = \\beta_0 + \\beta_1 \\times X \\] short.velocity dependent variable or outcome (\\(Y\\)) blood.glucose independent variable or predictor (\\(X\\)) The lm() function must be supplied with at least two options: a formula: Y ~ X a dataset: data = XXXXXXX When a model is fit and directly saved as a named object via the assignment opperator (&lt;-), no output is produced. fit_vel_glu &lt;- lm(short.velocity ~ blood.glucose, data = thuesen) Running the name of the fit object yields very little output: fit_vel_glu Call: lm(formula = short.velocity ~ blood.glucose, data = thuesen) Coefficients: (Intercept) blood.glucose 1.09781 0.02196 Appling the summary() funciton produced a good deal more output: summary(fit_vel_glu) Call: lm(formula = short.velocity ~ blood.glucose, data = thuesen) Residuals: Min 1Q Median 3Q Max -0.40141 -0.14760 -0.02202 0.03001 0.43490 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.09781 0.11748 9.345 6.26e-09 *** blood.glucose 0.02196 0.01045 2.101 0.0479 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2167 on 21 degrees of freedom (1 observation deleted due to missingness) Multiple R-squared: 0.1737, Adjusted R-squared: 0.1343 F-statistic: 4.414 on 1 and 21 DF, p-value: 0.0479 You may request specific pieces of the output: Coefficients or beta estimates: coef(fit_vel_glu) (Intercept) blood.glucose 1.09781488 0.02196252 95% confidence intervals for the coefficients or beta estimates: confint(fit_vel_glu) 2.5 % 97.5 % (Intercept) 0.8534993816 1.34213037 blood.glucose 0.0002231077 0.04370194 The F-test for overall modle fit vs. a \\(null\\) or empty model having only an intercept and no predictors. anova(fit_vel_glu) # A tibble: 2 x 5 Df `Sum Sq` `Mean Sq` `F value` `Pr(&gt;F)` &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0.207 0.207 4.41 0.0479 2 21 0.986 0.0470 NA NA Various other model fit indicies: logLik(fit_vel_glu) &#39;log Lik.&#39; 3.583612 (df=3) AIC(fit_vel_glu) [1] -1.167223 BIC(fit_vel_glu) [1] 2.239259 3.3.2 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_vel_glu, which = 1) plot(fit_vel_glu, which = 2) plot(fit_vel_glu, which = 5) plot(fit_vel_glu, which = 6) Viewing potentially influencial or outlier points based on plots above: thuesen %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::filter(id == c(13, 20, 24)) # A tibble: 3 x 3 blood.glucose short.velocity id &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 19 1.95 13 2 16.1 1.05 20 3 9.5 1.7 24 The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2021). car::residualPlots(fit_vel_glu) Test stat Pr(&gt;|Test stat|) blood.glucose 0.9289 0.3640 Tukey test 0.9289 0.3529 Here is a fancy way to visulaize potential problem cases with ggplot2: thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # keep only complete cases ggplot() + # name the FULL dataset aes(x = blood.glucose, # x-axis variable name y = short.velocity) + # y-axis variable name geom_point() + # do a scatterplot stat_smooth(method = &quot;lm&quot;) + # smooth: linear model theme_bw() + # black-and-while theme geom_point(data = thuesen %&gt;% # override the dataset from above filter(row_number() == c(13, 20, 24)), # with a reduced subset of cases size = 4, # make the points bigger in size color = &quot;red&quot;) # give the points a different color 3.3.3 Manually checking residual diagnostics You may extract values from the model in dataset form and then you can maually plot the residuals. thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # keep only complete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) # residual values # A tibble: 23 x 4 blood.glucose short.velocity pred resid &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 15.3 1.76 1.43 0.326 2 10.8 1.34 1.34 0.00499 3 8.1 1.27 1.28 -0.00571 4 19.5 1.47 1.53 -0.0561 5 7.2 1.27 1.26 0.0141 6 5.3 1.49 1.21 0.276 7 9.3 1.31 1.30 0.00793 8 11.1 1.09 1.34 -0.252 9 7.5 1.18 1.26 -0.0825 10 12.2 1.22 1.37 -0.146 # ... with 13 more rows Check for equal spread of points along the \\(y=0\\) horizontal line: thuesen %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # keep only complete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) %&gt;% # residual values ggplot() + aes(x = id, y = resid) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, size = 1, linetype = &quot;dashed&quot;) + theme_classic() + labs(title = &quot;Looking for homogeneity of residuals&quot;, subtitle = &quot;want to see equal spread all across&quot;) Check for normality: thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # keep only complete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) %&gt;% # residual values ggplot() + aes(resid) + geom_histogram(bins = 12, color = &quot;blue&quot;, fill = &quot;blue&quot;, alpha = 0.3) + geom_vline(xintercept = 0, size = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + theme_classic() + labs(title = &quot;Looking for normality of residuals&quot;, subtitle = &quot;want to see roughly a bell curve&quot;) 3.4 Conclusion 3.4.1 Tabulate the Final Model Summary You may also present the output in a table using two different packages: The stargazer package has stargazer() function: stargazer::stargazer(fit_vel_glu, type = &quot;html&quot;) Dependent variable: short.velocity blood.glucose 0.022** (0.010) Constant 1.098*** (0.117) Observations 23 R2 0.174 Adjusted R2 0.134 Residual Std. Error 0.217 (df = 21) F Statistic 4.414** (df = 1; 21) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 The stargazer package can produce the regression table in various output types: type = \"latex Default Use when knitting your .Rmd file to a .pdf via LaTeX type = \"text Default Use when working on a project and viewing tables on your computer screen type = \"html Default Use when knitting your .Rmd file to a .html document The texreg package has the texreg() fucntion: texreg::htmlreg(fit_vel_glu) &lt;!DOCTYPE HTML PUBLIC -//W3C//DTD HTML 4.01 Transitional//EN http://www.w3.org/TR/html4/loose.dtd&gt; Statistical models Model 1 (Intercept) 1.10*** (0.12) blood.glucose 0.02* (0.01) R2 0.17 Adj. R2 0.13 Num. obs. 23 RMSE 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 The texreg package contains three version of the regression table function. screenreg() Use when working on a project and viewing tables on your computer screen htmlreg() Use when knitting your .Rmd file to a .html document texreg() Use when knitting your .Rmd file to a .pdf via LaTeX 3.4.2 Plot the Model When a model only contains main effects, a plot is not important for interpretation, but can help understand the relationship between multiple predictors. The Effect() function from the effects package chooses 5 or 6 nice values for your continuous independent variable (\\(X\\)) based on the range of values found in the dataset on which the model was fit and plugs them into the regression equation \\(Y = \\beta_0 + \\beta_1 \\times X\\) to compute the predicted mean value of the outcome (\\(Y\\)) (Fox et al. 2020). effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), # IV variable name mod = fit_vel_glu) # fitted model name blood.glucose effect blood.glucose 4.2 8 12 16 20 1.190057 1.273515 1.361365 1.449215 1.537065 You may override the nice values using the xlevels = list(var_name = c(#, #, ...#) option. effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = c(5, 10, 15, 20))) blood.glucose effect blood.glucose 5 10 15 20 1.207627 1.317440 1.427253 1.537065 Adding a piped data frame step (%&gt;% data.frame()) will arrange the predicted \\(Y\\) values into a column called fit. This tidy data format is ready for plotting. effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu) %&gt;% data.frame() # A tibble: 5 x 5 blood.glucose fit se lower upper &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4.2 1.19 0.0788 1.03 1.35 2 8 1.27 0.0516 1.17 1.38 3 12 1.36 0.0483 1.26 1.46 4 16 1.45 0.0742 1.29 1.60 5 20 1.54 0.110 1.31 1.77 effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = c(5, 12, 20))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .5) + # ribbon transparency level geom_line() + theme_bw() Notice that although the regression line is smooth, the ribbon is choppy. This is because we are basing it on only THREE values of \\(X\\). c(5, 12, 20) [1] 5 12 20 Use the seq() function in base \\(R\\) to request many values of \\(X\\) seq(from = 5, to = 20, by = 5) [1] 5 10 15 20 seq(from = 5, to = 20, by = 2) [1] 5 7 9 11 13 15 17 19 seq(from = 5, to = 20, by = 1) [1] 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 seq(from = 5, to = 20, by = .5) [1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 11.5 [15] 12.0 12.5 13.0 13.5 14.0 14.5 15.0 15.5 16.0 16.5 17.0 17.5 18.0 18.5 [29] 19.0 19.5 20.0 effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .5) + # ribbon transparency level geom_line() + theme_bw() Now that we are basing our ribbon on MANY more points of \\(X\\), the ribbon is much smoother. For publication, you would of course want to clean up the plot a bit more: effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .3) + # ribbon transparency level geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) # axis labels The above plot has a ribbon that represents a 95% confidence interval (lower toupper) for the MEAN (fit) outcome. Sometimes we would rather display a ribbon for only the MEAN (fit) plus-or-minus ONE STANDARD ERROR (se) for the mean. You would do that by changing the variables that define the min and max edges of the ribbon (notice the range of the y-axis has changed): effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, y = fit) + geom_ribbon(aes(ymin = fit - se, # bottom edge of the ribbon ymax = fit + se), # top edge of the ribbon alpha = .3) + geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) Of course, you could do both ribbons together: effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, y = fit) + geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon = lower of the 95% CI ymax = upper), # top edge of the ribbon = upper of the 95% CI alpha = .3) + geom_ribbon(aes(ymin = fit - se, # bottom edge of the ribbon = mean - SE ymax = fit + se), # top edge of the ribbon = Mean + SE alpha = .3) + geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) # axis labels "],["multiple-linear-regression---ex-obesity-and-blood-pressure-interaction-between-a-continuous-and-categorical-ivs.html", "4 Multiple Linear Regression - Ex: Obesity and Blood Pressure (interaction between a continuous and categorical IVs) 4.1 Purpose 4.2 Exploratory Data Analysis 4.3 Regression Analysis 4.4 Conclusion", " 4 Multiple Linear Regression - Ex: Obesity and Blood Pressure (interaction between a continuous and categorical IVs) library(tidyverse) # super helpful everything! library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools library(GGally) # extensions to ggplot2 library(ISwR) # Introduction to Statistics with R (datasets) 4.1 Purpose 4.1.1 Research Question Is obsesity associated with higher blood pressure and is that relationship the same among men and women? 4.1.2 Data Description This dataset is included in the ISwR package (Dalgaard 2020), which was a companion to the texbook Introductory Statistics with R, 2nd ed. (Dalgaard 2008), although it was first published by Brown and Hollander (1977). To view the documentation for the dataset, type ?bp.obese in the console and enter or search the help tab for `bp.obese. The bp.obese data frame has 102 rows and 3 columns. It contains data from a random sample of Mexican-American adults in a small California town. This data frame contains the following columns: sex a numeric vector code, 0: male, 1: female obese a numeric vector, ratio of actual weight to ideal weight from New York Metropolitan Life Tables bp a numeric vector,systolic blood pressure (mm Hg) data(bp.obese, package = &quot;ISwR&quot;) bp.obese &lt;- bp.obese %&gt;% dplyr::mutate(sex = factor(sex, labels = c(&quot;Male&quot;, &quot;Female&quot;))) tibble::glimpse(bp.obese) Observations: 102 Variables: 3 $ sex &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Ma... $ obese &lt;dbl&gt; 1.31, 1.31, 1.19, 1.11, 1.34, 1.17, 1.56, 1.18, 1.04, 1.... $ bp &lt;int&gt; 130, 148, 146, 122, 140, 146, 132, 110, 124, 150, 120, 1... 4.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 4.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). bp.obese %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max obese 102 1.313 0.258 0.810 1.143 1.430 2.390 bp 102 127.020 18.184 94 116 137.5 208 4.2.2 Bivariate Relationships The furniture packages table1() function is a clean way to create a descriptive table that compares distinct subgroups of your sample (Barrett, Brignone, and Laxman 2021). bp.obese %&gt;% furniture::table1(obese, bp, splitby = ~ sex, test = TRUE, output = &quot;html&quot;) Male Female P-Value n = 44 n = 58 obese &lt;.001 1.2 (0.2) 1.4 (0.3) bp 0.653 128.0 (16.6) 126.3 (19.4) The ggpairs() function in the GGally package is helpful for showing all pairwise relationships in raw data, especially seperating out two or three groups (Schloerke et al. 2021). GGally::ggpairs(bp.obese, mapping = aes(fill = sex, col = sex, alpha = 0.1), upper = list(continuous = &quot;smooth&quot;, combo = &quot;facethist&quot;, discrete = &quot;ratio&quot;), lower = list(continuous = &quot;cor&quot;, combo = &quot;box&quot;, discrete = &quot;facetbar&quot;), title = &quot;Very Useful for Exploring Data&quot;) bp.obese %&gt;% ggplot() + aes(x = sex, y = bp, fill = sex) + geom_boxplot(alpha = 0.6) + scale_fill_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;)) + labs(x = &quot;Gender&quot;, y = &quot;Blood Pressure (mmHg)&quot;) + guides(fill = FALSE) + theme_bw() Visual inspection for an interaction (is gender a moderator?) bp.obese %&gt;% ggplot(aes(x = obese, y = bp, color = sex)) + geom_point(size = 3) + geom_smooth(aes(fill = sex), alpha = 0.2, method = &quot;lm&quot;) + scale_color_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;), breaks = c(&quot;male&quot;, &quot;female&quot;), labels = c(&quot;Men&quot;, &quot;Women&quot;)) + scale_fill_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;), breaks = c(&quot;male&quot;, &quot;female&quot;), labels = c(&quot;Men&quot;, &quot;Women&quot;)) + labs(title = &quot;Does Gender Moderate the Association Between Obesity and Blood Pressure?&quot;, x = &quot;Ratio: Actual Weight vs. Ideal Weight (NYM Life Tables)&quot;, y = &quot;Systolic Blood Pressure (mmHg)&quot;) + theme_bw() + scale_x_continuous(breaks = seq(from = 0, to = 3, by = 0.25 )) + scale_y_continuous(breaks = seq(from = 75, to = 300, by = 25)) + theme(legend.title = element_blank(), legend.key = element_rect(fill = &quot;white&quot;), legend.background = element_rect(color = &quot;black&quot;), legend.justification = c(1, 0), legend.position = c(1, 0)) bp.obese %&gt;% dplyr::mutate(sex = as.numeric(sex)) %&gt;% # cor needs only numeric cor() %&gt;% round(3) sex obese bp sex 1.000 0.405 -0.045 obese 0.405 1.000 0.326 bp -0.045 0.326 1.000 Often it is easier to digest a correlation matrix if it is visually presented, instead of just given as a table of many numbers. The corrplot package has a useful function called corrplot.mixed() for doing just that (Wei and Simko 2021). bp.obese %&gt;% dplyr::mutate(sex = as.numeric(sex)) %&gt;% # cor needs only numeric cor() %&gt;% corrplot::corrplot.mixed(lower = &quot;ellipse&quot;, upper = &quot;number&quot;, tl.col = &quot;black&quot;) 4.3 Regression Analysis 4.3.1 Fit Nested Models The bottom-up approach consists of starting with an initial NULL model with only an intercept term and them building additional models that are nested. Two models are considered nested if one is conains a subset of the terms (predictors or IV) compared to the other. fit_bp_null &lt;- lm(bp ~ 1, data = bp.obese) # intercept only or NULL model fit_bp_sex &lt;- lm(bp ~ sex, data = bp.obese) fit_bp_obe &lt;- lm(bp ~ obese, data = bp.obese) fit_bp_obesex &lt;- lm(bp ~ obese + sex, data = bp.obese) fit_bp_inter &lt;- lm(bp ~ obese*sex, data = bp.obese) 4.3.2 Comparing Nested Models 4.3.2.1 Model Comparison Table In single level, multiple linear regression significance of predictors (independent variables, IV) is usually based on both the Wald tests of significance for each beta estimate (shown with stars here) and comparisons in the model fit via the \\(R^2\\) values. Again the texreg package comes in handy to display several models in the same tal e (Leifeld 2020). texreg::htmlreg(list(fit_bp_null, fit_bp_sex, fit_bp_obe, fit_bp_obesex, fit_bp_inter), custom.model.names = c(&quot;No Predictors&quot;, &quot;Only Sex Quiz&quot;, &quot;Only Obesity&quot;, &quot;Both IVs&quot;, &quot;Add Interaction&quot;)) &lt;!DOCTYPE HTML PUBLIC -//W3C//DTD HTML 4.01 Transitional//EN http://www.w3.org/TR/html4/loose.dtd&gt; Statistical models No Predictors Only Sex Quiz Only Obesity Both IVs Add Interaction (Intercept) 127.02*** 127.95*** 96.82*** 93.29*** 102.11*** (1.80) (2.75) (8.92) (8.94) (18.23) sexFemale -1.64 -7.73* -19.60 (3.65) (3.72) (21.66) obese 23.00*** 29.04*** 21.65 (6.67) (7.17) (15.12) obese:sexFemale 9.56 (17.19) R2 0.00 0.00 0.11 0.14 0.15 Adj. R2 0.00 -0.01 0.10 0.13 0.12 Num. obs. 102 102 102 102 102 RMSE 18.18 18.26 17.28 17.00 17.05 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.3.2.2 Likelihood Ratio Test of Nested Models An alternative method for determing model fit and variable importance is the likelihood ratio test. This involves comparing the \\(-2LL\\) or inverse of twice the log of the likelihood value for the model. The difference in these values follows a Chi Squared distribution with degrees of freedom equal to the difference in the number of parameters estimated (number of betas). Test the main effect of math quiz: anova(fit_bp_null, fit_bp_sex) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 101 33398. NA NA NA NA 2 100 33330. 1 67.6 0.203 0.653 Test the main effect of math phobia anova(fit_bp_null, fit_bp_obe) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 101 33398. NA NA NA NA 2 100 29846. 1 3552. 11.9 0.000822 Test the main effect of math phobia, after controlling for math test anova(fit_bp_obe, fit_bp_obesex) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 100 29846. NA NA NA NA 2 99 28595. 1 1250. 4.33 0.0401 Test the interaction between math test and math phobia (i.e. moderation) anova(fit_bp_obesex, fit_bp_inter) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 99 28595. NA NA NA NA 2 98 28505. 1 89.9 0.309 0.579 4.3.3 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_bp_obesex, which = 1) plot(fit_bp_obesex, which = 4, id.n = 10) # Change the number labeled The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2021). car::residualPlots(fit_bp_obesex) Test stat Pr(&gt;|Test stat|) obese -0.2759 0.7832 sex Tukey test -0.6141 0.5391 you can adjust any part of a ggplot bp.obese %&gt;% dplyr::mutate(e_bp = resid(fit_bp_obesex)) %&gt;% # add the resid to the dataset ggplot(aes(x = sex, # x-axis variable name y = e_bp, # y-axis variable name color = sex, # color is the outline fill = sex)) + # fill is the inside geom_hline(yintercept = 0, # set at a meaningful value size = 1, # adjust line thickness linetype = &quot;dashed&quot;, # set type of line color = &quot;purple&quot;) + # color of line geom_boxplot(alpha = 0.5) + # level of transparency theme_bw() + # my favorite theme labs(title = &quot;Check Assumptions&quot;, # main title&#39;s text x = &quot;Gender&quot;, # x-axis text label y = &quot;Blood Pressure, Residual (bpm)&quot;) + # y-axis text label scale_y_continuous(breaks = seq(from = -40, # declare a sequence of to = 80, # values to make the by = 20)) + # tick marks at guides(color = FALSE, fill = FALSE) # no legends included bp.obese %&gt;% dplyr::mutate(e_bp = resid(fit_bp_obesex)) %&gt;% # add the resid to the dataset ggplot(aes(x = e_bp, # y-axis variable name color = sex, # color is the outline fill = sex)) + # fill is the inside geom_density(alpha = 0.5) + geom_vline(xintercept = 0, # set at a meaningful value size = 1, # adjust line thickness linetype = &quot;dashed&quot;, # set type of line color = &quot;purple&quot;) + # color of line theme_bw() + # my favorite theme labs(title = &quot;Check Assumptions&quot;, # main title&#39;s text x = &quot;Blood Pressure, Residual (bpm)&quot;) + # y-axis text label scale_x_continuous(breaks = seq(from = -40, # declare a sequence of to = 80, # values to make the by = 20)) # tick marks at 4.4 Conclusion Violations to the assumtions call the reliabity of the regression results into question. The data should be further investigated, specifically the \\(102^{nd}\\) case. "],["multiple-linear-regression---ex-ihnos-experiment-interaction-between-two-continuous-ivs.html", "5 Multiple Linear Regression - Ex: Ihnos Experiment (interaction between two continuous IVs) 5.1 Purpose 5.2 Exploratory Data Analysis 5.3 Regression Analysis 5.4 Conclusion 5.5 Write-up 5.6 New Playground", " 5 Multiple Linear Regression - Ex: Ihnos Experiment (interaction between two continuous IVs) library(tidyverse) # super helpful everything! library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools 5.1 Purpose 5.1.1 Research Question Does math phobia moderate the relationship between math and statistics performance? That is, does the assocation between math and stat quiz performance differ at variaous levels of math phobia? 5.1.2 Data Description Inhos dataset is included in the textbook Explaining Psychological Statistics (Cohen 2013) and details regarding the sample and measures is describe in this Encyclopedias Vol. 2 - Ihnos Dataset. data_ihno &lt;- haven::read_spss(&quot;https://raw.githubusercontent.com/CEHS-research/eBook_regression/master/data/Ihno_dataset.sav&quot;) %&gt;% dplyr::rename_all(tolower) %&gt;% dplyr::mutate(gender = factor(gender, levels = c(1, 2), labels = c(&quot;Female&quot;, &quot;Male&quot;))) %&gt;% dplyr::mutate(major = factor(major, levels = c(1, 2, 3, 4,5), labels = c(&quot;Psychology&quot;, &quot;Premed&quot;, &quot;Biology&quot;, &quot;Sociology&quot;, &quot;Economics&quot;))) %&gt;% dplyr::mutate(reason = factor(reason, levels = c(1, 2, 3), labels = c(&quot;Program requirement&quot;, &quot;Personal interest&quot;, &quot;Advisor recommendation&quot;))) %&gt;% dplyr::mutate(exp_cond = factor(exp_cond, levels = c(1, 2, 3, 4), labels = c(&quot;Easy&quot;, &quot;Moderate&quot;, &quot;Difficult&quot;, &quot;Impossible&quot;))) %&gt;% dplyr::mutate(coffee = factor(coffee, levels = c(0, 1), labels = c(&quot;Not a regular coffee drinker&quot;, &quot;Regularly drinks coffee&quot;))) %&gt;% dplyr::mutate(mathquiz = as.numeric(mathquiz)) tibble::glimpse(data_ihno) Rows: 100 Columns: 18 $ sub_num &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18~ $ gender &lt;fct&gt; Female, Female, Female, Female, Female, Female, Female, Femal~ $ major &lt;fct&gt; Psychology, Psychology, Psychology, Psychology, Psychology, P~ $ reason &lt;fct&gt; Advisor recommendation, Personal interest, Program requiremen~ $ exp_cond &lt;fct&gt; Easy, Easy, Easy, Easy, Easy, Moderate, Moderate, Moderate, M~ $ coffee &lt;fct&gt; Regularly drinks coffee, Not a regular coffee drinker, Not a ~ $ num_cups &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 2, 0, 2, 1, 0, 1, 2, 3, 0, 0, 3, 2, 1, 2~ $ phobia &lt;dbl&gt; 1, 1, 4, 4, 10, 4, 4, 4, 4, 5, 5, 4, 7, 4, 3, 8, 4, 5, 0, 4, ~ $ prevmath &lt;dbl&gt; 3, 4, 1, 0, 1, 1, 2, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 3, 1, 0~ $ mathquiz &lt;dbl&gt; 43, 49, 26, 29, 31, 20, 13, 23, 38, NA, 29, 32, 18, NA, 21, N~ $ statquiz &lt;dbl&gt; 6, 9, 8, 7, 6, 7, 3, 7, 8, 7, 8, 8, 1, 5, 8, 3, 8, 7, 10, 7, ~ $ exp_sqz &lt;dbl&gt; 7, 11, 8, 8, 6, 6, 4, 7, 7, 6, 10, 7, 3, 4, 6, 1, 7, 4, 9, 7,~ $ hr_base &lt;dbl&gt; 71, 73, 69, 72, 71, 70, 71, 77, 73, 78, 74, 73, 73, 72, 72, 7~ $ hr_pre &lt;dbl&gt; 68, 75, 76, 73, 83, 71, 70, 87, 72, 76, 72, 74, 76, 83, 74, 7~ $ hr_post &lt;dbl&gt; 65, 68, 72, 78, 74, 76, 66, 84, 67, 74, 73, 74, 78, 77, 68, 7~ $ anx_base &lt;dbl&gt; 17, 17, 19, 19, 26, 12, 12, 17, 20, 20, 21, 32, 19, 18, 21, 1~ $ anx_pre &lt;dbl&gt; 22, 19, 14, 13, 30, 15, 16, 19, 14, 24, 25, 35, 23, 27, 27, 1~ $ anx_post &lt;dbl&gt; 20, 16, 15, 16, 25, 19, 17, 22, 17, 19, 22, 33, 20, 28, 22, 2~ 5.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 5.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% data.frame() %&gt;% stargazer::stargazer(type = &quot;text&quot;) ============================================================ Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max ------------------------------------------------------------ phobia 100 3.310 2.444 0 1 4 10 mathquiz 85 29.071 9.480 9.000 22.000 35.000 49.000 statquiz 100 6.860 1.700 1 6 8 10 ------------------------------------------------------------ 5.2.2 Bivariate Relationships The furniture packages table1() function is a clean way to create a descriptive table that compares distinct subgroups of your sample (Barrett, Brignone, and Laxman 2021). Although categorizing continuous variables results in a loss of information (possible signal or noise), it is often done to investigate relationships in an exploratory way. data_ihno %&gt;% dplyr::mutate(phobia_cut3 = cut(phobia, breaks = c(0, 2, 4, 10), include.lowest = TRUE)) %&gt;% furniture::table1(mathquiz, statquiz, splitby = ~ phobia_cut3, na.rm = FALSE, test = TRUE, output = &quot;html&quot;) [0,2] (2,4] (4,10] P-Value n = 39 n = 37 n = 24 mathquiz 0.014 32.6 (8.5) 26.5 (9.8) 26.8 (8.9) statquiz 0.001 7.6 (1.3) 6.6 (1.6) 6.1 (2.0) One of the quickest ways to get a feel for all the pairwise relationships in your dataset (provided there arent too many variables) is with the pairs.panels() function in the psych package (Revelle 2021). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% data.frame() %&gt;% psych::pairs.panels(lm = TRUE, ci = TRUE, stars = TRUE) When two variables are both continuous, correlations (Pearsons \\(R\\)) are an important measure of association. Notice the discrepincy between the correlation between statquiz and phobia. Above, the psych::pairs.panels() function uses pairwise complete cases by default, so \\(r=-.39\\) is computed on all \\(n=100\\) subjects. Below, we specified use = \"complete.obs\" in the cor() fucntion, so all correlations will be based on the same \\(n=85\\) students, making it listwise complete. The choice of which method to you will vary by situation. Often it is easier to digest a correlation matrix if it is visually presented, instead of just given as a table of many numbers. The corrplot package has a useful function called corrplot.mixed() for doing just that (Wei and Simko 2021). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% cor(use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot.mixed(lower = &quot;ellipse&quot;, upper = &quot;number&quot;, tl.col = &quot;black&quot;) 5.3 Regression Analysis 5.3.1 Subset the Sample All regression models can only be fit to complete observations regarding the variables included in the model (dependent and independent). Removing any case that is incomplete with respect to even one variables is called list-wise deletion. In this analysis, models including the mathquiz variable will be fit on only 85 students (sincle 15 students did not take the math quiz), where as models not including this variable will be fit to all 100 studnets. This complicates model comparisons, which require nested models be fit to the same data (exactly). For this reason, the dataset has been reduced to the subset of students that are complete regarding the three variables utilized throughout the set of five nested models. data_ihno_fitting &lt;- data_ihno %&gt;% dplyr::filter(complete.cases(mathquiz, statquiz, phobia)) tibble::glimpse(data_ihno_fitting) Rows: 85 Columns: 18 $ sub_num &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 17, 18, 19, 21, 22~ $ gender &lt;fct&gt; Female, Female, Female, Female, Female, Female, Female, Femal~ $ major &lt;fct&gt; Psychology, Psychology, Psychology, Psychology, Psychology, P~ $ reason &lt;fct&gt; Advisor recommendation, Personal interest, Program requiremen~ $ exp_cond &lt;fct&gt; Easy, Easy, Easy, Easy, Easy, Moderate, Moderate, Moderate, M~ $ coffee &lt;fct&gt; Regularly drinks coffee, Not a regular coffee drinker, Not a ~ $ num_cups &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 1, 3, 0, 3, 2, 2, 0, 2, 2, 2~ $ phobia &lt;dbl&gt; 1, 1, 4, 4, 10, 4, 4, 4, 4, 5, 4, 7, 3, 4, 5, 0, 4, 3, 4, 0, ~ $ prevmath &lt;dbl&gt; 3, 4, 1, 0, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 3, 0, 1, 1, 3, 3~ $ mathquiz &lt;dbl&gt; 43, 49, 26, 29, 31, 20, 13, 23, 38, 29, 32, 18, 21, 37, 37, 3~ $ statquiz &lt;dbl&gt; 6, 9, 8, 7, 6, 7, 3, 7, 8, 8, 8, 1, 8, 8, 7, 10, 7, 4, 8, 8, ~ $ exp_sqz &lt;dbl&gt; 7, 11, 8, 8, 6, 6, 4, 7, 7, 10, 7, 3, 6, 7, 4, 9, 6, 3, 7, 7,~ $ hr_base &lt;dbl&gt; 71, 73, 69, 72, 71, 70, 71, 77, 73, 74, 73, 73, 72, 68, 77, 7~ $ hr_pre &lt;dbl&gt; 68, 75, 76, 73, 83, 71, 70, 87, 72, 72, 74, 76, 74, 67, 78, 7~ $ hr_post &lt;dbl&gt; 65, 68, 72, 78, 74, 76, 66, 84, 67, 73, 74, 78, 68, 74, 73, 7~ $ anx_base &lt;dbl&gt; 17, 17, 19, 19, 26, 12, 12, 17, 20, 21, 32, 19, 21, 15, 39, 2~ $ anx_pre &lt;dbl&gt; 22, 19, 14, 13, 30, 15, 16, 19, 14, 25, 35, 23, 27, 19, 39, 1~ $ anx_post &lt;dbl&gt; 20, 16, 15, 16, 25, 19, 17, 22, 17, 22, 33, 20, 22, 18, 40, 1~ 5.3.2 Fit Nested Models The bottom-up approach consists of starting with an initial NULL model with only an intercept term and them building additional models that are nested. Two models are considered nested if one is conains a subset of the terms (predictors or IV) compared to the other. fit_ihno_lm_0 &lt;- lm(statquiz ~ 1, # null model: intercept only data = data_ihno_fitting) fit_ihno_lm_1 &lt;- lm(statquiz ~ mathquiz, # only main effect of mathquiz data = data_ihno_fitting) fit_ihno_lm_2 &lt;- lm(statquiz ~ phobia, # only mian effect of phobia data = data_ihno_fitting) fit_ihno_lm_3 &lt;- lm(statquiz ~ mathquiz + phobia, # both main effects data = data_ihno_fitting) fit_ihno_lm_4 &lt;- lm(statquiz ~ mathquiz*phobia, # additional interaction data = data_ihno_fitting) 5.3.3 Comparing Nested Models 5.3.3.1 Model Comparison Table In single level, multiple linear regression significance of predictors (independent variables, IV) is usually based on both the Wald tests of significance for each beta estimate (shown with stars here) and comparisons in the model fit via the \\(R^2\\) values. Again the texreg package comes in handy to display several models in the same tal e (Leifeld 2020). texreg::knitreg(list(fit_ihno_lm_0, fit_ihno_lm_1, fit_ihno_lm_2, fit_ihno_lm_3, fit_ihno_lm_4), custom.model.names = c(&quot;No Predictors&quot;, &quot;Only Math Quiz&quot;, &quot;Only Phobia&quot;, &quot;Both IVs&quot;, &quot;Add Interaction&quot;)) Statistical models   No Predictors Only Math Quiz Only Phobia Both IVs Add Interaction (Intercept) 6.85*** 4.14*** 7.65*** 5.02*** 5.60***   (0.19) (0.53) (0.29) (0.63) (0.91) mathquiz   0.09***   0.08*** 0.06*     (0.02)   (0.02) (0.03) phobia     -0.25*** -0.16* -0.34       (0.07) (0.07) (0.21) mathquiz:phobia         0.01           (0.01) R2 0.00 0.26 0.13 0.31 0.31 Adj. R2 0.00 0.25 0.12 0.29 0.29 Num. obs. 85 85 85 85 85 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 5.3.3.2 Likelihood Ratio Test of Nested Models An alternative method for determing model fit and variable importance is the likelihood ratio test. This involves comparing the \\(-2LL\\) or inverse of twice the log of the likelihood value for the model. The difference in these values follows a Chi Squared distribution with degrees of freedom equal to the difference in the number of parameters estimated (number of betas). Test the main effect of math quiz: anova(fit_ihno_lm_0, fit_ihno_lm_1) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 84 253. NA NA NA NA 2 83 188. 1 65.3 28.8 0.000000700 Test the main effect of math phobia anova(fit_ihno_lm_0, fit_ihno_lm_2) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 84 253. NA NA NA NA 2 83 221. 1 32.3 12.1 0.000791 Test the main effect of math phobia, after controlling for math test anova(fit_ihno_lm_1, fit_ihno_lm_3) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 83 188. NA NA NA NA 2 82 175. 1 12.6 5.88 0.0175 Test the interaction between math test and math phobia (i.e. moderation) anova(fit_ihno_lm_3, fit_ihno_lm_4) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 82 175. NA NA NA NA 2 81 173. 1 1.69 0.789 0.377 5.3.4 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_ihno_lm_3, which = 1) plot(fit_ihno_lm_3, which = 2) The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2021). car::residualPlots(fit_ihno_lm_3) Test stat Pr(&gt;|Test stat|) mathquiz -1.7778 0.07918 . phobia 0.5004 0.61813 Tukey test -1.5749 0.11527 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 While the model tables give starts to denote significance, you may print the actual p-values with the summary() function applied to the model name. summary(fit_ihno_lm_3) Call: lm(formula = statquiz ~ mathquiz + phobia, data = data_ihno_fitting) Residuals: Min 1Q Median 3Q Max -4.3436 -0.8527 0.2805 0.9857 2.7370 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.01860 0.62791 7.993 7.23e-12 *** mathquiz 0.08097 0.01754 4.617 1.42e-05 *** phobia -0.16176 0.06670 -2.425 0.0175 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.462 on 82 degrees of freedom Multiple R-squared: 0.3076, Adjusted R-squared: 0.2907 F-statistic: 18.21 on 2 and 82 DF, p-value: 2.849e-07 summary(fit_ihno_lm_4) Call: lm(formula = statquiz ~ mathquiz * phobia, data = data_ihno_fitting) Residuals: Min 1Q Median 3Q Max -4.1634 -0.8433 0.2832 0.9685 2.9434 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.600183 0.907824 6.169 2.57e-08 *** mathquiz 0.061216 0.028334 2.161 0.0337 * phobia -0.339426 0.210907 -1.609 0.1114 mathquiz:phobia 0.006485 0.007303 0.888 0.3771 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.464 on 81 degrees of freedom Multiple R-squared: 0.3143, Adjusted R-squared: 0.2889 F-statistic: 12.37 on 3 and 81 DF, p-value: 9.637e-07 5.4 Conclusion 5.4.1 Tabulate the Final Model Summary Many journals prefer that regression tables include 95% confidence intervals, rater than standard errors for the beta estimates. texreg::knitreg(fit_ihno_lm_3, custom.model.names = &quot;Main Effects Model&quot;, ci.force = TRUE, # request 95% conf interv caption = &quot;Final Model for Stat&#39;s Quiz&quot;, single.row = TRUE) Final Model for Stats Quiz   Main Effects Model (Intercept) 5.02 [ 3.79; 6.25]* mathquiz 0.08 [ 0.05; 0.12]* phobia -0.16 [-0.29; -0.03]* R2 0.31 Adj. R2 0.29 Num. obs. 85 * 0 outside the confidence interval. 5.4.2 Plot the Model When a model only contains main effects, a plot is not important for interpretation, but can help understand the relationship between multiple predictors. interactions::interact_plot(model = fit_ihno_lm_3, pred = mathquiz, modx = phobia) Interval = 95% Confidence Interval interactions::interact_plot(model = fit_ihno_lm_3, pred = mathquiz, modx = phobia, modx.values = c(0, 5, 10), interval = TRUE) Interval = plus-or-minus one standard error for the mean (SEM) interactions::interact_plot(model = fit_ihno_lm_3, pred = mathquiz, modx = phobia, modx.values = c(0, 5, 10), interval = TRUE, int.width = .68) The Effect() function from the effects package chooses 5 or 6 nice values for each of your continuous independent variable (\\(X&#39;s\\)) based on the range of values found in the dataset on which the model and plugs all possible combinations of them into the regression equation \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 \\dots \\beta_k X_k\\) to compute the predicted mean value of the outcome (\\(Y\\)) (Fox et al. 2020). When plotting a regression model the outcome (dependent variable) is always on the y-axis (fit) and only one predictor (independent variable) may be used on the x-axis. You may incorporate additional predictor using colors, shapes, linetypes, or facets. For these predictors, you will want to specify only 2-4 values for illustration and then declare them as factors prior to plotting. effects::Effect(focal.predictors = c(&quot;mathquiz&quot;, &quot;phobia&quot;), mod = fit_ihno_lm_3, xlevels = list(phobia = c(0, 5, 10))) %&gt;% # values for illustration data.frame() %&gt;% dplyr::mutate(phobia = factor(phobia)) %&gt;% # factor for illustration ggplot() + aes(x = mathquiz, y = fit, fill = phobia) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se), alpha = .3) + geom_line(aes(color = phobia)) + theme_bw() + labs(x = &quot;Score on Math Quiz&quot;, y = &quot;Estimated Marginal Mean\\nScore on Stat Quiz&quot;, fill = &quot;Self Rated\\nMath Phobia&quot;, color = &quot;Self Rated\\nMath Phobia&quot;) + theme(legend.background = element_rect(color = &quot;black&quot;), legend.position = c(0, 1), legend.key.width = unit(1.5, &quot;cm&quot;), legend.justification = c(-0.1, 1.1)) 5.5 Write-up There is evidence both mathquiz and phobia are associated with statquiz and that the relationship is addative (i.e. no interaction). There is a strong association between math and stats quiz scores, \\(r = .51\\). Math phobia is associated with lower math, \\(r = -.28\\), and stats quiz scores, \\(r = -.36\\). When considered togehter, the combined effects of math phobia and math score account for 31% of the variance in statistical achievement. Not surprizingly, while higher self-reported math phobia was associated with lower statists scores, \\(b = -0.162\\), \\(p=.018\\), \\(95CI = [-0.29, -0.03]\\), higher math quiz scores were associated with higher stats score, \\(b = -0.081\\), \\(p&lt;.001\\), \\(95CI = [0.05, 0.12]\\). There was no evidence that math phobia moderated the relationship between math and quiz performance, \\(p=.377\\). 5.6 New Playground 5.6.1 Variable Inflation Factors (VIF) car::vif(fit_ihno_lm_3) mathquiz phobia 1.086652 1.086652 fit_ihno_lm_5 &lt;- lm(statquiz ~ mathquiz*phobia*reason, # additional interaction data = data_ihno_fitting) texreg::screenreg(fit_ihno_lm_5) ===================================================== Model 1 ----------------------------------------------------- (Intercept) 5.87 * (2.26) mathquiz 0.05 (0.08) phobia -0.46 (0.54) reasonPersonal interest -0.04 (2.61) reasonAdvisor recommendation 0.23 (2.85) mathquiz:phobia 0.01 (0.02) mathquiz:reasonPersonal interest 0.00 (0.09) mathquiz:reasonAdvisor recommendation -0.01 (0.10) phobia:reasonPersonal interest -0.58 (0.72) phobia:reasonAdvisor recommendation 0.32 (0.64) mathquiz:phobia:reasonPersonal interest 0.02 (0.03) mathquiz:phobia:reasonAdvisor recommendation -0.02 (0.02) ----------------------------------------------------- R^2 0.39 Adj. R^2 0.30 Num. obs. 85 ===================================================== *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 "],["interactions-example.html", "6 Interactions Example 6.1 Motivation 6.2 The Weight Loss Study 6.3 Exploratory Data Analysis 6.4 Simple Linear Regression 6.5 Continuous by Continuous 6.6 Continuous by Categorical 6.7 Categorical by Categorical 6.8 Three-way Interaction", " 6 Interactions Example library(tidyverse) library(emmeans) library(furniture) library(stargazer) library(psych) library(texreg) library(interactions) Taken from: https://stats.idre.ucla.edu/r/seminars/interactions-r/ 6.1 Motivation Suppose you are doing a simple study on weight loss and notice that people who spend more time exercising lose more weight. Upon further analysis you notice that those who spend the same amount of time exercising lose more weight if they are more effortful. The more effort people put into their workouts, the less time they need to spend exercising. This is popular in workouts like high intensity interval training (HIIT). You know that hours spent exercising improves weight loss, but how does it interact with effort? Here are three questions you can ask based on hypothetical scenarios. Im just starting out and dont want to put in too much effort. How many hours per week of exercise do I need to put in to lose 5 pounds? Im moderately fit and can put in an average level of effort into my workout. For every one hour increase per week in exercise, how much additional weight loss do I expect? Im a crossfit athlete and can perform with the utmost intensity. How much more weight loss would I expect for every one hour increase in exercise compared to the average amount of effort most people put in? Additionally, we can visualize the interaction to help us understand these relationships. 6.2 The Weight Loss Study This is a hypothetical study of weight loss for 900 participants in a year-long study of 3 different exercise programs, a jogging program, a swimming program, and a reading program which serves as a control activity. Variables loss: weight loss (continuous), positive = weight loss, negative scores = weight gain hours: hours spent exercising (continuous) effort: effort during exercise (continuous) 0 = minimal physical effort and 50 = maximum effort prog: exercise program (categorical) jogging=1 swimming=2 reading=3 gender: participant gender (binary) male=1 female=2 Definitions What exactly do I mean by decomposing, probing, and plotting an interaction? decompose: to break down the interaction into its lower order components (i.e., predicted means or simple slopes) probe: to use hypothesis testing to assess the statistical significance of simple slopes and simple slope differences (i.e., interactions) plot: to visually display the interaction in the form of simple slopes such as values of the dependent variable are on the y-axis, values of the predictor is on the x-axis, and the moderator separates the lines or bar graphs Lets define the essential elements of the interaction in a regression: DV: dependent variable (Y), the outcome of your study (e.g., weight loss) IV: independent variable (X), the predictor of your outcome (e.g., time exercising) MV: moderating variable (W) or moderator, a predictor that changes the relationship of the IV on the DV (e.g, effort) coefficient: estimate of the direction and magnitude of the relationship between an IV and DV continuous variable: a variable that can be measured on a continuous scale, e.g., weight, height categorical or binary variable: a variable that takes on discrete values, binary variables take on exactly two values, categorical variables can take on 3 or more values (e.g., gender, ethnicity) main effects or slopes: effects or slopes for models that do not involve interaction terms simple slope: when a continuous IV interacts with an MV, its slope at a particular level of an MV simple effect: when a categorical IV interacts with an MV, its effect at a particular level of an MV 6.2.1 Import Data data_raw &lt;- read.csv(&quot;https://stats.idre.ucla.edu/wp-content/uploads/2019/03/exercise.csv&quot;) tibble::glimpse(data_raw) Rows: 900 Columns: 6 $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, ~ $ loss &lt;dbl&gt; 18.022263, 10.186416, 19.747276, 1.883600, 14.242589, 19.694731~ $ hours &lt;dbl&gt; 1.836704, 2.389360, 2.362117, 2.520866, 1.889828, 2.367162, 1.9~ $ effort &lt;dbl&gt; 37.71218, 26.72401, 36.31657, 20.70048, 24.72712, 33.66948, 31.~ $ gender &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ $ prog &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ data_raw %&gt;% summary() id loss hours effort Min. : 1.0 Min. :-17.138 Min. :0.1751 Min. :12.95 1st Qu.:225.8 1st Qu.: -1.743 1st Qu.:1.6764 1st Qu.:26.26 Median :450.5 Median : 7.883 Median :2.0051 Median :29.63 Mean :450.5 Mean : 10.021 Mean :2.0024 Mean :29.66 3rd Qu.:675.2 3rd Qu.: 20.049 3rd Qu.:2.3375 3rd Qu.:33.10 Max. :900.0 Max. : 54.150 Max. :4.0722 Max. :44.08 gender prog Min. :1.0 Min. :1 1st Qu.:1.0 1st Qu.:1 Median :1.5 Median :2 Mean :1.5 Mean :2 3rd Qu.:2.0 3rd Qu.:3 Max. :2.0 Max. :3 6.2.2 Wrangle the data data_clean &lt;- data_raw %&gt;% dplyr::mutate(id = factor(id)) %&gt;% dplyr::mutate(gender = factor(gender) %&gt;% forcats::fct_recode(&quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;)) %&gt;% dplyr::mutate(prog = factor(prog) %&gt;% forcats::fct_recode(&quot;Jog&quot; = &quot;1&quot;, &quot;Swim&quot; = &quot;2&quot;, &quot;Read&quot; = &quot;3&quot;)) tibble::glimpse(data_clean) Rows: 900 Columns: 6 $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, ~ $ loss &lt;dbl&gt; 18.022263, 10.186416, 19.747276, 1.883600, 14.242589, 19.694731~ $ hours &lt;dbl&gt; 1.836704, 2.389360, 2.362117, 2.520866, 1.889828, 2.367162, 1.9~ $ effort &lt;dbl&gt; 37.71218, 26.72401, 36.31657, 20.70048, 24.72712, 33.66948, 31.~ $ gender &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Male, Mal~ $ prog &lt;fct&gt; Jog, Jog, Jog, Jog, Jog, Jog, Jog, Jog, Jog, Jog, Jog, Jog, Jog~ summary(data_clean) id loss hours effort gender 1 : 1 Min. :-17.138 Min. :0.1751 Min. :12.95 Male :450 2 : 1 1st Qu.: -1.743 1st Qu.:1.6764 1st Qu.:26.26 Female:450 3 : 1 Median : 7.883 Median :2.0051 Median :29.63 4 : 1 Mean : 10.021 Mean :2.0024 Mean :29.66 5 : 1 3rd Qu.: 20.049 3rd Qu.:2.3375 3rd Qu.:33.10 6 : 1 Max. : 54.150 Max. :4.0722 Max. :44.08 (Other):894 prog Jog :300 Swim:300 Read:300 6.3 Exploratory Data Analysis 6.3.1 Summary Statistics data_clean %&gt;% psych::describe() # A tibble: 6 x 13 vars n mean sd median trimmed mad min max range &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 900 450. 260. 450. 450. 334. 1 900 899 2 2 900 10.0 14.1 7.88 9.06 15.4 -17.1 54.2 71.3 3 3 900 2.00 0.495 2.01 2.00 0.492 0.175 4.07 3.90 4 4 900 29.7 5.14 29.6 29.6 5.08 12.9 44.1 31.1 5 5 900 1.5 0.500 1.5 1.5 0.741 1 2 1 6 6 900 2 0.817 2 2 1.48 1 3 2 # ... with 3 more variables: skew &lt;dbl&gt;, kurtosis &lt;dbl&gt;, se &lt;dbl&gt; data_clean %&gt;% stargazer::stargazer(type = &quot;text&quot;) ============================================================== Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max -------------------------------------------------------------- loss 900 10.021 14.100 -17.138 -1.743 20.049 54.150 hours 900 2.002 0.495 0.175 1.676 2.337 4.072 effort 900 29.659 5.143 12.949 26.258 33.095 44.076 -------------------------------------------------------------- data_clean %&gt;% furniture::table1(gender, loss, hours, effort) ----------------------------- Mean/Count (SD/%) n = 900 gender Male 450 (50%) Female 450 (50%) loss 10.0 (14.1) hours 2.0 (0.5) effort 29.7 (5.1) ----------------------------- data_clean %&gt;% dplyr::group_by(prog) %&gt;% furniture::table1(&quot;Gender&quot; = gender, &quot;Weight Loss, pounds&quot; = loss, &quot;Time Spent in Program, hours&quot; = hours, &quot;Effort in Program&quot; = effort, digits = 2, output = &quot;markdown&quot;, total = TRUE, test = TRUE) Total Jog Swim Read P-Value n = 900 n = 300 n = 300 n = 300 Gender 1 Male 450 (50%) 150 (50%) 150 (50%) 150 (50%) Female 450 (50%) 150 (50%) 150 (50%) 150 (50%) Weight Loss, pounds &lt;.001 10.02 (14.10) 8.03 (7.45) 25.82 (8.92) -3.79 (4.11) Time Spent in Program, hours 0.469 2.00 (0.49) 1.99 (0.47) 1.99 (0.48) 2.03 (0.53) Effort in Program 0.18 29.66 (5.14) 30.05 (5.04) 29.65 (5.17) 29.28 (5.20) 6.3.2 Correlations data_clean %&gt;% dplyr::select_if(is.numeric) %&gt;% furniture::tableC() --------------------------------------------- [1] [2] [3] [1]loss 1.00 [2]hours 0.087 (0.009) 1.00 [3]effort 0.259 (&lt;.001) 0.016 (0.626) 1.00 --------------------------------------------- data_clean %&gt;% cor.test(~ loss + hours, data = .) Pearson&#39;s product-moment correlation data: loss and hours t = 2.6054, df = 898, p-value = 0.009329 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.02138902 0.15110872 sample estimates: cor 0.08661599 data_clean %&gt;% cor.test(~ loss + effort, data = .) Pearson&#39;s product-moment correlation data: loss and effort t = 8.0213, df = 898, p-value = 3.255e-15 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.1965432 0.3185360 sample estimates: cor 0.2585703 data_clean %&gt;% cor.test(~ hours + effort, data = .) Pearson&#39;s product-moment correlation data: hours and effort t = 0.48812, df = 898, p-value = 0.6256 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.04911364 0.08154791 sample estimates: cor 0.01628667 data_clean %&gt;% dplyr::select_if(is.numeric) %&gt;% cor() loss hours effort loss 1.00000000 0.08661599 0.25857026 hours 0.08661599 1.00000000 0.01628667 effort 0.25857026 0.01628667 1.00000000 data_clean %&gt;% dplyr::select_if(is.numeric) %&gt;% cor() %&gt;% corrplot::corrplot() data_clean %&gt;% dplyr::select_if(is.numeric) %&gt;% cor() %&gt;% corrplot::corrplot.mixed() 6.3.3 Visualization Bivariate Relationships data_clean %&gt;% ggplot(aes(x = hours, y = loss)) + geom_point() + theme_bw() data_clean %&gt;% ggplot(aes(x = hours, y = loss)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + theme_bw() data_clean %&gt;% ggplot(aes(x = hours, y = loss)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + theme_bw() + facet_grid(~ gender) data_clean %&gt;% ggplot(aes(x = hours, y = loss)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + theme_bw() + facet_grid(~ prog) data_clean %&gt;% ggplot(aes(x = hours, y = loss)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + theme_bw() + facet_grid(gender ~ prog) 6.4 Simple Linear Regression 6.4.1 Fit a MLR Model Question: Does time spent on a program effect weight loss? fit_lm_loss_hr &lt;- lm(loss ~ hours, # DV ~ IV&#39;s data = data_clean) summary(fit_lm_loss_hr) Call: lm(formula = loss ~ hours, data = data_clean) Residuals: Min 1Q Median 3Q Max -27.164 -11.388 -2.086 10.011 42.787 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.0757 1.9550 2.596 0.00958 ** hours 2.4696 0.9479 2.605 0.00933 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14.06 on 898 degrees of freedom Multiple R-squared: 0.007502, Adjusted R-squared: 0.006397 F-statistic: 6.788 on 1 and 898 DF, p-value: 0.009329 Answer: Yes. There is evidence that an additional hour in a program is associated with slightly more weight lost, b = 2.47, p = .009, R^2 &lt; .01. 6.4.2 Check Assumptions Base function to plot main residual diagnostics to asses assumptions See: * http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/ http://www.contrib.andrew.cmu.edu/~achoulde/94842/homework/regression_diagnostics.html https://daviddalpiaz.github.io/appliedstats/model-diagnostics.html par(mfrow = c(2, 2)) # sets up a 2x2 grid of plots in base R plot(fit_lm_loss_hr) par(mfrow = c(1, 1)) # make sure to return back to a single plot fit_lm_loss_hr %&gt;% car::residualPlot(type = &quot;rstandard&quot;) car::qqPlot(residuals(fit_lm_loss_hr)) [1] 536 584 6.4.3 Predict and Pariwise Tests fit_lm_loss_hr %&gt;% emmeans::emmeans(~ hours) # default for mean hours hours emmean SE df lower.CL upper.CL 2 10 0.469 898 9.1 10.9 Confidence level used: 0.95 Interpretation: A participant that spends 2 hours on their program looses 10 pounds on average, SE = 0.47, 95% CI [9.1, 10.9]. fit_lm_loss_hr %&gt;% emmeans::emmeans(~ hours, at = list(hours = 1:4)) # set hours to plug in hours emmean SE df lower.CL upper.CL 1 7.55 1.059 898 5.47 9.62 2 10.01 0.469 898 9.10 10.93 3 12.48 1.055 898 10.41 14.56 4 14.95 1.951 898 11.13 18.78 Confidence level used: 0.95 Interpretation: Overall, on average the effect of time is an additional 2.47 pounds lost per hour spent on a program. 6.4.4 Plot Estimated Marginal Means create a sequence of numbers seq(from = .25, to = 4, by = .25) [1] 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 3.25 3.50 3.75 [16] 4.00 fit_lm_loss_hr %&gt;% emmeans::emmeans(~ hours, at = list(hours = seq(from = .25, to = 4, by = .25))) %&gt;% data.frame() # A tibble: 16 x 6 hours emmean SE df lower.CL upper.CL &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.25 5.69 1.73 898 2.31 9.08 2 0.5 6.31 1.50 898 3.37 9.25 3 0.75 6.93 1.28 898 4.42 9.43 4 1 7.55 1.06 898 5.47 9.62 5 1.25 8.16 0.853 898 6.49 9.84 6 1.5 8.78 0.668 898 7.47 10.1 7 1.75 9.40 0.526 898 8.37 10.4 8 2 10.0 0.469 898 9.10 10.9 9 2.25 10.6 0.524 898 9.60 11.7 10 2.5 11.2 0.665 898 9.94 12.6 11 2.75 11.9 0.850 898 10.2 13.5 12 3 12.5 1.06 898 10.4 14.6 13 3.25 13.1 1.27 898 10.6 15.6 14 3.5 13.7 1.49 898 10.8 16.7 15 3.75 14.3 1.72 898 11.0 17.7 16 4 15.0 1.95 898 11.1 18.8 fit_lm_loss_hr %&gt;% emmeans::emmeans(~ hours, at = list(hours = seq(from = .25, to = 4, by = .25))) %&gt;% data.frame() %&gt;% ggplot(aes(x = hours, y = emmean)) + geom_ribbon(aes(ymin = lower.CL, ymax = upper.CL), alpha = .25) + geom_line() + theme_bw() 6.5 Continuous by Continuous 6.5.1 Fit MLR Model The same model: * loss ~ hours*effort * loss ~ hours + effore + hours:effort Question: Is there an interaction between time spent and effort in a weight loss program? fit_lm_loss_hr_eff &lt;- lm(loss ~ hours*effort, data = data_clean) summary(fit_lm_loss_hr_eff) Call: lm(formula = loss ~ hours * effort, data = data_clean) Residuals: Min 1Q Median 3Q Max -29.52 -10.60 -1.78 11.13 34.51 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.79864 11.60362 0.672 0.5017 hours -9.37568 5.66392 -1.655 0.0982 . effort -0.08028 0.38465 -0.209 0.8347 hours:effort 0.39335 0.18750 2.098 0.0362 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 13.56 on 896 degrees of freedom Multiple R-squared: 0.07818, Adjusted R-squared: 0.07509 F-statistic: 25.33 on 3 and 896 DF, p-value: 9.826e-16 Answer: Effect does moderate time spent in a program for weight loss, b = 0.39, p = .036. The combined effect of time and effort account for nearly 8% of the variation in pounds lost. 6.5.2 Table of Parameter Estimates Sometimes its nice to have a table of the parameter estimates of competing models. texreg::screenreg(list(fit_lm_loss_hr, fit_lm_loss_hr_eff), single.row = TRUE) ================================================ Model 1 Model 2 ------------------------------------------------ (Intercept) 5.08 (1.96) ** 7.80 (11.60) hours 2.47 (0.95) ** -9.38 (5.66) effort -0.08 (0.38) hours:effort 0.39 (0.19) * ------------------------------------------------ R^2 0.01 0.08 Adj. R^2 0.01 0.08 Num. obs. 900 900 ================================================ *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 6.5.3 Check Assumptions par(mfrow = c(2, 2)) plot(fit_lm_loss_hr_eff) par(mfrow = c(1, 1)) 6.5.4 Plot Estimated Marginal Means The emmeans package can make interaction plots with the emmip() function, but it only can create confidene intervals, not confidence bands. fit_lm_loss_hr_eff %&gt;% emmeans::emmip(effort ~ hours, at = list(hours = seq(from = .25, to = 4, by = .25), effort = c(20, 30, 40)), CIs = TRUE) fit_lm_loss_hr_eff %&gt;% interactions::interact_plot(pred = hours, modx = effort, interval = TRUE, int.type = &quot;confidence&quot;) Since these packages are built on ggplot2 you can customise them further to make them better for publication. fit_lm_loss_hr_eff %&gt;% interactions::interact_plot(pred = hours, modx = effort, interval = TRUE, int.type = &quot;confidence&quot;, legend.main = &quot;Effort:&quot;) + labs(x = &quot;Hours Spent Exercising&quot;, y = &quot;Weight Loss, pounds&quot;) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) fit_lm_loss_hr_eff %&gt;% interactions::interact_plot(pred = hours, modx = effort, modx.values = c(15, 24, 35, 45), interval = TRUE, int.type = &quot;confidence&quot;, legend.main = &quot;Effort:&quot;) + labs(x = &quot;Hours Spent Exercising&quot;, y = &quot;Weight Loss, pounds&quot;) + geom_hline(yintercept = 0) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) Interpretation: When little time is spent on the program, participants lost just over 5 pounds, irrespective of effort. More time spent on the programs only translated to additional weight lost if their effort was high. 6.5.5 Predict and Pairwise Tests By default, we get a prediction for the mean hours (2) and the mean effort (29.7). fit_lm_loss_hr_eff %&gt;% emmeans::emmeans(~ hours*effort) hours effort emmean SE df lower.CL upper.CL 2 29.7 10 0.452 896 9.12 10.9 Confidence level used: 0.95 We can customize which hours and effort for which we want to get predictions for. fit_lm_loss_hr_eff %&gt;% emmeans::emmeans(~ hours*effort, at = list(hours = 1:4, effort = c(20, 30, 40))) hours effort emmean SE df lower.CL upper.CL 1 20 4.684 2.253 896 0.262 9.11 2 20 3.176 0.962 896 1.287 5.06 3 20 1.667 2.284 896 -2.815 6.15 4 20 0.158 4.236 896 -8.156 8.47 1 30 7.815 1.024 896 5.805 9.82 2 30 10.240 0.453 896 9.351 11.13 3 30 12.665 1.019 896 10.665 14.66 4 30 15.089 1.883 896 11.394 18.78 1 40 10.946 2.357 896 6.319 15.57 2 40 17.304 1.016 896 15.311 19.30 3 40 23.662 2.341 896 19.068 28.26 4 40 30.020 4.348 896 21.488 38.55 Confidence level used: 0.95 By adding the word pairwise we also get pairwise t-tests! Tukeys HSD adjustment for multiple compairisons is done by default. fit_lm_loss_hr_eff %&gt;% emmeans::emmeans(pairwise ~ hours*effort, at = list(hours = 2:3, effort = c(25, 35))) $emmeans hours effort emmean SE df lower.CL upper.CL 2 25 6.71 0.610 896 5.51 7.91 3 25 7.17 1.431 896 4.36 9.98 2 35 13.77 0.652 896 12.49 15.05 3 35 18.16 1.477 896 15.26 21.06 Confidence level used: 0.95 $contrasts contrast estimate SE df t.ratio p.value 2 25 - 3 25 -0.458 1.28 896 -0.357 0.9845 2 25 - 2 35 -7.064 0.88 896 -8.031 &lt;.0001 2 25 - 3 35 -11.456 1.59 896 -7.186 &lt;.0001 3 25 - 2 35 -6.606 1.57 896 -4.212 0.0002 3 25 - 3 35 -10.998 2.08 896 -5.297 &lt;.0001 2 35 - 3 35 -4.391 1.34 896 -3.288 0.0058 P value adjustment: tukey method for comparing a family of 4 estimates By changing the astrics (*) to a vertical bar (|) we get compairisons between different hours WITHIN each effort level. fit_lm_loss_hr_eff %&gt;% emmeans::emmeans(pairwise ~ hours|effort, at = list(hours = c(1, 4), effort = c(20, 30, 40))) $emmeans effort = 20: hours emmean SE df lower.CL upper.CL 1 4.684 2.25 896 0.262 9.11 4 0.158 4.24 896 -8.156 8.47 effort = 30: hours emmean SE df lower.CL upper.CL 1 7.815 1.02 896 5.805 9.82 4 15.089 1.88 896 11.394 18.78 effort = 40: hours emmean SE df lower.CL upper.CL 1 10.946 2.36 896 6.319 15.57 4 30.020 4.35 896 21.488 38.55 Confidence level used: 0.95 $contrasts effort = 20: contrast estimate SE df t.ratio p.value 1 - 4 4.53 6.16 896 0.734 0.4629 effort = 30: contrast estimate SE df t.ratio p.value 1 - 4 -7.27 2.75 896 -2.649 0.0082 effort = 40: contrast estimate SE df t.ratio p.value 1 - 4 -19.07 6.35 896 -3.002 0.0028 6.5.6 Simple Slopes Analysis By default, we only get the overall slope for time spent for the mean effort (29.7) and plus-or-minus one standard deviation (24.51 &amp; 34.80). fit_lm_loss_hr_eff %&gt;% interactions::sim_slopes(pred = hours, modx = effort) JOHNSON-NEYMAN INTERVAL When effort is OUTSIDE the interval [-64.73, 28.55], the slope of hours is p &lt; .05. Note: The range of observed values of effort is [12.95, 44.08] SIMPLE SLOPES ANALYSIS Slope of hours when effort = 24.51646 (- 1 SD): Est. S.E. t val. p ------ ------ -------- ------ 0.27 1.35 0.20 0.84 Slope of hours when effort = 29.65922 (Mean): Est. S.E. t val. p ------ ------ -------- ------ 2.29 0.92 2.50 0.01 Slope of hours when effort = 34.80198 (+ 1 SD): Est. S.E. t val. p ------ ------ -------- ------ 4.31 1.31 3.30 0.00 You may specify other specific values. fit_lm_loss_hr_eff %&gt;% interactions::sim_slopes(pred = hours, modx = effort, modx.values = c(15, 25, 23, 45)) JOHNSON-NEYMAN INTERVAL When effort is OUTSIDE the interval [-64.73, 28.55], the slope of hours is p &lt; .05. Note: The range of observed values of effort is [12.95, 44.08] SIMPLE SLOPES ANALYSIS Slope of hours when effort = 15.00: Est. S.E. t val. p ------- ------ -------- ------ -3.48 2.92 -1.19 0.23 Slope of hours when effort = 25.00: Est. S.E. t val. p ------ ------ -------- ------ 0.46 1.28 0.36 0.72 Slope of hours when effort = 23.00: Est. S.E. t val. p ------- ------ -------- ------ -0.33 1.57 -0.21 0.83 Slope of hours when effort = 45.00: Est. S.E. t val. p ------ ------ -------- ------ 8.32 2.99 2.78 0.01 6.6 Continuous by Categorical 6.6.1 Fit MLR Model Question: Did weight loss depend on gender? fit_lm_loss_gen &lt;- lm(loss ~ gender, data = data_clean) summary(fit_lm_loss_gen) Call: lm(formula = loss ~ gender, data = data_clean) Residuals: Min 1Q Median 3Q Max -27.067 -11.788 -2.156 9.977 44.222 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 10.1129 0.6650 15.206 &lt;2e-16 *** genderFemale -0.1842 0.9405 -0.196 0.845 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14.11 on 898 degrees of freedom Multiple R-squared: 4.271e-05, Adjusted R-squared: -0.001071 F-statistic: 0.03835 on 1 and 898 DF, p-value: 0.8448 Answer:The main effect of gender is not significant. Question: Does gender moderate the effect of spending addition time on the program? (ignoring the role of effort and program type for the time being) fit_lm_loss_hrs_gen &lt;- lm(loss ~ hours*gender, data = data_clean) summary(fit_lm_loss_hrs_gen) Call: lm(formula = loss ~ hours * gender, data = data_clean) Residuals: Min 1Q Median 3Q Max -27.118 -11.350 -1.963 10.001 42.376 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.906 2.805 2.462 0.014 * hours 1.591 1.352 1.177 0.240 genderFemale -3.571 3.915 -0.912 0.362 hours:genderFemale 1.724 1.898 0.908 0.364 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14.06 on 896 degrees of freedom Multiple R-squared: 0.008433, Adjusted R-squared: 0.005113 F-statistic: 2.54 on 3 and 896 DF, p-value: 0.05523 Answer: No, gender does not interact with time spent. Question: Is the effect of time spend moderated by type of program? fit_lm_loss_prog &lt;- lm(loss ~ prog, data = data_clean) summary(fit_lm_loss_prog) Call: lm(formula = loss ~ prog, data = data_clean) Residuals: Min 1Q Median 3Q Max -22.4700 -4.6636 -0.0762 4.1810 28.3304 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 8.0304 0.4110 19.54 &lt;2e-16 *** progSwim 17.7897 0.5813 30.61 &lt;2e-16 *** progRead -11.8182 0.5813 -20.33 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 7.119 on 897 degrees of freedom Multiple R-squared: 0.7457, Adjusted R-squared: 0.7451 F-statistic: 1315 on 2 and 897 DF, p-value: &lt; 2.2e-16 fit_lm_loss_hrs_prog &lt;- lm(loss ~ hours*prog, data = data_clean) summary(fit_lm_loss_hrs_prog) Call: lm(formula = loss ~ hours * prog, data = data_clean) Residuals: Min 1Q Median 3Q Max -24.977 -4.146 -0.213 3.992 25.067 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -6.7807 1.6438 -4.125 4.06e-05 *** hours 7.4527 0.8053 9.255 &lt; 2e-16 *** progSwim 18.9296 2.2877 8.275 4.66e-16 *** progRead 8.9970 2.2160 4.060 5.34e-05 *** hours:progSwim -0.5787 1.1193 -0.517 0.605 hours:progRead -10.4089 1.0723 -9.708 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 6.502 on 894 degrees of freedom Multiple R-squared: 0.7885, Adjusted R-squared: 0.7874 F-statistic: 666.8 on 5 and 894 DF, p-value: &lt; 2.2e-16 anova(fit_lm_loss_hrs_prog) # A tibble: 4 x 5 Df `Sum Sq` `Mean Sq` `F value` `Pr(&gt;F)` &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 1341. 1341. 31.7 2.39e- 8 2 2 134281. 67140. 1588. 5.60e-295 3 2 5319. 2660. 62.9 2.75e- 26 4 894 37795. 42.3 NA NA Answer: Yes! The type of program does moderate the effect of time spent on weight loss, F(2, 894) = 62.91, p &lt; .001. 6.6.2 Table Comparing Models Sometimes its nice to have a table of the parameter estimates of competing models. texreg::screenreg(list(fit_lm_loss_hr, fit_lm_loss_gen, fit_lm_loss_prog, fit_lm_loss_hrs_gen, fit_lm_loss_hrs_prog), custom.model.names = c(&quot;Hrs&quot;, &quot;Gender&quot;, &quot;Program&quot;, &quot;Hrs + Gender&quot;, &quot;Hrs + Program&quot;)) ================================================================================== Hrs Gender Program Hrs + Gender Hrs + Program ---------------------------------------------------------------------------------- (Intercept) 5.08 ** 10.11 *** 8.03 *** 6.91 * -6.78 *** (1.96) (0.67) (0.41) (2.81) (1.64) hours 2.47 ** 1.59 7.45 *** (0.95) (1.35) (0.81) genderFemale -0.18 -3.57 (0.94) (3.91) progSwim 17.79 *** 18.93 *** (0.58) (2.29) progRead -11.82 *** 9.00 *** (0.58) (2.22) hours:genderFemale 1.72 (1.90) hours:progSwim -0.58 (1.12) hours:progRead -10.41 *** (1.07) ---------------------------------------------------------------------------------- R^2 0.01 0.00 0.75 0.01 0.79 Adj. R^2 0.01 -0.00 0.75 0.01 0.79 Num. obs. 900 900 900 900 900 ================================================================================== *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 6.6.3 Check Assumptions par(mfrow = c(2, 2)) plot(fit_lm_loss_hrs_prog) par(mfrow = c(1, 1)) 6.6.4 Plot Estimated Marginal Means fit_lm_loss_hrs_prog %&gt;% interactions::interact_plot(pred = hours, modx = prog, interval = TRUE, int.type = &quot;confidence&quot;, legend.main = &quot;Program:&quot;) + theme_bw() + theme(legend.background = element_rect(color = &quot;black&quot;), legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.key.width = unit(2, &quot;cm&quot;)) + labs(x = &quot;Time Spend on Program, hours&quot;, y = &quot;Weight Loss, pounds&quot;, caption = &quot;Note. Bands represent 95% confidence intervals&quot;) + geom_hline(yintercept = 0) 6.6.5 Predict and Pairwise Tests fit_lm_loss_hrs_prog %&gt;% emmeans::emmeans(~hours) hours emmean SE df lower.CL upper.CL 2 10.1 0.217 894 9.69 10.5 Results are averaged over the levels of: prog Confidence level used: 0.95 fit_lm_loss_hrs_prog %&gt;% emmeans::emmeans(~ prog) prog emmean SE df lower.CL upper.CL Jog 8.14 0.376 894 7.41 8.88 Swim 25.91 0.376 894 25.18 26.65 Read -3.70 0.376 894 -4.44 -2.97 Confidence level used: 0.95 fit_lm_loss_hrs_prog %&gt;% emmeans::emmeans(~ hours|prog) prog = Jog: hours emmean SE df lower.CL upper.CL 2 8.14 0.376 894 7.41 8.88 prog = Swim: hours emmean SE df lower.CL upper.CL 2 25.91 0.376 894 25.18 26.65 prog = Read: hours emmean SE df lower.CL upper.CL 2 -3.70 0.376 894 -4.44 -2.97 Confidence level used: 0.95 fit_lm_loss_hrs_prog %&gt;% emmeans::emmeans(~ hours|prog, at = list(hours = 1:4)) prog = Jog: hours emmean SE df lower.CL upper.CL 1 0.672 0.879 894 -1.05 2.398 2 8.125 0.376 894 7.39 8.862 3 15.578 0.898 894 13.82 17.340 4 23.030 1.664 894 19.77 26.296 prog = Swim: hours emmean SE df lower.CL upper.CL 1 19.023 0.855 894 17.34 20.702 2 25.897 0.375 894 25.16 26.634 3 32.771 0.871 894 31.06 34.481 4 39.645 1.608 894 36.49 42.801 prog = Read: hours emmean SE df lower.CL upper.CL 1 -0.740 0.821 894 -2.35 0.871 2 -3.696 0.376 894 -4.43 -2.958 3 -6.652 0.782 894 -8.19 -5.117 4 -9.608 1.444 894 -12.44 -6.775 Confidence level used: 0.95 6.6.6 Simple Slopes Analysis fit_lm_loss_hrs_prog %&gt;% interactions::sim_slopes(pred = hours, modx = prog) SIMPLE SLOPES ANALYSIS Slope of hours when prog = Read: Est. S.E. t val. p ------- ------ -------- ------ -2.96 0.71 -4.18 0.00 Slope of hours when prog = Swim: Est. S.E. t val. p ------ ------ -------- ------ 6.87 0.78 8.84 0.00 Slope of hours when prog = Jog: Est. S.E. t val. p ------ ------ -------- ------ 7.45 0.81 9.25 0.00 Interpretation: Participants in the jogging program (M = 7.45 lb/hr, SE = 0.81) and the swimming program (M = 6.87 lb/hr, SE = 0.78), loose about the same amount of weight for each additional hour spent exercising, p = .863, conversely, each additional hour reading is associated with more weight gained (M = -2.96 lb/hr, SE = 0.71). 6.7 Categorical by Categorical 6.7.1 Fit MLR Model Question: Do both genders have the same success in all the programs? fit_lm_loss_prog &lt;- lm(loss ~ prog, data = data_clean) fit_lm_loss_prog_gen &lt;- lm(loss ~ prog*gender, data = data_clean) summary(fit_lm_loss_prog_gen) Call: lm(formula = loss ~ prog * gender, data = data_clean) Residuals: Min 1Q Median 3Q Max -19.1723 -4.1894 -0.0994 3.7506 27.6939 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.7720 0.5322 22.118 &lt; 2e-16 *** progSwim 10.7504 0.7527 14.282 &lt; 2e-16 *** progRead -15.7276 0.7527 -20.895 &lt; 2e-16 *** genderFemale -7.4833 0.7527 -9.942 &lt; 2e-16 *** progSwim:genderFemale 14.0787 1.0645 13.226 &lt; 2e-16 *** progRead:genderFemale 7.8188 1.0645 7.345 4.63e-13 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 6.519 on 894 degrees of freedom Multiple R-squared: 0.7875, Adjusted R-squared: 0.7863 F-statistic: 662.5 on 5 and 894 DF, p-value: &lt; 2.2e-16 anova(fit_lm_loss_prog_gen) # A tibble: 4 x 5 Df `Sum Sq` `Mean Sq` `F value` `Pr(&gt;F)` &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2 133277. 66639. 1568. 4.49e-293 2 1 7.63 7.63 0.180 6.72e- 1 3 2 7463. 3732. 87.8 1.51e- 35 4 894 37988. 42.5 NA NA Answer: At least one of the programs results in more weight lost for one of the genders. 6.7.2 Table of Parameter Estimates texreg::screenreg(list(fit_lm_loss_gen, fit_lm_loss_prog, fit_lm_loss_prog_gen), custom.model.names = c(&quot;Gender&quot;, &quot;Program&quot;, &quot;Interacting&quot;)) ========================================================== Gender Program Interacting ---------------------------------------------------------- (Intercept) 10.11 *** 8.03 *** 11.77 *** (0.67) (0.41) (0.53) genderFemale -0.18 -7.48 *** (0.94) (0.75) progSwim 17.79 *** 10.75 *** (0.58) (0.75) progRead -11.82 *** -15.73 *** (0.58) (0.75) progSwim:genderFemale 14.08 *** (1.06) progRead:genderFemale 7.82 *** (1.06) ---------------------------------------------------------- R^2 0.00 0.75 0.79 Adj. R^2 -0.00 0.75 0.79 Num. obs. 900 900 900 ========================================================== *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 6.7.3 Check Assumptions par(mfrow = c(2, 2)) plot(fit_lm_loss_prog_gen) par(mfrow = c(1, 1)) 6.7.4 Plot Estimated Marginal Means fit_lm_loss_prog_gen %&gt;% interactions::cat_plot(pred = prog, modx = gender) fit_lm_loss_prog_gen %&gt;% interactions::cat_plot(pred = prog, modx = gender, geom = &quot;line&quot;, dodge.width = .2, errorbar.width = .25, point.shape = TRUE, # vary.lty = TRUE, colors = c(&quot;gray10&quot;, &quot;gray50&quot;), legend.main = &quot;Gender:&quot;) + theme_bw() + theme(legend.background = element_rect(color = &quot;black&quot;), legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.key.width = unit(2, &quot;cm&quot;)) + labs(x = &quot;Program&quot;, y = &quot;Weight Loss, pounds&quot;, caption = &quot;Note. Error Bars represent 95% confidence intervals&quot;) + geom_hline(yintercept = 0) + scale_shape_manual(&quot;Gender:&quot;, values = c(15, 16)) 6.7.5 Predict and Pairwise Tests Question: Which gender or program looses the most and least weight? fit_lm_loss_prog_gen %&gt;% emmeans::emmeans(pairwise ~ prog | gender) $emmeans gender = Male: prog emmean SE df lower.CL upper.CL Jog 11.77 0.532 894 10.73 12.82 Swim 22.52 0.532 894 21.48 23.57 Read -3.96 0.532 894 -5.00 -2.91 gender = Female: prog emmean SE df lower.CL upper.CL Jog 4.29 0.532 894 3.24 5.33 Swim 29.12 0.532 894 28.07 30.16 Read -3.62 0.532 894 -4.66 -2.58 Confidence level used: 0.95 $contrasts gender = Male: contrast estimate SE df t.ratio p.value Jog - Swim -10.75 0.753 894 -14.282 &lt;.0001 Jog - Read 15.73 0.753 894 20.895 &lt;.0001 Swim - Read 26.48 0.753 894 35.177 &lt;.0001 gender = Female: contrast estimate SE df t.ratio p.value Jog - Swim -24.83 0.753 894 -32.986 &lt;.0001 Jog - Read 7.91 0.753 894 10.507 &lt;.0001 Swim - Read 32.74 0.753 894 43.494 &lt;.0001 P value adjustment: tukey method for comparing a family of 3 estimates Answer: For each gender, the swimming program resultsed in the most weight lost, followed by the jogging program, where as the the reading program paticipants gained weight. fit_lm_loss_prog_gen %&gt;% emmeans::emmeans(pairwise ~ gender | prog) $emmeans prog = Jog: gender emmean SE df lower.CL upper.CL Male 11.77 0.532 894 10.73 12.82 Female 4.29 0.532 894 3.24 5.33 prog = Swim: gender emmean SE df lower.CL upper.CL Male 22.52 0.532 894 21.48 23.57 Female 29.12 0.532 894 28.07 30.16 prog = Read: gender emmean SE df lower.CL upper.CL Male -3.96 0.532 894 -5.00 -2.91 Female -3.62 0.532 894 -4.66 -2.58 Confidence level used: 0.95 $contrasts prog = Jog: contrast estimate SE df t.ratio p.value Male - Female 7.483 0.753 894 9.942 &lt;.0001 prog = Swim: contrast estimate SE df t.ratio p.value Male - Female -6.595 0.753 894 -8.762 &lt;.0001 prog = Read: contrast estimate SE df t.ratio p.value Male - Female -0.335 0.753 894 -0.446 0.6559 Answer: In the swimming program females lost more weight than males, but the reverse trend was observed in the jogging group. In the reading group there was no gender gap in weight lost. 6.7.6 Simple Slopes Analysis Not appropriate for categorical-only interactions 6.8 Three-way Interaction 6.8.1 Fit MLR Models fit_lm_loss_3way_a &lt;- lm(loss ~ prog*gender*effort, data = data_clean) summary(fit_lm_loss_3way_a) Call: lm(formula = loss ~ prog * gender * effort, data = data_clean) Residuals: Min 1Q Median 3Q Max -19.455 -3.266 -0.009 3.128 15.941 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -13.99416 2.44383 -5.726 1.40e-08 *** progSwim 2.14594 3.43352 0.625 0.53213 progRead 7.85129 3.41229 2.301 0.02163 * genderFemale -0.31768 3.53356 -0.090 0.92838 effort 0.85225 0.07968 10.696 &lt; 2e-16 *** progSwim:genderFemale 5.81629 4.89795 1.187 0.23535 progRead:genderFemale 1.99989 4.85732 0.412 0.68064 progSwim:effort 0.30249 0.11280 2.682 0.00746 ** progRead:effort -0.77742 0.11308 -6.875 1.17e-11 *** genderFemale:effort -0.22963 0.11601 -1.979 0.04807 * progSwim:genderFemale:effort 0.27596 0.16174 1.706 0.08832 . progRead:genderFemale:effort 0.18347 0.16131 1.137 0.25570 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 5.041 on 888 degrees of freedom Multiple R-squared: 0.8738, Adjusted R-squared: 0.8722 F-statistic: 558.8 on 11 and 888 DF, p-value: &lt; 2.2e-16 anova(fit_lm_loss_3way_a) # A tibble: 8 x 5 Df `Sum Sq` `Mean Sq` `F value` `Pr(&gt;F)` &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2 133277. 66639. 2623. 0 2 1 7.63 7.63 0.300 5.84e- 1 3 1 10220. 10220. 402. 4.37e-74 4 2 7361. 3681. 145. 3.57e-55 5 2 5200. 2600. 102. 1.03e-40 6 1 31.9 31.9 1.26 2.63e- 1 7 2 76.3 38.1 1.50 2.24e- 1 8 888 22563. 25.4 NA NA fit_lm_loss_3way_b &lt;- lm(loss ~ prog*gender*hours, data = data_clean) summary(fit_lm_loss_3way_b) Call: lm(formula = loss ~ prog * gender * hours, data = data_clean) Residuals: Min 1Q Median 3Q Max -22.5750 -3.6364 -0.2174 3.7454 22.6508 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.9697 2.0709 1.434 0.151920 progSwim 1.6405 2.8765 0.570 0.568597 progRead 0.8296 2.8621 0.290 0.771979 genderFemale -17.8974 2.9467 -6.074 1.85e-09 *** hours 4.3646 0.9995 4.367 1.41e-05 *** progSwim:genderFemale 33.6854 4.0969 8.222 7.06e-16 *** progRead:genderFemale 14.9821 3.9766 3.768 0.000176 *** progSwim:hours 4.6917 1.4001 3.351 0.000839 *** progRead:hours -8.1430 1.3682 -5.952 3.82e-09 *** genderFemale:hours 5.4500 1.4445 3.773 0.000172 *** progSwim:genderFemale:hours -10.1461 2.0052 -5.060 5.10e-07 *** progRead:genderFemale:hours -3.9129 1.9242 -2.033 0.042302 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 5.814 on 888 degrees of freedom Multiple R-squared: 0.832, Adjusted R-squared: 0.83 F-statistic: 399.9 on 11 and 888 DF, p-value: &lt; 2.2e-16 anova(fit_lm_loss_3way_b) # A tibble: 8 x 5 Df `Sum Sq` `Mean Sq` `F value` `Pr(&gt;F)` &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2 133277. 66639. 1971. 0 2 1 7.63 7.63 0.226 6.35e- 1 3 1 2339. 2339. 69.2 3.35e-16 4 2 7202. 3601. 107. 3.46e-42 5 2 4973. 2486. 73.5 2.79e-30 6 1 27.0 27.0 0.798 3.72e- 1 7 2 889. 445. 13.2 2.35e- 6 8 888 30022. 33.8 NA NA 6.8.2 Table of Parameter Estimates We can ask the table to show confidence intervals (95% by default) instead of standard errors. texreg::screenreg(list(fit_lm_loss_3way_b), single.row = TRUE, ci.force = TRUE) ====================================================== Model 1 ------------------------------------------------------ (Intercept) 2.97 [ -1.09; 7.03] progSwim 1.64 [ -4.00; 7.28] progRead 0.83 [ -4.78; 6.44] genderFemale -17.90 [-23.67; -12.12] * hours 4.36 [ 2.41; 6.32] * progSwim:genderFemale 33.69 [ 25.66; 41.72] * progRead:genderFemale 14.98 [ 7.19; 22.78] * progSwim:hours 4.69 [ 1.95; 7.44] * progRead:hours -8.14 [-10.82; -5.46] * genderFemale:hours 5.45 [ 2.62; 8.28] * progSwim:genderFemale:hours -10.15 [-14.08; -6.22] * progRead:genderFemale:hours -3.91 [ -7.68; -0.14] * ------------------------------------------------------ R^2 0.83 Adj. R^2 0.83 Num. obs. 900 ====================================================== * 0 outside the confidence interval. 6.8.3 Check Assumptions par(mfrow = c(2, 2)) plot(fit_lm_loss_3way_b) par(mfrow = c(1, 1)) 6.8.4 Plot Estimated Marginal Means fit_lm_loss_3way_b %&gt;% interactions::interact_plot(pred = hours, # x-axis modx = prog, # separate lines mod2 = gender, # separate panels interval = TRUE, # add bands int.type = &quot;confidence&quot;, legend.main = &quot;Program:&quot;) + theme_bw() + theme(legend.background = element_rect(color = &quot;black&quot;), legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Time Spend on Program, hours&quot;, y = &quot;Weight Loss, pounds&quot;, caption = &quot;Note. Bands represent 95% confidence intervals&quot;) + geom_hline(yintercept = 0) fit_lm_loss_3way_b %&gt;% interactions::interact_plot(pred = hours, # x-axis modx = gender, # separate lines mod2 = prog, # separate panels interval = TRUE, # add bands int.type = &quot;confidence&quot;, legend.main = &quot;Gender:&quot;) + theme_bw() + theme(legend.background = element_rect(color = &quot;black&quot;), legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Time Spend on Program, hours&quot;, y = &quot;Weight Loss, pounds&quot;, caption = &quot;Note. Bands represent 95% confidence intervals&quot;) + geom_hline(yintercept = 0) 6.8.5 Simple Slopes Analysis By default, we only get the overall slope for time spent for the mean effort (29.7) and plus-or-minus one standard deviation (24.51 &amp; 34.80). fit_lm_loss_3way_b %&gt;% interactions::sim_slopes(pred = hours, modx = prog, mod2 = gender) ###################### While gender (2nd moderator) = Male ##################### SIMPLE SLOPES ANALYSIS Slope of hours when prog = Jog: Est. S.E. t val. p ------ ------ -------- ------ 4.36 1.00 4.37 0.00 Slope of hours when prog = Swim: Est. S.E. t val. p ------ ------ -------- ------ 9.06 0.98 9.24 0.00 Slope of hours when prog = Read: Est. S.E. t val. p ------- ------ -------- ------ -3.78 0.93 -4.04 0.00 ##################### While gender (2nd moderator) = Female #################### SIMPLE SLOPES ANALYSIS Slope of hours when prog = Jog: Est. S.E. t val. p ------ ------ -------- ------ 9.81 1.04 9.41 0.00 Slope of hours when prog = Swim: Est. S.E. t val. p ------ ------ -------- ------ 4.36 0.99 4.42 0.00 Slope of hours when prog = Read: Est. S.E. t val. p ------- ------ -------- ------ -2.24 0.86 -2.60 0.01 "],["logistic-regression---ex-bronchopulmonary-dysplasia-in-premature-infants.html", "7 Logistic Regression - Ex: Bronchopulmonary Dysplasia in Premature Infants 7.1 Background 7.2 Logistic Regresion: Fit the Model to the data 7.3 Model Fit 7.4 Variance Explained 7.5 Model Compairisons, Inferential 7.6 Parameter Estimates 7.7 Significance of Terms 7.8 Parameter Estimate Tables 7.9 Marginal or Predicted Values 7.10 Marginal Model Plots", " 7 Logistic Regression - Ex: Bronchopulmonary Dysplasia in Premature Infants example walk through: https://stats.idre.ucla.edu/r/dae/logit-regression/ info: https://onlinecourses.science.psu.edu/stat504/node/216/ sjPlot::tab_model (HTML only) http://www.strengejacke.de/sjPlot/articles/sjtlm.html#changing-summary-style-and-content finafit https://www.r-bloggers.com/elegant-regression-results-tables-and-plots-in-r-the-finalfit-package/ Install a package Dr. Schwartz wrote: remotes::install_github(&quot;sarbearschwartz/texreghelpr&quot;) library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML tables library(texreghelpr) # Dr. Schwartz&#39;s helper funtcions for texreg tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(pscl) # psudo R-squared function library(interactions) # interaction plots library(sjPlot) # various plots library(performance) # r-squared values 7.1 Background Simple example demonstrating basic modeling approach: Data on Bronchopulmonary Dysplasia (BPD) from 223 low birth weight infants (weighing less than 1750 grams). 7.1.1 Source Data courtesy of Dr. Linda Van Marter. 7.1.2 Reference Van Marter, L.J., Leviton, A., Kuban, K.C.K., Pagano, M. &amp; Allred, E.N. (1990). Maternal glucocorticoid therapy and reduced risk of bronchopulmonary dysplasia. Pediatrics, 86, 331-336. The data are from a study of low birth weight infants in a neonatal intensive care unit. The study was designed to examine the development of bronchopulmonary dysplasia (BPD), a chronic lung disease, in a sample of 223 infants weighing less than 1750 grams. The response variable is binary, denoting whether an infant develops BPD by day 28 of life (where BPD is defined by both oxygen requirement and compatible chest radiograph). 7.1.3 Variables bpd 0 = no, 1 = yes brthwght number of grams gestage number of weeks toxemia in mother, 0 = no, 1 = yes bpd_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/Regression/VanMarter_%20BPD.txt&quot;, header = TRUE, strip.white = TRUE) n &lt;- nrow(bpd_raw) n [1] 223 tibble::glimpse(bpd_raw) Rows: 223 Columns: 4 $ bpd &lt;int&gt; 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0~ $ brthwght &lt;int&gt; 850, 1500, 1360, 960, 1560, 1120, 810, 1620, 1000, 700, 1330,~ $ gestage &lt;int&gt; 27, 33, 32, 35, 33, 29, 28, 32, 30, 26, 31, 31, 31, 29, 33, 3~ $ toxemia &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ head(bpd_raw) # A tibble: 6 x 4 bpd brthwght gestage toxemia &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 850 27 0 2 0 1500 33 0 3 1 1360 32 0 4 0 960 35 1 5 0 1560 33 0 6 0 1120 29 0 Note: For logistic regression, you need to leave the outcome (dependent variable) coded as zeros 0 and ones 1 and NOT apply lables. You do want to apply labels to factors that function as predictors (independent varaibles). bpd_clean &lt;- bpd_raw %&gt;% dplyr::mutate(toxemia = factor(toxemia, levels = c(0, 1), labels = c(&quot;No&quot;, &quot;Yes&quot;))) summary(bpd_clean) bpd brthwght gestage toxemia Min. :0.0000 Min. : 450 Min. :25.00 No :194 1st Qu.:0.0000 1st Qu.: 895 1st Qu.:28.00 Yes: 29 Median :0.0000 Median :1140 Median :30.00 Mean :0.3408 Mean :1173 Mean :30.09 3rd Qu.:1.0000 3rd Qu.:1465 3rd Qu.:32.00 Max. :1.0000 Max. :1730 Max. :37.00 7.2 Logistic Regresion: Fit the Model to the data Instead of using the lm() function from base R, you use glm(). You also need to add an option to specify which generalization you want to use. To do logistic regression for a binary outcome, use family = binomial(link = \"logit\"). 7.2.1 Null Model: no independent variables fit_glm_0 &lt;- glm(bpd ~ 1, data = bpd_clean, family = binomial(link = &quot;logit&quot;)) summary(fit_glm_0) Call: glm(formula = bpd ~ 1, family = binomial(link = &quot;logit&quot;), data = bpd_clean) Deviance Residuals: Min 1Q Median 3Q Max -0.913 -0.913 -0.913 1.467 1.467 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.6597 0.1413 -4.669 3.02e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 286.14 on 222 degrees of freedom Residual deviance: 286.14 on 222 degrees of freedom AIC: 288.14 Number of Fisher Scoring iterations: 4 7.2.2 Main Effects Model: add 3 predictors Note: Since the unites of weight are so small, the estimated parameter will be super small. To offset the small units, we can re-scale the weights by dividing the grams by 100 to create hectograms. fit_glm_1 &lt;- glm(bpd ~ I(brthwght/100) + gestage + toxemia, data = bpd_clean, family = binomial(link = &quot;logit&quot;)) summary(fit_glm_1) Call: glm(formula = bpd ~ I(brthwght/100) + gestage + toxemia, family = binomial(link = &quot;logit&quot;), data = bpd_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.8400 -0.7029 -0.3352 0.7261 2.9902 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 13.93608 2.98255 4.673 2.98e-06 *** I(brthwght/100) -0.26436 0.08123 -3.254 0.00114 ** gestage -0.38854 0.11489 -3.382 0.00072 *** toxemiaYes -1.34379 0.60750 -2.212 0.02697 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 286.14 on 222 degrees of freedom Residual deviance: 203.71 on 219 degrees of freedom AIC: 211.71 Number of Fisher Scoring iterations: 5 7.3 Model Fit 7.3.1 Log Likelihood and Deviance logLik(fit_glm_0) # Null Model &#39;log Lik.&#39; -143.07 (df=1) logLik(fit_glm_1) # Full Model &#39;log Lik.&#39; -101.8538 (df=4) Note: Deviance = -2 times the Log Likelihood deviance(fit_glm_0) # Null Model [1] 286.14 deviance(fit_glm_1) # Full Model [1] 203.7075 7.3.2 AIC and BIC AIC(fit_glm_0, fit_glm_1) # Full Model # A tibble: 2 x 2 df AIC &lt;dbl&gt; &lt;dbl&gt; 1 1 288. 2 4 212. BIC(fit_glm_0, fit_glm_1) # Full Model # A tibble: 2 x 2 df BIC &lt;dbl&gt; &lt;dbl&gt; 1 1 292. 2 4 225. 7.4 Variance Explained 7.4.1 Many Options Technically, \\(R^2\\) cannot be computed the same way in logistic regression as it is in OLS regression. There are several (over 10) alternatives that endever to calculate a similar metric in different ways. Website: Statistical Horizons Author: Paul Allison Blog Post: Whats the Best R-Squared for Logistic Regression? Compares and contrasts different options and his/our progression through them, in which he now prefers Tjurs statistic (pronounced choor). Great Quote: For those who want an R^2 that behaves like a linear-model R^2, this is deeply unsettling. Note: Dr. Allison is very active at answering questions in the comments of this post. Website: UCLA Institute for Digital Research and Education (IDRE) Article: FAQ: WHAT ARE PSEUDO R-SQUAREDS? Describes several of the most comment R-squared type measures for logistic regression (with Stata). Website: The Stats Geek Author: Jonathan Bartlett, Department of Mathematical Sciences, University of Bath and Associate Editor for the journal Biometrics Blog Post: 2014: R squared in logistic regression Focus on McFaddens pseudo-R squared, in R. 7.4.2 McFaddens pseud-R^2 McFaddens \\(pseudo-R^2\\), in logistic regression, is defined as \\(1\\frac{L_1}{L_0}\\), where \\(L_0\\) represents the log likelihood for the constant-only or \\[ R^2_{McF} = 1 - \\frac{L_1}{L_0} \\] MFR2 &lt;- 1 - (logLik(fit_glm_1)/logLik(fit_glm_0)) MFR2 &#39;log Lik.&#39; 0.2880843 (df=4) performance::r2_mcfadden(fit_glm_1) # R2 for Generalized Linear Regression R2: -0.996 adj. R2: -1.003 7.4.3 Cox &amp; Snell \\(l = e^{L}\\), since \\(L\\) is the log of the likelihood and \\(l\\) is the likelihood\\(log(l) = L\\) \\[ R^2_{CS} = 1 - \\Bigg( \\frac{l_0}{l_1} \\Bigg) ^{2 \\backslash n} \\\\ n = \\text{sample size} \\] CSR2 &lt;- 1 - (exp(logLik(fit_glm_0))/exp(logLik(fit_glm_1)))^(2/n) CSR2 &#39;log Lik.&#39; 0.3090253 (df=1) performance::r2_coxsnell(fit_glm_1) Cox &amp; Snell&#39;s R2 0.3090253 7.4.4 Nagelkerke or Cragg and Uhlers \\[ R^2_{Nag} = \\frac{1 - \\Bigg( \\frac{l_0}{l_1} \\Bigg) ^{2 \\backslash n}} {1 - \\Big( l_0 \\Big) ^{2 \\backslash n}} \\] NR2 &lt;- CSR2 / (1 - exp(logLik(fit_glm_0))^(2/n)) NR2 &#39;log Lik.&#39; 0.4275191 (df=1) performance::r2_nagelkerke(fit_glm_1) Nagelkerke&#39;s R2 0.4275191 7.4.5 Tjurs statistic performance::r2(fit_glm_1) # R2 for Logistic Regression Tjur&#39;s R2: 0.346 7.4.6 Several at Once the pscl::pR2() function Outputs: llh The log-likelihood from the fitted model llhNull The log-likelihood from the intercept-only restricted model G2 Minus two times the difference in the log-likelihoods McFadden McFaddens pseudo r-squared r2ML Maximum likelihood pseudo r-squared r2CU Cragg and Uhlers pseudo r-squared pscl::pR2(fit_glm_1) fitting null model for pseudo-r2 llh llhNull G2 McFadden r2ML r2CU -101.8537711 -143.0699809 82.4324196 0.2880843 0.3090253 0.4275191 7.5 Model Compairisons, Inferential 7.5.1 Likelihood Ratio Test (LRT, aka. Deviance Difference Test) anova(fit_glm_0, fit_glm_1, test = &quot;LRT&quot;) # A tibble: 2 x 5 `Resid. Df` `Resid. Dev` Df Deviance `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 222 286. NA NA NA 2 219 204. 3 82.4 9.23e-18 7.5.2 Bayes Factor and Performance Score performance::compare_performance(fit_glm_0, fit_glm_1, rank = TRUE) # A tibble: 2 x 12 Name Model AIC BIC R2_Tjur RMSE Sigma Log_loss Score_log Score_spherical &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 fit_glm_1 glm 212. 225. 0.346 0.380 0.964 0.457 -40.0 0.0196 2 fit_glm_0 glm 288. 292. 0 0.474 1.14 0.642 -30.4 0.0410 # ... with 2 more variables: PCP &lt;dbl&gt;, Performance_Score &lt;dbl&gt; 7.6 Parameter Estimates 7.6.1 Link: Logit Scale fit_glm_1 %&gt;% coef() (Intercept) I(brthwght/100) gestage toxemiaYes 13.9360826 -0.2643578 -0.3885357 -1.3437865 fit_glm_1 %&gt;% confint() 2.5 % 97.5 % (Intercept) 8.3899004 20.1418979 I(brthwght/100) -0.4289642 -0.1089495 gestage -0.6252493 -0.1725921 toxemiaYes -2.6152602 -0.2133274 7.6.2 Exponentiate: Odds Ratio Scale fit_glm_1 %&gt;% coef() %&gt;% exp() (Intercept) I(brthwght/100) gestage toxemiaYes 1.128142e+06 7.676988e-01 6.780490e-01 2.608561e-01 fit_glm_1 %&gt;% confint() %&gt;% exp() 2.5 % 97.5 % (Intercept) 4.402379e+03 5.591330e+08 I(brthwght/100) 6.511832e-01 8.967757e-01 gestage 5.351280e-01 8.414808e-01 toxemiaYes 7.314875e-02 8.078916e-01 7.7 Significance of Terms 7.7.1 Walds \\(t\\)-Test fit_glm_1 %&gt;% summary() %&gt;% coef() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 13.9360826 2.98255085 4.672538 2.975003e-06 I(brthwght/100) -0.2643578 0.08123149 -3.254376 1.136419e-03 gestage -0.3885357 0.11489128 -3.381768 7.202086e-04 toxemiaYes -1.3437865 0.60750335 -2.211982 2.696791e-02 7.7.2 Single term deletion, \\(\\chi^2\\) LRT Note: Significance of each variable is assessed by comparing it to the model that drops just that one term (type = 3); order doesnt matter. drop1(fit_glm_1, type = 3, test = &quot;LRT&quot;) # A tibble: 4 x 5 Df Deviance AIC LRT `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 NA 204. 212. NA NA 2 1 215. 221. 11.4 0.000744 3 1 217. 223. 13.1 0.000293 4 1 209. 215. 5.52 0.0188 7.7.3 Sequential addition, \\(\\chi^2\\) LRT Note: Signifcance of each additional variable at a time; ordered first to last anova(fit_glm_1, test = &quot;LRT&quot;) # A tibble: 4 x 5 Df Deviance `Resid. Df` `Resid. Dev` `Pr(&gt;Chi)` &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 NA NA 222 286. NA 2 1 62.4 221 224. 2.78e-15 3 1 14.5 220 209. 1.41e- 4 4 1 5.52 219 204. 1.88e- 2 7.8 Parameter Estimate Tables 7.8.1 Logit scale (Link, default) texreg::knitreg(fit_glm_1, single.row = TRUE) Statistical models   Model 1 (Intercept) 13.94 (2.98)*** brthwght/100 -0.26 (0.08)** gestage -0.39 (0.11)*** toxemiaYes -1.34 (0.61)* AIC 211.71 BIC 225.34 Log Likelihood -101.85 Deviance 203.71 Num. obs. 223 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 Note: You may request: Confidence Intervals on Logit scale with the options: ci.force = TRUE, ci.test = 1 texreg::knitreg(fit_glm_1, single.row = TRUE, ci.force = TRUE, ci.test = 1, digits = 6) Statistical models   Model 1 (Intercept) 13.936083 [ 8.090390; 19.781775]* brthwght/100 -0.264358 [-0.423569; -0.105147]* gestage -0.388536 [-0.613718; -0.163353]* toxemiaYes -1.343786 [-2.534471; -0.153102]* AIC 211.707542 BIC 225.336229 Log Likelihood -101.853771 Deviance 203.707542 Num. obs. 223 * 1 outside the confidence interval. 7.8.2 Odds-Ratio Scale (exponentiate) texreg::knitreg(texreghelpr::extract_glm_exp(fit_glm_1), single.row = TRUE) Statistical models   Model 1 (Intercept) 1128141.99 [4402.38; 559132968.51]* brthwght/100 0.77 [ 0.65; 0.90]* gestage 0.68 [ 0.54; 0.84]* toxemiaYes 0.26 [ 0.07; 0.81]* * 0 outside the confidence interval. 7.8.3 BOTH: Logit and Odds-Ratio texreg::knitreg(list(fit_glm_1, texreghelpr::extract_glm_exp(fit_glm_1, include.aic = FALSE, include.bic = FALSE, include.loglik = FALSE, include.deviance = FALSE, include.nobs = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;OR [95% CI]&quot;), single.row = TRUE, ci.test = 1) Statistical models   b (SE) OR [95% CI] (Intercept) 13.94 (2.98)*** 1128141.99 [4402.38; 559132968.51]* brthwght/100 -0.26 (0.08)** 0.77 [ 0.65; 0.90]* gestage -0.39 (0.11)*** 0.68 [ 0.54; 0.84]* toxemiaYes -1.34 (0.61)* 0.26 [ 0.07; 0.81]* AIC 211.71   BIC 225.34   Log Likelihood -101.85   Deviance 203.71   Num. obs. 223   p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 7.9 Marginal or Predicted Values 7.9.1 Across All Predictors Note: By default it will select 5-6 nice values for each continuous variable. All levels of categorical factors will be included. effects::Effect(focal.predictors = c(&quot;brthwght&quot;, &quot;gestage&quot;, &quot;toxemia&quot;), mod = fit_glm_1) brthwght*gestage*toxemia effect , , toxemia = No gestage brthwght 25 28 31 34 37 450 0.9540464 0.8661657 0.66860145 0.38609882 0.163919741 770 0.8990883 0.7352702 0.46404255 0.21253946 0.077608513 1100 0.7883077 0.5372180 0.26571767 0.10137254 0.033971437 1400 0.6275409 0.3443597 0.14069462 0.04856170 0.015661772 1700 0.4325654 0.1920105 0.06897089 0.02257203 0.007147496 , , toxemia = Yes gestage brthwght 25 28 31 34 37 450 0.8441313 0.62800950 0.34481263 0.140937262 0.048654446 770 0.6991701 0.42012556 0.18424240 0.065775334 0.021476635 1100 0.4927426 0.23243034 0.08625483 0.028585525 0.009089901 1400 0.3053170 0.12049913 0.04096070 0.013139235 0.004133317 1700 0.1658709 0.05837137 0.01895794 0.005987954 0.001874370 7.9.2 Specify Some Predictors Note: if a predictor is left off thefocal.predictors, the predictions are AVERAGED over that variable. effects::Effect(focal.predictors = c(&quot;brthwght&quot;, &quot;gestage&quot;), mod = fit_glm_1) brthwght*gestage effect gestage brthwght 25 28 31 34 37 450 0.9457476 0.8445817 0.62880972 0.34558725 0.141352684 770 0.8820911 0.6998904 0.42096066 0.18475801 0.065986229 1100 0.7576801 0.4935992 0.23304229 0.08652530 0.028680839 1400 0.5858727 0.3060443 0.12086278 0.04109553 0.013183745 1700 0.3902779 0.1663456 0.05856001 0.01902178 0.006008386 effects::Effect(focal.predictors = c(&quot;brthwght&quot;, &quot;toxemia&quot;), mod = fit_glm_1) brthwght*toxemia effect toxemia brthwght No Yes 450 0.74150653 0.42801049 770 0.55178083 0.24307063 1100 0.33972675 0.11833437 1400 0.18883689 0.05725008 1700 0.09529265 0.02674118 7.9.3 Set a constant (fixed) value for a predictor(s) effects::Effect(focal.predictors = c(&quot;brthwght&quot;), fixed.predictors = list(gestage = 34, toxemia = &quot;no&quot;), mod = fit_glm_1) brthwght effect brthwght 450 770 1100 1400 1700 0.70662760 0.50827827 0.30168968 0.16351033 0.08125536 7.9.4 Set values for a continuous predictor effects::Effect(focal.predictors = c(&quot;brthwght&quot;, &quot;gestage&quot;), fixed.predictors = list(toxemia = &quot;no&quot;), xlevels = list(gestage = c(24, 32, 36)), mod = fit_glm_1) brthwght*gestage effect gestage brthwght 24 32 36 450 0.9625602 0.53458922 0.195357871 770 0.9168973 0.33018096 0.094361297 1100 0.8217923 0.17083126 0.041730765 1400 0.6760033 0.08526886 0.019322688 1700 0.4856018 0.04046955 0.008836077 7.9.5 Add SE and 95% Confidence Interval effects::Effect(focal.predictors = c(&quot;brthwght&quot;, &quot;gestage&quot;, &quot;toxemia&quot;), xlevels = list(brthwght = c(500, 1000, 1500), gestage = c(30, 36)), mod = fit_glm_1) %&gt;% data.frame() # A tibble: 12 x 7 brthwght gestage toxemia fit se lower upper &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 500 30 No 0.723 0.109 0.474 0.883 2 1000 30 No 0.410 0.0523 0.313 0.515 3 1500 30 No 0.156 0.0475 0.0839 0.273 4 500 36 No 0.202 0.179 0.0281 0.690 5 1000 36 No 0.0633 0.0484 0.0135 0.251 6 1500 36 No 0.0177 0.0115 0.00492 0.0616 7 500 30 Yes 0.405 0.171 0.145 0.732 8 1000 30 Yes 0.154 0.0756 0.0548 0.362 9 1500 30 Yes 0.0461 0.0312 0.0119 0.162 10 500 36 Yes 0.0620 0.0701 0.00619 0.412 11 1000 36 Yes 0.0173 0.0168 0.00255 0.109 12 1500 36 Yes 0.00468 0.00422 0.000794 0.0270 7.10 Marginal Model Plots 7.10.1 Individual Marginal Plots for one IV, individually The sjPlot::plot_model() function automatically transforms the predictions to the probability score when you include the type = \"pred\" option. sjPlot::plot_model(fit_glm_1, type = &quot;pred&quot;) $brthwght $gestage $toxemia 7.10.2 Combination Marginal Plots for two-three IVs, all at once For continuous IV that are not on the x-axis (pred), be default three values will be selected: the mean and plus-or-minus one stadard error for the mean (SEM). The outcome.scale = \"link\" option plots the LOGIT scale on the y-axis. interactions::interact_plot(model = fit_glm_1, pred = brthwght, modx = gestage, mod2 = toxemia, outcome.scale = &quot;link&quot;) Alternatively, you may use the modx.labels option to set specific values at which to plot the moderator. The outcome.scale = \"response\" option plots the PROBABILITY scale on the y-axis. interactions::interact_plot(model = fit_glm_1, pred = brthwght, modx = gestage, modx.labels = c(28, 32, 36), mod2 = toxemia, outcome.scale = &quot;response&quot;) You can always do more work to get to a PUBLISH-ABLE version interactions::interact_plot(model = fit_glm_1, pred = brthwght, modx = gestage, modx.labels = c(28, 30, 32), mod2 = toxemia, outcome.scale = &quot;response&quot;, x.label = &quot;Birthweight, grams&quot;, y.label = &quot;Probability of Bronchopulmonary Dysplasia&quot;, legend.main = &quot;Gestational Age, weeks&quot;, mod2.label = c(&quot;Mother does NOT have Toxemia&quot;, &quot;Mother DOES have Toxemia&quot;), colors = rep(&quot;black&quot;, 3)) + geom_hline(yintercept = .5, alpha = .2) + theme_bw() + theme(legend.background = element_rect(color = &quot;black&quot;), legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.key.width = unit(2, &quot;cm&quot;)) 7.10.3 Total Control is also available effects::Effect(focal.predictors = c(&quot;brthwght&quot;, &quot;toxemia&quot;, &quot;gestage&quot;), mod = fit_glm_1, xlevels = list(brthwght = seq(from = 450, to = 1730, by = 10), gestage = c(28, 32, 36))) %&gt;% data.frame() %&gt;% dplyr::mutate(gestage = factor(gestage)) %&gt;% ggplot(aes(x = brthwght, y = fit)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = toxemia), alpha = .2) + geom_line(aes(linetype = toxemia, color = toxemia), size = 1) + facet_grid(. ~ gestage, labeller = label_both) + theme_bw() effects::Effect(focal.predictors = c(&quot;brthwght&quot;, &quot;toxemia&quot;, &quot;gestage&quot;), mod = fit_glm_1, xlevels = list(brthwght = seq(from = 450, to = 1730, by = 10), gestage = c(28, 32, 36))) %&gt;% data.frame() %&gt;% dplyr::mutate(gestage = factor(gestage)) %&gt;% ggplot(aes(x = brthwght, y = fit)) + geom_line(aes(linetype = toxemia, color = toxemia), size = 1) + facet_grid(. ~ gestage, labeller = label_both) + theme_bw() "],["logistic-regression---ex-maternal-risk-factor-for-low-birth-weight-delivery.html", "8 Logistic Regression - Ex: Maternal Risk Factor for Low Birth Weight Delivery 8.1 Background 8.2 Exploratory Data Analysis 8.3 Logistic Regression - Simple, unadjusted models 8.4 Logistic Regression - Multivariate, with Main Effects Only 8.5 Logistic Regression - Multivariate, with Interactions 8.6 Logistic Regression - Multivariate, Simplify 8.7 Logistic Regression - Multivariate, Final Model", " 8 Logistic Regression - Ex: Maternal Risk Factor for Low Birth Weight Delivery library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(texreghelpr) # library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(sjPlot) # Quick plots and tables for models library(pscl) # psudo R-squared function library(glue) # Interpreted String Literals library(interactions) # interaction plots library(sjPlot) # various plots library(performance) # r-squared values 8.1 Background More complex example demonstrating modeling decisions Another set of data from a study investigating predictors of low birth weight id infants unique identification number Dependent variable (DV) or outcome low Low birth weight (outcome) 0 = birth weight &gt;2500 g (normal) 1 = birth weight &lt; 2500 g (low)) bwt actual infant birth weight in grams (ignore for now) Independent variables (IV) or predictors age Age of mother, in years lwt Mothers weight at last menstrual period, in pounds race Race: 1 = White, 2 = Black, 3 = Other smoke Smoking status during pregnancy:1 = Yes, 0 = No ptl History of premature labor: 0 = None, 1 = One, 2 = two, 3 = three ht History of hypertension: 1 = Yes, 0 = No ui Uterine irritability: 1 = Yes, 0 = No ftv Number of physician visits in 1st trimester: 0 = None, 1 = One,  6 = six 8.1.1 Raw Dataset The data is saved in a text file (.txt) without any labels. lowbwt_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/Regression/lowbwt.txt&quot;, header = TRUE, sep = &quot;&quot;, na.strings = &quot;NA&quot;, dec = &quot;.&quot;, strip.white = TRUE) tibble::glimpse(lowbwt_raw) Observations: 189 Variables: 11 $ id &lt;int&gt; 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, ... $ low &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... $ age &lt;int&gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, ... $ lwt &lt;int&gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 15... $ race &lt;int&gt; 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3,... $ smoke &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,... $ ptl &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,... $ ht &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,... $ ui &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,... $ ftv &lt;int&gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0,... $ bwt &lt;int&gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 26... 8.1.2 Declare Factors lowbwt_clean &lt;- lowbwt_raw %&gt;% dplyr::mutate(id = factor(id)) %&gt;% dplyr::mutate(low = low %&gt;% factor() %&gt;% forcats::fct_recode(&quot;birth weight &gt;2500 g (normal)&quot; = &quot;0&quot;, &quot;birth weight &lt; 2500 g (low)&quot; = &quot;1&quot;)) %&gt;% dplyr::mutate(race = race %&gt;% factor() %&gt;% forcats::fct_recode(&quot;White&quot; = &quot;1&quot;, &quot;Black&quot; = &quot;2&quot;, &quot;Other&quot; = &quot;3&quot;)) %&gt;% dplyr::mutate(ptl_any = as.numeric(ptl &gt; 0)) %&gt;% # collapse into 0 = none vs. 1 = at least one dplyr::mutate(ptl = factor(ptl)) %&gt;% # declare the number of pre-term labors to be a factor: 0, 1, 2, 3 dplyr::mutate_at(vars(smoke, ht, ui, ptl_any), # declare all there variables to be factors with the same two levels factor, levels = 0:1, labels = c(&quot;No&quot;, &quot;Yes&quot;)) Display the structure of the clean version of the dataset tibble::glimpse(lowbwt_clean) Rows: 189 Columns: 12 $ id &lt;fct&gt; 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 1~ $ low &lt;fct&gt; birth weight &gt;2500 g (normal), birth weight &gt;2500 g (normal), ~ $ age &lt;int&gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, 18, 18~ $ lwt &lt;int&gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 150, 95,~ $ race &lt;fct&gt; Black, Other, White, White, White, Other, White, Other, White,~ $ smoke &lt;fct&gt; No, No, Yes, Yes, Yes, No, No, No, Yes, Yes, No, No, No, No, Y~ $ ptl &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,~ $ ht &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, Yes, No, No, N~ $ ui &lt;fct&gt; Yes, No, No, Yes, Yes, No, No, No, No, No, No, No, No, Yes, No~ $ ftv &lt;int&gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 1, 2,~ $ bwt &lt;int&gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 2665, 27~ $ ptl_any &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, Yes, No, N~ lowbwt_clean # A tibble: 189 x 12 id low age lwt race smoke ptl ht ui ftv bwt ptl_any &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; 1 85 birth we~ 19 182 Black No 0 No Yes 0 2523 No 2 86 birth we~ 33 155 Other No 0 No No 3 2551 No 3 87 birth we~ 20 105 White Yes 0 No No 1 2557 No 4 88 birth we~ 21 108 White Yes 0 No Yes 2 2594 No 5 89 birth we~ 18 107 White Yes 0 No Yes 0 2600 No 6 91 birth we~ 21 124 Other No 0 No No 0 2622 No 7 92 birth we~ 22 118 White No 0 No No 1 2637 No 8 93 birth we~ 17 103 Other No 0 No No 1 2637 No 9 94 birth we~ 29 123 White Yes 0 No No 1 2663 No 10 95 birth we~ 26 113 White Yes 0 No No 0 2665 No # ... with 179 more rows 8.2 Exploratory Data Analysis lowbwt_clean %&gt;% furniture::table1(&quot;Age, years&quot; = age, &quot;Weight, pounds&quot; = lwt, &quot;Race&quot; = race, &quot;Smoking During pregnancy&quot; = smoke, &quot;History of Premature Labor, any&quot; = ptl_any, &quot;History of Premature Labor, number&quot; = ptl, &quot;History of Hypertension&quot; = ht, &quot;Uterince Irritability&quot; = ui, &quot;1st Tri Dr Visits&quot; = ftv, splitby = ~ low, test = TRUE, output = &quot;markdown&quot;) birth weight &gt;2500 g (normal) birth weight &lt; 2500 g (low) P-Value n = 130 n = 59 Age, years 0.103 23.7 (5.6) 22.3 (4.5) Weight, pounds 0.02 133.3 (31.7) 122.1 (26.6) Race 0.082 White 73 (56.2%) 23 (39%) Black 15 (11.5%) 11 (18.6%) Other 42 (32.3%) 25 (42.4%) Smoking During pregnancy 0.04 No 86 (66.2%) 29 (49.2%) Yes 44 (33.8%) 30 (50.8%) History of Premature Labor, any &lt;.001 No 118 (90.8%) 41 (69.5%) Yes 12 (9.2%) 18 (30.5%) History of Premature Labor, number &lt;.001 0 118 (90.8%) 41 (69.5%) 1 8 (6.2%) 16 (27.1%) 2 3 (2.3%) 2 (3.4%) 3 1 (0.8%) 0 (0%) History of Hypertension 0.076 No 125 (96.2%) 52 (88.1%) Yes 5 (3.8%) 7 (11.9%) Uterince Irritability 0.035 No 116 (89.2%) 45 (76.3%) Yes 14 (10.8%) 14 (23.7%) 1st Tri Dr Visits 0.389 0.8 (1.1) 0.7 (1.0) 8.3 Logistic Regression - Simple, unadjusted models low1.age &lt;- glm(low ~ age, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) low1.lwt &lt;- glm(low ~ lwt, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) low1.race &lt;- glm(low ~ race, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) low1.smoke &lt;- glm(low ~ smoke, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) low1.ptl &lt;- glm(low ~ ptl_any, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) low1.ht &lt;- glm(low ~ ht, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) low1.ui &lt;- glm(low ~ ui, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) low1.ftv &lt;- glm(low ~ ftv, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) Note: the parameter estimates here are for the LOGIT scale, not the odds ration (OR) or even the probability. texreg::knitreg(list(low1.age, low1.lwt, low1.race, low1.smoke), custom.model.names = c(&quot;Age&quot;, &quot;Weight&quot;, &quot;Race&quot;, &quot;Smoker&quot;), caption = &quot;Simple, Unadjusted Logistic Regression: Models 1-4&quot;, caption.above = TRUE, digits = 3) Simple, Unadjusted Logistic Regression: Models 1-4   Age Weight Race Smoker (Intercept) 0.385 0.998 -1.155*** -1.087***   (0.732) (0.785) (0.239) (0.215) age -0.051         (0.032)       lwt   -0.014*         (0.006)     raceBlack     0.845         (0.463)   raceOther     0.636         (0.348)   smokeYes       0.704*         (0.320) AIC 235.912 232.691 235.662 233.805 BIC 242.395 239.174 245.387 240.288 Log Likelihood -115.956 -114.345 -114.831 -114.902 Deviance 231.912 228.691 229.662 229.805 Num. obs. 189 189 189 189 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 texreg::knitreg(list(low1.ptl, low1.ht, low1.ui, low1.ftv), custom.model.names = c(&quot;Pre-Labor&quot;, &quot;Hypertension&quot;, &quot;Uterine&quot;, &quot;Visits&quot;), caption = &quot;Simple, Unadjusted Logistic Regression: Models 5-8&quot;, caption.above = TRUE, digits = 3) Simple, Unadjusted Logistic Regression: Models 5-8   Pre-Labor Hypertension Uterine Visits (Intercept) -1.057*** -0.877*** -0.947*** -0.687***   (0.181) (0.165) (0.176) (0.195) ptl_anyYes 1.463***         (0.414)       htYes   1.214*         (0.608)     uiYes     0.947*         (0.417)   ftv       -0.135         (0.157) AIC 225.898 234.650 233.596 237.899 BIC 232.381 241.133 240.079 244.382 Log Likelihood -110.949 -115.325 -114.798 -116.949 Deviance 221.898 230.650 229.596 233.899 Num. obs. 189 189 189 189 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 8.4 Logistic Regression - Multivariate, with Main Effects Only Main-effects multiple logistic regression model low1_1 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) summary(low1_1) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6459 -0.7992 -0.5103 0.9388 2.2018 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.63691 1.23028 0.518 0.60467 age -0.03775 0.03781 -0.998 0.31808 lwt -0.01491 0.00704 -2.118 0.03419 * raceBlack 1.21274 0.53248 2.278 0.02275 * raceOther 0.80412 0.44843 1.793 0.07294 . smokeYes 0.84640 0.40806 2.074 0.03806 * ptl_anyYes 1.22175 0.46301 2.639 0.00832 ** htYes 1.83869 0.70324 2.615 0.00893 ** uiYes 0.71113 0.46311 1.536 0.12465 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 196.83 on 180 degrees of freedom AIC: 214.83 Number of Fisher Scoring iterations: 4 8.5 Logistic Regression - Multivariate, with Interactions Before removing non-significant main effects, test plausible interactions Try interactions between age and lwt, age and smoke, lwt and smoke, 1 at a time 8.5.1 Age and Weight low1_2 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:lwt, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) summary(low1_2) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:lwt, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6426 -0.8004 -0.5163 0.9400 2.1989 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.1396293 4.0443041 0.282 0.77811 age -0.0597273 0.1724339 -0.346 0.72906 lwt -0.0188377 0.0309225 -0.609 0.54240 raceBlack 1.2071359 0.5341716 2.260 0.02383 * raceOther 0.7977750 0.4505381 1.771 0.07661 . smokeYes 0.8433655 0.4083953 2.065 0.03892 * ptl_anyYes 1.2260463 0.4642795 2.641 0.00827 ** htYes 1.8389418 0.7034162 2.614 0.00894 ** uiYes 0.7142688 0.4642468 1.539 0.12391 age:lwt 0.0001718 0.0013146 0.131 0.89600 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 196.82 on 179 degrees of freedom AIC: 216.82 Number of Fisher Scoring iterations: 5 8.5.1.1 Compare Model Fits vs. Likelihood Ratio Test anova(low1_1, low1_2, test = &#39;LRT&#39;) # A tibble: 2 x 5 `Resid. Df` `Resid. Dev` Df Deviance `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 180 197. NA NA NA 2 179 197. 1 0.0170 0.896 8.5.1.2 Type II Analysis of Deviance Table Anova(low1_2, test = &#39;LR&#39;) # A tibble: 8 x 3 `LR Chisq` Df `Pr(&gt;Chisq)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1.02 1 0.313 2 5.00 1 0.0254 3 6.27 2 0.0436 4 4.38 1 0.0364 5 7.13 1 0.00758 6 7.18 1 0.00738 7 2.33 1 0.127 8 0.0170 1 0.896 8.5.1.3 Type III Analysis of Deviance Table Anova(low1_2, test = &#39;LR&#39;, type = &#39;III&#39;) # A tibble: 8 x 3 `LR Chisq` Df `Pr(&gt;Chisq)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.119 1 0.730 2 0.372 1 0.542 3 6.27 2 0.0436 4 4.38 1 0.0364 5 7.13 1 0.00758 6 7.18 1 0.00738 7 2.33 1 0.127 8 0.0170 1 0.896 8.5.2 Age and Smoking low1_3 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:smoke, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) summary(low1_3) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui + age:smoke, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.7020 -0.8016 -0.4970 0.9055 2.2124 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.311029 1.483078 0.884 0.37670 age -0.068293 0.053865 -1.268 0.20485 lwt -0.014701 0.007001 -2.100 0.03575 * raceBlack 1.126482 0.543684 2.072 0.03827 * raceOther 0.768241 0.451199 1.703 0.08863 . smokeYes -0.601286 1.782234 -0.337 0.73583 ptl_anyYes 1.197295 0.460992 2.597 0.00940 ** htYes 1.860851 0.705311 2.638 0.00833 ** uiYes 0.783999 0.472451 1.659 0.09703 . age:smokeYes 0.063744 0.076634 0.832 0.40552 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 196.14 on 179 degrees of freedom AIC: 216.14 Number of Fisher Scoring iterations: 5 8.5.2.1 Compare Model Fits vs. Likelihood Ratio Test anova(low1_1, low1_3, test = &#39;LRT&#39;) # A tibble: 2 x 5 `Resid. Df` `Resid. Dev` Df Deviance `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 180 197. NA NA NA 2 179 196. 1 0.699 0.403 performance::compare_performance(low1_1, low1_3, rank = TRUE) # A tibble: 2 x 12 Name Model AIC BIC R2_Tjur RMSE Sigma Log_loss Score_log Score_spherical &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 low1_1 glm 215. 244. 0.190 0.418 1.05 0.521 -24.6 0.0164 2 low1_3 glm 216. 249. 0.192 0.417 1.05 0.519 -24.5 0.0156 # ... with 2 more variables: PCP &lt;dbl&gt;, Performance_Score &lt;dbl&gt; 8.5.3 Weight and Smoking low1_4 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht + ui + lwt:smoke, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) summary(low1_4) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht + ui + lwt:smoke, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6816 -0.7874 -0.5251 0.8876 2.2365 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.74840 1.59439 1.097 0.27282 age -0.03768 0.03809 -0.989 0.32259 lwt -0.02362 0.01077 -2.193 0.02828 * raceBlack 1.23110 0.53358 2.307 0.02104 * raceOther 0.71646 0.45183 1.586 0.11281 smokeYes -1.13566 1.75557 -0.647 0.51770 ptl_anyYes 1.26472 0.46488 2.721 0.00652 ** htYes 1.74326 0.70738 2.464 0.01373 * uiYes 0.80121 0.47044 1.703 0.08855 . lwt:smokeYes 0.01555 0.01344 1.157 0.24740 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 195.44 on 179 degrees of freedom AIC: 215.44 Number of Fisher Scoring iterations: 5 8.5.3.1 Compare Model Fits vs. Likelihood Ratio Test anova(low1_1, low1_4, test = &#39;LRT&#39;) # A tibble: 2 x 5 `Resid. Df` `Resid. Dev` Df Deviance `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 180 197. NA NA NA 2 179 195. 1 1.39 0.238 8.6 Logistic Regression - Multivariate, Simplify No interactions are significant Remove non-significant main effects 8.6.1 Remove the least significant perdictor: ui low1_5 &lt;- glm(low ~ age + lwt + race + smoke + ptl_any + ht, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) summary(low1_5) Call: glm(formula = low ~ age + lwt + race + smoke + ptl_any + ht, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6533 -0.8202 -0.5299 0.9709 2.1982 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.924910 1.202693 0.769 0.44187 age -0.042784 0.037567 -1.139 0.25475 lwt -0.015436 0.007044 -2.191 0.02843 * raceBlack 1.168452 0.532577 2.194 0.02824 * raceOther 0.814620 0.442740 1.840 0.06578 . smokeYes 0.858332 0.404787 2.120 0.03397 * ptl_anyYes 1.333970 0.457573 2.915 0.00355 ** htYes 1.740511 0.703104 2.475 0.01331 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 199.15 on 181 degrees of freedom AIC: 215.15 Number of Fisher Scoring iterations: 4 8.7 Logistic Regression - Multivariate, Final Model Since the mothers age is theoretically a meaningful variable, it should probably be retained. Revise so that age is interpreted in 5-year and lwt in 20 lb increments and the intercept has meaning. low1_6 &lt;- glm(low ~ I((age - 20)/5) + I((lwt - 125)/20) + race + smoke + ptl_any + ht, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) summary(low1_6) Call: glm(formula = low ~ I((age - 20)/5) + I((lwt - 125)/20) + race + smoke + ptl_any + ht, family = binomial(link = &quot;logit&quot;), data = lowbwt_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6533 -0.8202 -0.5299 0.9709 2.1982 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.8603 0.4092 -4.546 5.48e-06 *** I((age - 20)/5) -0.2139 0.1878 -1.139 0.25475 I((lwt - 125)/20) -0.3087 0.1409 -2.191 0.02843 * raceBlack 1.1685 0.5326 2.194 0.02824 * raceOther 0.8146 0.4427 1.840 0.06578 . smokeYes 0.8583 0.4048 2.120 0.03397 * ptl_anyYes 1.3340 0.4576 2.915 0.00355 ** htYes 1.7405 0.7031 2.475 0.01331 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 199.15 on 181 degrees of freedom AIC: 215.15 Number of Fisher Scoring iterations: 4 8.7.1 Several \\(R^2\\) measures with the pscl::pR2() function pscl::pR2(low1_6) fitting null model for pseudo-r2 llh llhNull G2 McFadden r2ML r2CU -99.5757045 -117.3359981 35.5205871 0.1513627 0.1713353 0.2409463 performance::r2(low1_6) # R2 for Logistic Regression Tjur&#39;s R2: 0.180 8.7.2 Parameter Estiamtes Table 8.7.2.1 Using texreg::screenreg() Default: parameters are in terms of the logit or log odds ratio texreg::knitreg(low1_6, single.row = TRUE, digits = 3) Statistical models   Model 1 (Intercept) -1.860 (0.409)*** (age - 20)/5 -0.214 (0.188) (lwt - 125)/20 -0.309 (0.141)* raceBlack 1.168 (0.533)* raceOther 0.815 (0.443) smokeYes 0.858 (0.405)* ptl_anyYes 1.334 (0.458)** htYes 1.741 (0.703)* AIC 215.151 BIC 241.085 Log Likelihood -99.576 Deviance 199.151 Num. obs. 189 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 The texreg package uses an intermediate function called extract() to extract information for the model and then put it in the right places in the table. I have writen a function called extract_glm_exp() that is helpful. texreg::knitreg(extract_glm_exp(low1_6), custom.coef.names = c(&quot;BL: 125 lb, 20 yr old White Mother&quot;, &quot;Additional 5 years older&quot;, &quot;Additional 20 lbs pre-pregnancy&quot;, &quot;Race: Black vs. White&quot;, &quot;Race: Other vs. White&quot;, &quot;Smoking During pregnancy&quot;, &quot;History of Any Premature Labor&quot;, &quot;History of Hypertension&quot;), custom.model.names = &quot;OR, Low Birth Weight&quot;, single.row = TRUE, ci.test = 1) Statistical models   OR, Low Birth Weight BL: 125 lb, 20 yr old White Mother 0.16 [0.07; 0.33]* Additional 5 years older 0.81 [0.55; 1.16] Additional 20 lbs pre-pregnancy 0.73 [0.55; 0.95]* Race: Black vs. White 3.22 [1.13; 9.31]* Race: Other vs. White 2.26 [0.96; 5.50] Smoking During pregnancy 2.36 [1.08; 5.32]* History of Any Premature Labor 3.80 [1.57; 9.53]* History of Hypertension 5.70 [1.49; 24.76]* * 1 outside the confidence interval. texreg::knitreg(list(low1_6, extract_glm_exp(low1_6, include.aic = FALSE, include.bic = FALSE, include.loglik = FALSE, include.deviance = FALSE, include.nobs = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;OR [95 CI]&quot;), custom.coef.map = list(&quot;(Intercept)&quot; = &quot;BL: 125 lb, 20 yr old White Mother&quot;, &quot;I((age - 20)/5)&quot; = &quot;Additional 5 years older&quot;, &quot;I((lwt - 125)/20)&quot; = &quot;Additional 20 lbs pre-pregnancy&quot;, &quot;raceBlack&quot; = &quot;Race: Black vs. White&quot;, &quot;raceOther&quot; = &quot;Race: Other vs. White&quot;, &quot;smokeYes&quot; = &quot;Smoking During pregnancy&quot;, &quot;ptl_anyYes&quot; = &quot;History of Any Premature Labor&quot;, &quot;htYes&quot; = &quot;History of Hypertension&quot;), caption = &quot;Maternal Factors effect Low Birthweight of Infant&quot;, caption.above = TRUE, single.row = TRUE, ci.test = 1) Maternal Factors effect Low Birthweight of Infant   b (SE) OR [95 CI] BL: 125 lb, 20 yr old White Mother -1.86 (0.41)*** 0.16 [0.07; 0.33]* Additional 5 years older -0.21 (0.19) 0.81 [0.55; 1.16] Additional 20 lbs pre-pregnancy -0.31 (0.14)* 0.73 [0.55; 0.95]* Race: Black vs. White 1.17 (0.53)* 3.22 [1.13; 9.31]* Race: Other vs. White 0.81 (0.44) 2.26 [0.96; 5.50] Smoking During pregnancy 0.86 (0.40)* 2.36 [1.08; 5.32]* History of Any Premature Labor 1.33 (0.46)** 3.80 [1.57; 9.53]* History of Hypertension 1.74 (0.70)* 5.70 [1.49; 24.76]* AIC 215.15   BIC 241.09   Log Likelihood -99.58   Deviance 199.15   Num. obs. 189   p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 8.7.3 Marginal Model Plot 8.7.3.1 Focus on: Mothers Age, weight, and race interactions::interact_plot(model = low1_6, pred = lwt, modx = race, mod2 = age) effects::Effect(focal.predictors = c(&quot;age&quot;, &quot;lwt&quot;, &quot;race&quot;), mod = low1_6, xlevels = list(age = c(20, 30, 40), lwt = seq(from = 80, to = 250, by = 5))) %&gt;% data.frame() %&gt;% dplyr::mutate(age_labels = glue(&quot;Mother Age: {age}&quot;)) %&gt;% ggplot(aes(x = lwt, y = fit)) + geom_line(aes(color = race, linetype = race), size = 1) + theme_bw() + facet_grid(.~ age_labels) + labs(title = &quot;Risk of Low Birth Weight&quot;, subtitle = &quot;Illustates risk given mother is a non-smoker, without a history of pre-term labor or hypertension&quot;, x = &quot;Mother&#39;s Weight Pre-Pregnancy, pounds&quot;, y = &quot;Predicted Probability\\nBaby has Low Birth Weight (&lt; 2500 grams)&quot;, color = &quot;Mother&#39;s Race&quot;, linetype = &quot;Mother&#39;s Race&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + scale_linetype_manual(values = c(&quot;longdash&quot;, &quot;dotted&quot;, &quot;solid&quot;)) + scale_color_manual(values = c( &quot;coral2&quot;, &quot;dodger blue&quot;, &quot;gray50&quot;)) 8.7.3.2 Focus on: Mothers weight and smoking status during pregnancy, as well as history of any per-term labor and hypertension interactions::interact_plot(model = low1_6, pred = lwt, modx = smoke, mod2 = ptl_any) interactions::interact_plot(model = low1_6, pred = lwt, modx = smoke, mod2 = ht) effects::Effect(focal.predictors = c(&quot;lwt&quot;, &quot;smoke&quot;, &quot;ptl_any&quot;, &quot;ht&quot;), fixed.predictors = list(age = 20), mod = low1_6, xlevels = list(lwt = seq(from = 80, to = 250, by = 5))) %&gt;% data.frame() %&gt;% dplyr::mutate(smoke = forcats::fct_rev(smoke)) %&gt;% dplyr::mutate(ptl_any_labels = glue(&quot;History of Preterm Labor: {ptl_any}&quot;)) %&gt;% dplyr::mutate(ht_labels = glue(&quot;History of Hypertension: {ht}&quot;) %&gt;% forcats::fct_rev()) %&gt;% ggplot(aes(x = lwt, y = fit)) + geom_line(aes(color = smoke, linetype = smoke), size = 1) + theme_bw() + facet_grid(ht_labels ~ ptl_any_labels) + labs(title = &quot;Risk of Low Birth Weight&quot;, subtitle = &quot;Illustates risk given the mother is 20 years old and white&quot;, x = &quot;Mother&#39;s Weight Pre-Pregnancy, pounds&quot;, y = &quot;Predicted Probability\\nBaby has Low Birth Weight (&lt; 2500 grams)&quot;, color = &quot;Mother Smoked&quot;, linetype = &quot;Mother Smoked&quot;) + theme(legend.position = c(1, .5), legend.justification = c(1.1, 1.15), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + scale_linetype_manual(values = c(&quot;longdash&quot;, &quot;solid&quot;)) + scale_color_manual(values = c( &quot;coral2&quot;, &quot;dodger blue&quot;)) "],["logistic-regression---ex-depression-hoffman.html", "9 Logistic Regression - Ex: Depression (Hoffman) 9.1 Exploratory Data Analysis 9.2 Calcualte: Probability, Odds, and Odds-Ratios 9.3 Logisitc Regression Model 1: one IV 9.4 Logisitic Regression Model 2: many IVs 9.5 Compare Models", " 9 Logistic Regression - Ex: Depression (Hoffman) library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(texreghelpr) # GITHUB: sarbearschartz/texreghelpr library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(sjPlot) # Quick plots and tables for models library(pscl) # psudo R-squared function library(glue) # Interpreted String Literals library(interactions) # interaction plots library(sjPlot) # various plots library(performance) # r-squared values This dataset comes from John Hoffmans textbook: Regression Models for Categorical, Count, and Related Variables: An Applied Approach (2004) Amazon link, 2014 edition Chapter 3: Logistic and Probit Regression Models Dataset: The following example uses the SPSS data set Depress.sav. The dependent variable of interest is a measure of life satisfaction, labeled satlife. df_depress &lt;- haven::read_spss(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/Hoffmann_datasets/depress.sav&quot;) %&gt;% haven::as_factor() tibble::glimpse(df_depress) Rows: 118 Columns: 14 $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,~ $ age &lt;dbl&gt; 39, 41, 42, 30, 35, 44, 31, 39, 35, 33, 38, 31, 40, 44, 43, 32~ $ iq &lt;dbl&gt; 94, 89, 83, 99, 94, 90, 94, 87, NA, 92, 92, 94, 91, 86, 90, NA~ $ anxiety &lt;fct&gt; medium low, medium low, medium high, medium low, medium low, N~ $ depress &lt;fct&gt; medium, medium, high, medium, low, low, medium, medium, medium~ $ sleep &lt;fct&gt; low, low, low, low, high, low, NA, low, low, low, high, low, l~ $ sex &lt;fct&gt; female, female, female, female, female, male, female, female, ~ $ lifesat &lt;fct&gt; low, low, low, low, high, high, low, high, low, low, high, hig~ $ weight &lt;dbl&gt; 4.9, 2.2, 4.0, -2.6, -0.3, 0.9, -1.5, 3.5, -1.2, 0.8, -1.9, 5.~ $ satlife &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,~ $ male &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,~ $ sleep1 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, NA, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, N~ $ newiq &lt;dbl&gt; 2.21, -2.79, -8.79, 7.21, 2.21, -1.79, 2.21, -4.79, NA, 0.21, ~ $ newage &lt;dbl&gt; 1.5424, 3.5424, 4.5424, -7.4576, -2.4576, 6.5424, -6.4576, 1.5~ psych::headTail(df_depress) # A tibble: 9 x 14 id age iq anxiety depress sleep sex lifesat weight satlife male &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1 39 94 medium low medium low fema~ low 4.9 0 0 2 2 41 89 medium low medium low fema~ low 2.2 0 0 3 3 42 83 medium high high low fema~ low 4 0 0 4 4 30 99 medium low medium low fema~ low -2.6 0 0 5 ... ... ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... ... 6 115 39 87 medium low medium low male low &lt;NA&gt; 0 1 7 116 41 86 medium high medium high male low -1 0 1 8 117 33 89 low low high male high 6.5 1 1 9 118 42 &lt;NA&gt; medium high medium low fema~ low 4.9 0 0 # ... with 3 more variables: sleep1 &lt;chr&gt;, newiq &lt;chr&gt;, newage &lt;chr&gt; df_depress %&gt;% dplyr::select(satlife, lifesat) %&gt;% table() %&gt;% addmargins() lifesat satlife high low Sum 0 0 65 65 1 52 0 52 Sum 52 65 117 9.1 Exploratory Data Analysis Dependent Variable = satlife (numeric version) or lifesat (factor version) 9.1.1 Visualize df_depress %&gt;% ggplot(aes(x = age, y = satlife)) + geom_count() + geom_smooth(method = &quot;lm&quot;) + theme_bw() + labs(x = &quot;Age in Years&quot;, y = &quot;Life Satisfaction, numeric&quot;) Figure 9.1: Hoffmans Figure 2.3, top of page 46 df_depress %&gt;% ggplot(aes(x = age, y = satlife)) + geom_count() + geom_smooth(method = &quot;lm&quot;) + theme_bw() + labs(x = &quot;Age in Years&quot;, y = &quot;Life Satisfaction, numeric&quot;) + facet_grid(~ sex) + theme(legend.position = &quot;bottom&quot;) 9.1.2 Summary Table Independent = sex df_depress %&gt;% dplyr::select(lifesat, sex) %&gt;% table() %&gt;% addmargins() sex lifesat male female Sum high 14 38 52 low 7 58 65 Sum 21 96 117 df_depress %&gt;% dplyr::group_by(sex) %&gt;% furniture::table1(lifesat, total = TRUE, caption = &quot;Hoffman&#39;s EXAMPLE 3.1 Cross-Tabulation of Gender and Life Satisfaction (top page 50)&quot;, output = &quot;markdown&quot;) Table 9.1: Hoffmans EXAMPLE 3.1 Cross-Tabulation of Gender and Life Satisfaction (top page 50) Total male female n = 117 n = 21 n = 96 lifesat high 52 (44.4%) 14 (66.7%) 38 (39.6%) low 65 (55.6%) 7 (33.3%) 58 (60.4%) 9.2 Calcualte: Probability, Odds, and Odds-Ratios 9.2.1 Marginal, over all the sample Tally the number of participants happy and not happy (i.e. depressed). df_depress %&gt;% dplyr::select(satlife) %&gt;% table() %&gt;% addmargins() . 0 1 Sum 65 52 117 9.2.1.1 Probability of being happy \\[ prob_{yes} = \\frac{n_{yes}}{n_{total}} = \\frac{n_{yes}}{n_{yes} + n_{no}} \\] prob &lt;- 52 / 117 prob [1] 0.4444444 9.2.1.2 Odds of being happy \\[ odds_{yes} = \\frac{n_{yes}}{n_{no}} = \\frac{n_{yes}}{n_{total} - n_{yes}} \\] odds &lt;- 52/65 odds [1] 0.8 \\[ odds_{yes} = \\frac{prob_{yes}}{prob_{no}} = \\frac{prob_{yes}}{1 - prob_{yes}} \\] prob/(1 - prob) [1] 0.8 9.2.2 Comparing by Sex Cross-tabulate happiness (satlife) with sex (male vs. female). df_depress %&gt;% dplyr::select(satlife, sex) %&gt;% table() %&gt;% addmargins() sex satlife male female Sum 0 7 58 65 1 14 38 52 Sum 21 96 117 9.2.2.1 Probability of being happy, by sex Reference category = male prob_male &lt;- 14 / 21 prob_male [1] 0.6666667 Comparison Category = female prob_female &lt;- 38 / 96 prob_female [1] 0.3958333 9.2.2.2 Odds of being happy, by sex Reference category = male odds_male &lt;- 14 / 7 odds_male [1] 2 Comparison Category = female odds_female &lt;- 38 / 58 odds_female [1] 0.6551724 9.2.2.3 Odds-Ratio for sex \\[ OR_{\\text{female vs. male}} = \\frac{odds_{female}}{odds_{male}} \\] odds_ratio &lt;- odds_female / odds_male odds_ratio [1] 0.3275862 \\[ OR_{\\text{female vs. male}} = \\frac{\\frac{prob_{female}}{1 - prob_{female}}}{\\frac{prob_{male}}{1 - prob_{male}}} \\] (prob_female / (1 - prob_female)) / (prob_male / (1 - prob_male)) [1] 0.3275862 \\[ OR_{\\text{female vs. male}} = \\frac{\\frac{n_{yes|female}}{n_{no|female}}}{\\frac{n_{yes|male}}{n_{no|male}}} \\] ((38 / 58)/(14 / 7)) [1] 0.3275862 9.3 Logisitc Regression Model 1: one IV 9.3.1 Fit the Unadjusted Model fit_glm_1 &lt;- glm(satlife ~ sex, data = df_depress, family = binomial(link = &quot;logit&quot;)) fit_glm_1 %&gt;% summary() %&gt;% coef() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.6931472 0.4629100 1.497369 0.13429719 sexfemale -1.1160040 0.5077822 -2.197800 0.02796333 summary(fit_glm_1) Call: glm(formula = satlife ~ sex, family = binomial(link = &quot;logit&quot;), data = df_depress) Deviance Residuals: Min 1Q Median 3Q Max -1.482 -1.004 -1.004 1.361 1.361 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.6931 0.4629 1.497 0.134 sexfemale -1.1160 0.5078 -2.198 0.028 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 160.75 on 116 degrees of freedom Residual deviance: 155.62 on 115 degrees of freedom (1 observation deleted due to missingness) AIC: 159.62 Number of Fisher Scoring iterations: 4 9.3.2 Tabulate Parameters 9.3.2.1 Logit Scale texreg::knitreg(fit_glm_1, caption = &quot;Hoffman&#39;s EXAMPLE 3.2 A Loistic Regression Model of Gender and Life Satisfaction, top of page 51&quot;, caption.above = TRUE, single.row = TRUE, digits = 4) Hoffmans EXAMPLE 3.2 A Loistic Regression Model of Gender and Life Satisfaction, top of page 51   Model 1 (Intercept) 0.6931 (0.4629) sexfemale -1.1160 (0.5078)* AIC 159.6205 BIC 165.1449 Log Likelihood -77.8103 Deviance 155.6205 Num. obs. 117 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 9.3.2.2 Both Logit and Odds-ratio Scales texreg::knitreg(list(fit_glm_1, texreghelpr::extract_glm_exp(fit_glm_1)), custom.model.names = c(&quot;b (SE)&quot;, &quot;OR [95 CI]&quot;), caption = &quot;Hoffman&#39;s EXAMPLE 3.2 A Loistic Regression Model of Gender and Life Satisfaction, top of page 51&quot;, caption.above = TRUE, single.row = TRUE, digits = 4, ci.test = 1) Hoffmans EXAMPLE 3.2 A Loistic Regression Model of Gender and Life Satisfaction, top of page 51   b (SE) OR [95 CI] (Intercept) 0.6931 (0.4629) 2.0000 [0.8322; 5.2772] sexfemale -1.1160 (0.5078)* 0.3276 [0.1148; 0.8625]* AIC 159.6205   BIC 165.1449   Log Likelihood -77.8103   Deviance 155.6205   Num. obs. 117   p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 9.3.3 Assess Model Fit 9.3.3.1 Likelihood Ratio Test (LRT, aka. Deviance Difference Test) drop1(fit_glm_1, test = &quot;LRT&quot;) # A tibble: 2 x 5 Df Deviance AIC LRT `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 NA 156. 160. NA NA 2 1 161. 163. 5.13 0.0235 performance::compare_performance(fit_glm_1) # A tibble: 1 x 11 Name Model AIC BIC R2_Tjur RMSE Sigma Log_loss Score_log Score_spherical &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 fit_glm_1 glm 160. 165. 0.0437 0.486 1.16 0.665 -30.1 0.0550 # ... with 1 more variable: PCP &lt;dbl&gt; 9.3.3.2 R-squared like measures performance::r2(fit_glm_1) # R2 for Logistic Regression Tjur&#39;s R2: 0.044 performance::r2_mcfadden(fit_glm_1) # R2 for Generalized Linear Regression R2: 0.032 adj. R2: 0.019 performance::r2_nagelkerke(fit_glm_1) Nagelkerke&#39;s R2 0.05742029 pscl::pR2(fit_glm_1) fitting null model for pseudo-r2 llh llhNull G2 McFadden r2ML r2CU -77.81025523 -80.37450446 5.12849846 0.03190376 0.04288652 0.05742029 9.3.4 Plot Predicted Probabilitites sjPlot::plot_model(model = fit_glm_1, type = &quot;pred&quot;) $sex 9.3.4.1 Logit scale fit_glm_1 %&gt;% emmeans::emmeans(~ sex) sex emmean SE df asymp.LCL asymp.UCL male 0.693 0.463 Inf -0.214 1.6004 female -0.423 0.209 Inf -0.832 -0.0138 Results are given on the logit (not the response) scale. Confidence level used: 0.95 fit_glm_1 %&gt;% emmeans::emmeans(~ sex) %&gt;% pairs() contrast estimate SE df z.ratio p.value male - female 1.12 0.508 Inf 2.198 0.0280 Results are given on the log odds ratio (not the response) scale. 9.3.4.2 Response Scale (probability) fit_glm_1 %&gt;% emmeans::emmeans(~ sex, type = &quot;response&quot;) sex prob SE df asymp.LCL asymp.UCL male 0.667 0.1029 Inf 0.447 0.832 female 0.396 0.0499 Inf 0.303 0.497 Confidence level used: 0.95 Intervals are back-transformed from the logit scale fit_glm_1 %&gt;% emmeans::emmeans(~ sex, type = &quot;response&quot;) %&gt;% pairs() contrast odds.ratio SE df null z.ratio p.value male / female 3.05 1.55 Inf 1 2.198 0.0280 Tests are performed on the log odds ratio scale 9.3.5 Interpretation On average, two out of every three males is depressed, b = 0.667, odds = 1.95, 95% CI [1.58, 2.40]. Females have nearly a quarter lower odds of being depressed, compared to men, b = -0.27, OR = 0.77, 95% IC [0.61, 0.96], p = .028. 9.3.6 Diagnostics 9.3.6.1 Influential values Influential values are extreme individual data points that can alter the quality of the logistic regression model. The most extreme values in the data can be examined by visualizing the Cooks distance values. Here we label the top 7 largest values: plot(fit_glm_1, which = 4, id.n = 7) Note that, not all outliers are influential observations. To check whether the data contains potential influential observations, the standardized residual error can be inspected. Data points with an absolute standardized residuals above 3 represent possible outliers and may deserve closer attention. 9.3.6.2 Standardized Residuals The following R code computes the standardized residuals (.std.resid) using the R function augment() [broom package]. fit_glm_1 %&gt;% broom::augment() %&gt;% ggplot(aes(x = .rownames, .std.resid)) + geom_point(aes(color = sex), alpha = .5) + theme_bw() 9.4 Logisitic Regression Model 2: many IVs 9.4.1 Fit the Model fit_glm_2 &lt;- glm(satlife ~ sex + iq + age + weight, data = df_depress, family = binomial(link = &quot;logit&quot;)) fit_glm_2 %&gt;% summary() %&gt;% coef() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 9.02054236 6.01858970 1.4987801 0.13393069 sexfemale -1.27924381 0.55971456 -2.2855289 0.02228183 iq -0.07279837 0.05361036 -1.3579161 0.17449031 age -0.04112838 0.05250052 -0.7833900 0.43339814 weight -0.03768972 0.08885942 -0.4241499 0.67145648 9.4.2 Tabulate Parameters texreg::knitreg(list(fit_glm_2, texreghelpr::extract_glm_exp(fit_glm_2)), custom.model.names = c(&quot;b (SE)&quot;, &quot;OR [95 CI]&quot;), caption = &quot;EXAMPLE 3.3 A Logistic Regression Model of Life Satisfaction with Multiple Independent Variables, middle of page 52&quot;, caption.above = TRUE, single.row = TRUE, digits = 4, ci.test = 1) EXAMPLE 3.3 A Logistic Regression Model of Life Satisfaction with Multiple Independent Variables, middle of page 52   b (SE) OR [95 CI] (Intercept) 9.0205 (6.0186) 8271.2619 [0.0854; 2077566352.1374] sexfemale -1.2792 (0.5597)* 0.2782 [0.0870; 0.8060]* iq -0.0728 (0.0536) 0.9298 [0.8327; 1.0304] age -0.0411 (0.0525) 0.9597 [0.8635; 1.0627] weight -0.0377 (0.0889) 0.9630 [0.8073; 1.1472] AIC 137.5217   BIC 150.4973   Log Likelihood -63.7609   Deviance 127.5217   Num. obs. 99   p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 9.4.3 Assess Model Fit drop1(fit_glm_2, test = &quot;LRT&quot;) # A tibble: 5 x 5 Df Deviance AIC LRT `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 NA 128. 138. NA NA 2 1 133. 141. 5.59 0.0180 3 1 129. 137. 1.91 0.167 4 1 128. 136. 0.621 0.431 5 1 128. 136. 0.180 0.671 9.4.4 Variance Explained performance::compare_performance(fit_glm_2) # A tibble: 1 x 11 Name Model AIC BIC R2_Tjur RMSE Sigma Log_loss Score_log Score_spherical &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 fit_glm_2 glm 138. 150. 0.0753 0.475 1.16 0.644 -23.1 0.0220 # ... with 1 more variable: PCP &lt;dbl&gt; 9.4.4.1 R-squared lik measures performance::r2_nagelkerke(fit_glm_2) Nagelkerke&#39;s R2 0.09728404 9.4.5 Diagnostics 9.4.5.1 Multicollinearity Multicollinearity corresponds to a situation where the data contain highly correlated predictor variables. Read more in Chapter (ref?)(multicollinearity). Multicollinearity is an important issue in regression analysis and should be fixed by removing the concerned variables. It can be assessed using the R function vif() [car package], which computes the variance inflation factors: car::vif(fit_glm_2) sex iq age weight 1.011608 1.294334 1.418975 1.233364 As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. In our example, there is no collinearity: all variables have a value of VIF well below 5. 9.5 Compare Models 9.5.1 Refit to Complete Cases Restrict the data to only participant that have all four of these predictors. df_depress_model &lt;- df_depress %&gt;% dplyr::filter(complete.cases(sex, iq, age, weight)) Refit Model 1 with only participant complete on all the predictors. fit_glm_1_redo &lt;- glm(satlife ~ sex, data = df_depress_model) fit_glm_2_redo &lt;- glm(satlife ~ sex + iq + age + weight, data = df_depress_model) texreg::knitreg(list(texreghelpr::extract_glm_exp(fit_glm_1_redo), texreghelpr::extract_glm_exp(fit_glm_2_redo)), custom.model.names = c(&quot;Single IV&quot;, &quot;Multiple IVs&quot;), caption.above = TRUE, single.row = TRUE, digits = 4, ci.test = 1) Statistical models   Single IV Multiple IVs (Intercept) 1.9477 [1.5562; 2.4377]* 12.7108 [0.8969; 180.1414] sexfemale 0.7436 [0.5802; 0.9529]* 0.7388 [0.5755; 0.9483]* iq   0.9837 [0.9606; 1.0074] age   0.9908 [0.9677; 1.0144] weight   0.9913 [0.9521; 1.0322] * Null hypothesis value outside the confidence interval. anova(fit_glm_1_redo, fit_glm_2_redo, test = &quot;LRT&quot;) # A tibble: 2 x 5 `Resid. Df` `Resid. Dev` Df Deviance `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 97 22.9 NA NA NA 2 94 22.4 3 0.498 0.554 performance::compare_performance(fit_glm_1_redo, fit_glm_2_redo, rank = TRUE) # A tibble: 2 x 8 Name Model AIC BIC R2 RMSE Sigma Performance_Score &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 fit_glm_1_redo glm 142. 150. 0.0535 0.481 0.486 0.6 2 fit_glm_2_redo glm 146. 161. 0.0741 0.476 0.488 0.4 9.5.2 Interpretation Only sex is predictive of depression. There is no evidence IQ, age, or weight are associated with depression, all ps &gt; .16. "],["logistic-regression---ex-volunteering-hoffman.html", "10 Logistic Regression - Ex: volunteering (Hoffman) 10.1 Exploratory Data Analysis 10.2 Fit Model 10.3 Interpretation", " 10 Logistic Regression - Ex: volunteering (Hoffman) library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(texreghelpr) # GITHUB: sarbearschartz/texreghelpr library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(sjPlot) # Quick plots and tables for models library(pscl) # psudo R-squared function library(glue) # Interpreted String Literals library(interactions) # interaction plots library(sjPlot) # various plots library(performance) # r-squared values This dataset comes from John Hoffmans textbook: Regression Models for Categorical, Count, and Related Variables: An Applied Approach (2004) Amazon link, 2014 edition Chapter 3: Logistic and Probit Regression Models Dataset: The following example uses the SPSS data set gss.sav. The dependent variable of interest is labeled volrelig. \"The variable labeled volrelig, which indicates whether or not a respondent volunteered for a religious organization in the previous year is coded 0 = no, 1 = yes. A hypothesis we wish to explore is that females are more likely than males to volunteer for religious organizations. Hence, in this data set, we code gender as 0 = male and 1 = female. In order to preclude the possibility that age and education explain the proposed association between gender and volrelig, we include these variables in the model after transforming them into z-scores. An advantage of this transformation is that it becomes a simple exercise to compute odds or probabilities for males and females at the mean of age and education, because these variables have now been transformed to have a mean of zero. df_gss &lt;- haven::read_spss(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/Hoffmann_datasets/gss.sav&quot;) %&gt;% haven::as_factor() tibble::glimpse(df_gss) Rows: 2,903 Columns: 20 $ id &lt;dbl&gt; 402, 1473, 1909, 334, 1751, 456, 292, 2817, 2810, 2232, 2174,~ $ marital &lt;fct&gt; divorced, widowed, widowed, widowed, married, divorced, never~ $ divorce &lt;fct&gt; yes, no, no, yes, no, yes, no, yes, no, no, no, no, no, no, y~ $ childs &lt;fct&gt; 2, 0, 7, 2, 2, 0, 2, 3, 0, 2, 2, 5, 0, 2, 1, 0, 1, 0, 0, 0, 0~ $ age &lt;dbl&gt; 54, 24, 75, 41, 37, 40, 36, 33, 18, 35, 35, 34, 40, 37, 41, 6~ $ income &lt;dbl&gt; 10, 2, NA, NA, 12, NA, 9, NA, NA, 6, 12, 11, 12, 10, 12, 12, ~ $ polviews &lt;fct&gt; middle of the road, slight conservative, extreme conservative~ $ fund &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ $ attend &lt;fct&gt; NA, NA, NA, NA, NA, NA, nearly every week, several times a ye~ $ spanking &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ $ totrelig &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 1000, NA, NA, NA, NA, NA, NA, NA,~ $ sei &lt;dbl&gt; 38.9, 29.0, 29.1, 29.0, 38.1, 39.4, 38.4, 31.3, NA, 39.0, 29.~ $ pasei &lt;dbl&gt; NA, 48.6, 22.5, 26.7, 38.1, NA, NA, NA, NA, 50.7, 78.5, NA, 7~ $ volteer &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0~ $ female &lt;fct&gt; female, male, female, female, male, female, female, female, f~ $ nonwhite &lt;fct&gt; non-white, white, non-white, white, white, white, non-white, ~ $ prayer &lt;fct&gt; daily, several times a week, daily, several times a week, sev~ $ educate &lt;dbl&gt; 12, 17, 8, 12, 12, NA, 15, 12, 11, 14, 14, 12, 20, 12, 15, 20~ $ volrelig &lt;fct&gt; no, no, no, yes, yes, no, no, no, no, no, no, no, no, no, no,~ $ polview1 &lt;fct&gt; moderate, conservative, conservative, moderate, conservative,~ psych::headTail(df_gss) # A tibble: 9 x 20 id marital divorce childs age income polviews fund attend spanking &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 402 divorced yes 2 54 10 middle ~ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2 1473 widowed no 0 24 2 slight ~ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3 1909 widowed no 7 75 &lt;NA&gt; extreme~ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4 334 widowed yes 2 41 &lt;NA&gt; middle ~ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 6 375 never married no 0 32 12 liberal libe~ about~ strongl~ 7 399 never married no 1 43 12 slight ~ fund~ about~ agree 8 1640 never married no 0 29 12 liberal fund~ once ~ agree 9 2344 never married no 0 23 5 slight ~ mode~ once ~ strongl~ # ... with 10 more variables: totrelig &lt;chr&gt;, sei &lt;chr&gt;, pasei &lt;chr&gt;, # volteer &lt;chr&gt;, female &lt;fct&gt;, nonwhite &lt;fct&gt;, prayer &lt;fct&gt;, educate &lt;chr&gt;, # volrelig &lt;fct&gt;, polview1 &lt;fct&gt; 10.1 Exploratory Data Analysis 10.1.1 Visualization df_gss %&gt;% ggplot(aes(x = educate, y = volrelig)) + geom_count() + theme_bw() + labs(x = &quot;Education in Years&quot;, y = &quot;Respondent Volunteered for a Religious Organization\\nin the Previous Year&quot;) 10.1.2 Summary Statistics df_gss %&gt;% dplyr::group_by(volrelig) %&gt;% furniture::table1(female, age, educate, total = TRUE, test = TRUE, digits = 3) ------------------------------------------------------------------- volrelig Total no yes P-Value n = 2894 n = 2688 n = 206 female 0.044 male 1283 (44.3%) 1206 (44.9%) 77 (37.4%) female 1611 (55.7%) 1482 (55.1%) 129 (62.6%) age 0.135 44.767 (16.850) 44.656 (17.034) 46.218 (14.192) educate &lt;.001 13.363 (2.928) 13.296 (2.922) 14.238 (2.867) ------------------------------------------------------------------- 10.2 Fit Model df_gss_model &lt;- df_gss %&gt;% dplyr::mutate(volrelig01 = case_when(volrelig == &quot;no&quot; ~ 0, volrelig == &quot;yes&quot; ~ 1)) %&gt;% dplyr::mutate(z_age = (age - 44.767)/16.850) %&gt;% dplyr::mutate(z_educ = (educate - 13.363)/2.928) %&gt;% dplyr::select(id, age, z_age, educate, z_educ, female, volrelig, volrelig01) df_gss_model # A tibble: 2,903 x 8 id age z_age educate z_educ female volrelig volrelig01 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; 1 402 54 0.548 12 -0.466 female no 0 2 1473 24 -1.23 17 1.24 male no 0 3 1909 75 1.79 8 -1.83 female no 0 4 334 41 -0.224 12 -0.466 female yes 1 5 1751 37 -0.461 12 -0.466 male yes 1 6 456 40 -0.283 NA NA female no 0 7 292 36 -0.520 15 0.559 female no 0 8 2817 33 -0.698 12 -0.466 female no 0 9 2810 18 -1.59 11 -0.807 female no 0 10 2232 35 -0.580 14 0.218 male no 0 # ... with 2,893 more rows fit_glm_1 &lt;- glm(volrelig01 ~ female + z_age + z_educ, data = df_gss_model, family = binomial(link = &quot;logit&quot;)) fit_glm_1 %&gt;% summary() Call: glm(formula = volrelig01 ~ female + z_age + z_educ, family = binomial(link = &quot;logit&quot;), data = df_gss_model) Deviance Residuals: Min 1Q Median 3Q Max -0.6949 -0.4138 -0.3621 -0.3142 2.6399 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.8319 0.1215 -23.311 &lt; 2e-16 *** femalefemale 0.3543 0.1503 2.357 0.0184 * z_age 0.1418 0.0739 1.919 0.0549 . z_educ 0.3562 0.0737 4.833 1.34e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 1485.7 on 2893 degrees of freedom Residual deviance: 1456.3 on 2890 degrees of freedom (9 observations deleted due to missingness) AIC: 1464.3 Number of Fisher Scoring iterations: 5 fit_glm_1 %&gt;% summary() %&gt;% coef() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.8318640 0.12148235 -23.310908 3.436457e-120 femalefemale 0.3542592 0.15031927 2.356712 1.843756e-02 z_age 0.1418388 0.07390196 1.919284 5.494843e-02 z_educ 0.3561886 0.07369657 4.833177 1.343714e-06 texreg::knitreg(list(fit_glm_1, texreghelpr::extract_glm_exp(fit_glm_1)), custom.model.names = c(&quot;b (SE)&quot;, &quot;OR [95 CI]&quot;), caption = &quot;Hoffman&#39;s EXAMPLE 3.4 A Logistic Regression Model of Volunteer Work, bottom of page 53&quot;, caption.above = TRUE, single.row = TRUE, digits = 4, ci.test = 1) Hoffmans EXAMPLE 3.4 A Logistic Regression Model of Volunteer Work, bottom of page 53   b (SE) OR [95 CI] (Intercept) -2.8319 (0.1215)*** 0.0589 [0.0460; 0.0742]* femalefemale 0.3543 (0.1503)* 1.4251 [1.0644; 1.9203]* z_age 0.1418 (0.0739) 1.1524 [0.9957; 1.3306] z_educ 0.3562 (0.0737)*** 1.4279 [1.2362; 1.6504]* AIC 1464.2759   BIC 1488.1574   Log Likelihood -728.1379   Deviance 1456.2759   Num. obs. 2894   p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 10.3 Interpretation 10.3.1 Odds-ratio Scale fit_glm_1 %&gt;% coef() (Intercept) femalefemale z_age z_educ -2.8318640 0.3542592 0.1418388 0.3561886 exp(-2.831) [1] 0.05895387 exp(-2.831 + 0.354) [1] 0.08399483 exp(0.354) [1] 1.424755 fit_glm_1 %&gt;% coef() %&gt;% exp() (Intercept) femalefemale z_age z_educ 0.05890295 1.42512448 1.15239090 1.42787675 NOTE: Odds = 1 &gt; There is a 50-50 change of that thing happening for whom ever we are refering to. NOTE: an odds-ratio = 1 &gt; There is the same change of that thing happening for both groups. Controlling for age and education, the odds of volunteering among MALES is exp(-2.831) = .0589, and the odds of volunteering among FEMALES is exp(-2.831 + 0.354) = exp(-2.48) = .0840. Females have 42% higher odds of having volunteered for a religious organization over the previous Year. What, then, is the odds ratio? (0.0840)/(0.0589) [1] 1.426146 10.3.2 Response Scale (aka. Probability) fit_glm_1 %&gt;% emmeans::emmeans(~ female, type = &quot;response&quot;) female prob SE df asymp.LCL asymp.UCL male 0.0556 0.00638 Inf 0.0444 0.0695 female 0.0774 0.00671 Inf 0.0653 0.0917 Confidence level used: 0.95 Intervals are back-transformed from the logit scale Controlling for age and education, the probability of volunteering among MALES is .0556 and the probability of volunteering among FEMALES is .0774. Use these probabilities to compute the odds ratio for gender. (.0774/(1 - .0774))/(.0556/(1 - .0556)) [1] 1.42498 Note that these odds and probabilities are similar. This often occurs when we are dealing with probabilities that are relatively close to zero; in other words, it is a common occurrence for rare events. To see this, simply compute a cross-tabulation of volrelig and gender and compare the odds and probabilities. Then try it out for any rare event you may wish to simulate "],["extensions-of-logistic-regression---ex-canadian-womens-labour-force-participation.html", "11 Extensions of Logistic Regression - Ex: Canadian Womens Labour-Force Participation 11.1 Background 11.2 Exploratory Data Analysis 11.3 Hierarchical (nested) Logistic Regression 11.4 Multinomial (nominal) Logistic Regression 11.5 Proportional-odds (ordinal) Logistic Regression 11.6 Compare Model Fits: Multinomial vs. Ordinal", " 11 Extensions of Logistic Regression - Ex: Canadian Womens Labour-Force Participation library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(sjPlot) # Quick plots and tables for models library(car) # Companion to Applied Regression (a text book - includes datasets) library(MASS) # Support Functions and Datasets library(nnet) # Multinomial Log-Linear Models 11.1 Background The Womenlf data frame has 263 rows and 4 columns. The data are from a 1977 survey of the Canadian population. Dependent variable (DV) or outcome partic Labour-Force Participation, a factor with levels: fulltime Working full-time not.work Not working outside the home parttime Working part-time Indepdentend variables (IV) or predictors hincome Husbands income, in $1000s children Presence of children in the household, a factor with levels: absent no children in the home present at least one child at home region A factor with levels: Atlantic Atlantic Canada BC British Columbia Ontario Prairie Prairie provinces Quebec 11.1.1 Raw Dataset The data is included in the carData package which installs and loads with the car package. data(Womenlf, package = &quot;carData&quot;) # load the internal data tibble::glimpse(Womenlf) # glimpse a bit of the data Rows: 263 Columns: 4 $ partic &lt;fct&gt; not.work, not.work, not.work, not.work, not.work, not.work, n~ $ hincome &lt;int&gt; 15, 13, 45, 23, 19, 7, 15, 7, 15, 23, 23, 13, 9, 9, 45, 15, 5~ $ children &lt;fct&gt; present, present, present, present, present, present, present~ $ region &lt;fct&gt; Ontario, Ontario, Ontario, Ontario, Ontario, Ontario, Ontario~ Womenlf %&gt;% dplyr:: filter(row_number() %in% sample(1:nrow(.), size = 10)) # select a random sample of 10 rows # A tibble: 10 x 4 partic hincome children region &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; 1 not.work 9 present Prairie 2 not.work 45 present Atlantic 3 not.work 9 present Ontario 4 parttime 9 present Ontario 5 parttime 13 present Prairie 6 not.work 28 present Ontario 7 not.work 23 present BC 8 fulltime 9 absent Ontario 9 fulltime 5 absent Quebec 10 not.work 23 absent Quebec Notice the order of the factor levels, especially for the partic factor str(Womenlf) # view the structure of the data &#39;data.frame&#39;: 263 obs. of 4 variables: $ partic : Factor w/ 3 levels &quot;fulltime&quot;,&quot;not.work&quot;,..: 2 2 2 2 2 2 2 1 2 2 ... $ hincome : int 15 13 45 23 19 7 15 7 15 23 ... $ children: Factor w/ 2 levels &quot;absent&quot;,&quot;present&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ region : Factor w/ 5 levels &quot;Atlantic&quot;,&quot;BC&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... We can view the order of the factors levels Womenlf$partic %&gt;% levels() # view the levels (in order) of the variable [1] &quot;fulltime&quot; &quot;not.work&quot; &quot;parttime&quot; 11.1.2 Declare Factors Womenlf_clean &lt;- Womenlf %&gt;% dplyr::mutate(working_ord = partic %&gt;% forcats::fct_recode(&quot;Full Time&quot; = &quot;fulltime&quot;, &quot;Not at All&quot; = &quot;not.work&quot;, &quot;Part Time&quot; = &quot;parttime&quot;) %&gt;% factor(levels = c(&quot;Not at All&quot;, &quot;Part Time&quot;, &quot;Full Time&quot;))) %&gt;% dplyr::mutate(working_any = dplyr::case_when(partic %in% c(&quot;fulltime&quot;, &quot;parttime&quot;) ~ &quot;At Least Part Time&quot;, partic == &quot;not.work&quot; ~ &quot;Not at All&quot;) %&gt;% factor(levels = c(&quot;Not at All&quot;, &quot;At Least Part Time&quot;))) %&gt;% dplyr::mutate(working_full = dplyr::case_when(partic == &quot;fulltime&quot; ~ &quot;Full Time&quot;, partic %in% c(&quot;not.work&quot;, &quot;parttime&quot;) ~ &quot;Less Than Full Time&quot;)%&gt;% factor(levels = c(&quot;Less Than Full Time&quot;, &quot;Full Time&quot;))) %&gt;% dplyr::mutate(working_type = dplyr::case_when(partic == &quot;fulltime&quot; ~ &quot;Full Time&quot;, partic == &quot;parttime&quot; ~ &quot;Part Time&quot;)%&gt;% factor(levels = c(&quot;Part Time&quot;, &quot;Full Time&quot;))) Display the structure of the clean version of the dataset str(Womenlf_clean) # view the structure of the data &#39;data.frame&#39;: 263 obs. of 8 variables: $ partic : Factor w/ 3 levels &quot;fulltime&quot;,&quot;not.work&quot;,..: 2 2 2 2 2 2 2 1 2 2 ... $ hincome : int 15 13 45 23 19 7 15 7 15 23 ... $ children : Factor w/ 2 levels &quot;absent&quot;,&quot;present&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ region : Factor w/ 5 levels &quot;Atlantic&quot;,&quot;BC&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... $ working_ord : Factor w/ 3 levels &quot;Not at All&quot;,&quot;Part Time&quot;,..: 1 1 1 1 1 1 1 3 1 1 ... $ working_any : Factor w/ 2 levels &quot;Not at All&quot;,&quot;At Least Part Time&quot;: 1 1 1 1 1 1 1 2 1 1 ... $ working_full: Factor w/ 2 levels &quot;Less Than Full Time&quot;,..: 1 1 1 1 1 1 1 2 1 1 ... $ working_type: Factor w/ 2 levels &quot;Part Time&quot;,&quot;Full Time&quot;: NA NA NA NA NA NA NA 2 NA NA ... 11.2 Exploratory Data Analysis Three versions of the outcome Womenlf_clean %&gt;% furniture::table1(working_ord, working_any, working_full, working_type, na.rm = FALSE, # do NOT restrict to complete cases!!! output = &quot;markdown&quot;) Mean/Count (SD/%) n = 263 working_ord Not at All 155 (58.9%) Part Time 42 (16%) Full Time 66 (25.1%) NA 0 (0%) working_any Not at All 155 (58.9%) At Least Part Time 108 (41.1%) NA 0 (0%) working_full Less Than Full Time 197 (74.9%) Full Time 66 (25.1%) NA 0 (0%) working_type Part Time 42 (16%) Full Time 66 (25.1%) NA 155 (58.9%) Other Predisctors, univariate Womenlf_clean %&gt;% furniture::table1(&quot;Husband&#39;s Income, $1000&#39;s&quot; = hincome, &quot;Children In the Home&quot; = children, &quot;Region of Canada&quot; = region, output = &quot;markdown&quot;) Mean/Count (SD/%) n = 263 Husbands Income, $1000s 14.8 (7.2) Children In the Home absent 79 (30%) present 184 (70%) Region of Canada Atlantic 30 (11.4%) BC 29 (11%) Ontario 108 (41.1%) Prairie 31 (11.8%) Quebec 65 (24.7%) Womenlf_clean %&gt;% furniture::table1(&quot;Husband&#39;s Income, $1000&#39;s&quot; = hincome, &quot;Children In the Home&quot; = children, &quot;Region of Canada&quot; = region, splitby = ~ working_ord, row_wise = TRUE, # show row %s rather than default column %s test = TRUE, total = TRUE, output = &quot;markdown&quot;) Total Not at All Part Time Full Time P-Value n = 263 n = 155 n = 42 n = 66 Husbands Income, $1000s 0.002 14.8 (7.2) 15.6 (7.2) 16.0 (8.1) 12.1 (6.1) Children In the Home &lt;.001 absent 79 (100%) 26 (32.9%) 7 (8.9%) 46 (58.2%) present 184 (100%) 129 (70.1%) 35 (19%) 20 (10.9%) Region of Canada 0.71 Atlantic 30 (100%) 20 (66.7%) 4 (13.3%) 6 (20%) BC 29 (100%) 14 (48.3%) 8 (27.6%) 7 (24.1%) Ontario 108 (100%) 64 (59.3%) 17 (15.7%) 27 (25%) Prairie 31 (100%) 17 (54.8%) 6 (19.4%) 8 (25.8%) Quebec 65 (100%) 40 (61.5%) 7 (10.8%) 18 (27.7%) 11.2.1 Husbands Income Womenlf_clean %&gt;% ggplot(aes(hincome, fill = working_ord)) + geom_density(alpha = .3) Womenlf_clean %&gt;% ggplot(aes(x = working_ord, y = hincome)) + geom_jitter(position=position_jitter(0.2)) + stat_summary(fun.y = mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y..), width = .75, color = &quot;red&quot;, size = 1) Womenlf_clean %&gt;% ggplot(aes(hincome, x = working_ord, fill = working_ord)) + geom_boxplot(alpha = .3) Womenlf_clean %&gt;% ggplot(aes(hincome, x = working_ord, fill = working_ord)) + geom_violin(alpha = .3) + stat_summary(fun = mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y..), width = .75, color = &quot;red&quot;, size = 1) 11.2.2 Children in Home Womenlf_clean %&gt;% ggplot(aes(x = children, fill = working_ord)) + geom_bar() Womenlf_clean %&gt;% ggplot(aes(x = children, fill = working_ord %&gt;% fct_rev)) + geom_bar(position=&quot;fill&quot;) + labs(x = &quot;Children in the Home&quot;, y = &quot;Proportion of Women&quot;, fill = &quot;Working&quot;) + theme_bw() + scale_fill_manual(values = c(&quot;gray25&quot;, &quot;gray50&quot;, &quot;gray75&quot;)) 11.2.3 Region of Canada Womenlf_clean %&gt;% ggplot(aes(x = region, fill = working_ord)) + geom_bar() Womenlf_clean %&gt;% ggplot(aes(x = region, fill = working_ord %&gt;% fct_rev)) + geom_bar(position=&quot;fill&quot;) + labs(x = &quot;Region of Canada&quot;, y = &quot;Proportion of Women&quot;, fill = &quot;Working&quot;) + theme_bw() + scale_fill_manual(values = c(&quot;gray25&quot;, &quot;gray50&quot;, &quot;gray75&quot;)) 11.3 Hierarchical (nested) Logistic Regression For an \\(m-\\)category polytomy dependent variable is respecified as a series of \\(m  1\\) nested dichotomies. A single or combined levels of outcome compared to another single or combination of levels. Then they are analyzed using a series of binary logistic regressions, such that: Dichotomies selected based on theory Avoid redundancy Similar to contrast coding, but for outcome For this dataset example, the outcome (partic) has \\(3-\\)categories, so we will investigate TWO nested dichotomies outcome = working_any outcome = working_type 11.3.1 Role of Predictors on ANY working Fit a regular logistic model with all three predictors regressed on the binary indicator for any working. Use the glm() function in the base \\(R\\) stats package. fit_glm_1 &lt;- glm(working_any ~ hincome + children + region, data = Womenlf_clean, family = binomial(link = &quot;logit&quot;)) summary(fit_glm_1) Call: glm(formula = working_any ~ hincome + children + region, family = binomial(link = &quot;logit&quot;), data = Womenlf_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.7927 -0.8828 -0.7283 0.9562 2.0074 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.26771 0.55296 2.293 0.0219 * hincome -0.04534 0.02057 -2.204 0.0275 * childrenpresent -1.60434 0.30187 -5.315 1.07e-07 *** regionBC 0.34196 0.58504 0.585 0.5589 regionOntario 0.18778 0.46762 0.402 0.6880 regionPrairie 0.47186 0.55680 0.847 0.3967 regionQuebec -0.17310 0.49957 -0.347 0.7290 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 356.15 on 262 degrees of freedom Residual deviance: 317.30 on 256 degrees of freedom AIC: 331.3 Number of Fisher Scoring iterations: 4 Check if region is statistically significant with the drop1() function from the base \\(R\\) stats package. This may be done with a Likelihood Ratio Test (test = \"LRT\", which is the same as test = \"Chisq\" for glm models). drop1(fit_glm_1, test = &quot;LRT&quot;) # A tibble: 4 x 5 Df Deviance AIC LRT `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 NA 317. 331. NA NA 2 1 322. 334. 5.13 0.0236 3 1 348. 360. 30.5 0.0000000326 4 4 320. 326. 2.43 0.657 Since the region doesnt have exhibit any effect on odds a women is in the labor force, remove that predictor in the model to simplify to a best final model. Also, center husbands income at a value near the mean so the intercept has meaning. fit_glm_2 &lt;- glm(working_any ~ I(hincome - 14) + children, data = Womenlf_clean, family = binomial(link = &quot;logit&quot;)) summary(fit_glm_2) Call: glm(formula = working_any ~ I(hincome - 14) + children, family = binomial(link = &quot;logit&quot;), data = Womenlf_clean) Deviance Residuals: Min 1Q Median 3Q Max -1.6767 -0.8652 -0.7768 0.9292 1.9970 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.74351 0.24312 3.058 0.00223 ** I(hincome - 14) -0.04231 0.01978 -2.139 0.03244 * childrenpresent -1.57565 0.29226 -5.391 7e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 356.15 on 262 degrees of freedom Residual deviance: 319.73 on 260 degrees of freedom AIC: 325.73 Number of Fisher Scoring iterations: 4 The texreg package uses an intermediate function called extract() to extract information for the model and then put it in the right places in the table. We can invervene by writing our own extract_exp() function to use instead. extract_exp &lt;- function(fit_glm){ beta = coef(fit_glm) betaci = confint(fit_glm) fit_glm_exp = texreg::extract(fit_glm) fit_glm_exp@coef = exp(beta) fit_glm_exp@ci.low = exp(betaci[, 1]) fit_glm_exp@ci.up = exp(betaci[, 2]) return(fit_glm_exp) } texreg::knitreg(extract_glm_exp(fit_glm_2), custom.coef.names = c(&quot;BL: No children, Husband Earns $14,000/yr&quot;, &quot;Husband&#39;s Income, $1000&#39;s&quot;, &quot;Children in the Home&quot;), custom.model.names = &quot;OR, Women is in the Workforce at All&quot;, single.row = TRUE, custom.note = &quot;* The value of &#39;1&#39; is outside the confidence interval for the OR&quot;) Statistical models   OR, Women is in the Workforce at All BL: No children, Husband Earns $14,000/yr 2.10 [1.32; 3.44]* Husbands Income, $1000s 0.96 [0.92; 1.00]* Children in the Home 0.21 [0.12; 0.36]* The value of 1 is outside the confidence interval for the OR Interpretation: Among women without children in the home and a husband making $14,000 annually, there is about a 2:1 odds she is in the workforce. For each additional thousand dollars the husband makes, the odds ratio decreases by about 4 percent. If there are children in the home, the odds of being in the workforce is nearly a fifth as large. interactions::interact_plot(model = fit_glm_2, pred = hincome, modx = children, interval = TRUE) interactions::interact_plot(model = fit_glm_2, pred = hincome, modx = children, interval = TRUE, x.label = &quot;Husband&#39;s Income, in $1,000&#39;s&quot;, y.label = &quot;Predicted Probability of\\nWomen Being in the Workforce&quot;, legend.main = &quot;Children in the Home:&quot;, modx.labels = c(&quot;Absent&quot;, &quot;Present&quot;), colors = rep(&quot;black&quot;, 2)) + geom_vline(xintercept = 14, color = &quot;gray25&quot;) + # reference line for intercept theme_bw() + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) effects::allEffects(fit_glm_2) model: working_any ~ I(hincome - 14) + children hincome effect hincome 1 10 20 30 40 0.5476466 0.4527392 0.3514450 0.2619655 0.1886414 children effect children absent present 0.6707323 0.2964731 effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_glm_2) %&gt;% data.frame() %&gt;% ggplot(aes(x = hincome, y = fit, color = children, linetype = children)) + geom_vline(xintercept = 14, color = &quot;gray25&quot;) + # reference line for intercept geom_line(size = 1) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability of\\nWomen Being in the Workforce&quot;, color = &quot;Children in\\nthe Home:&quot;, linetype = &quot;Children in\\nthe Home:&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;)) + coord_cartesian(ylim = c(0, 1)) 11.3.2 Role of Predictors on TYPE of work Fit a regular logistic model with all three predictors regressed on the binary indicator for level/type of working. fit_glm_3 &lt;- glm(working_type ~ hincome + children + region, data = Womenlf_clean, family = binomial(link = &quot;logit&quot;)) summary(fit_glm_3) Call: glm(formula = working_type ~ hincome + children + region, family = binomial(link = &quot;logit&quot;), data = Womenlf_clean) Deviance Residuals: Min 1Q Median 3Q Max -2.5202 -0.8048 0.3583 0.7201 1.9957 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 3.76164 1.05718 3.558 0.000373 *** hincome -0.10475 0.04032 -2.598 0.009383 ** childrenpresent -2.74781 0.56893 -4.830 1.37e-06 *** regionBC -1.18248 1.02764 -1.151 0.249865 regionOntario -0.14876 0.84703 -0.176 0.860589 regionPrairie -0.39173 0.96310 -0.407 0.684200 regionQuebec 0.14842 0.93300 0.159 0.873612 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 144.34 on 107 degrees of freedom Residual deviance: 101.84 on 101 degrees of freedom (155 observations deleted due to missingness) AIC: 115.84 Number of Fisher Scoring iterations: 5 Check if region is statistically significant with the drop1() function from the base \\(R\\) stats package. This may be done with a Likelihood Ratio Test (test = \"LRT\", which is the same as test = \"Chisq\" for glm models). drop1(fit_glm_3, test = &quot;LRT&quot;) # A tibble: 4 x 5 Df Deviance AIC LRT `Pr(&gt;Chi)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 NA 102. 116. NA NA 2 1 110. 122. 7.84 0.00512 3 1 134. 146. 31.9 0.0000000162 4 4 104. 110. 2.65 0.618 Since the region doesnt have exhibit any effect on odds a working women is in the labor force full time, remove that predictor in the model to simplify to a best final model. Also, center husbands income at a value near the mean so the intercept has meaning. fit_glm_4 &lt;- glm(working_type ~ I(hincome - 14) + children, data = Womenlf_clean, family = binomial(link = &quot;logit&quot;)) summary(fit_glm_4) Call: glm(formula = working_type ~ I(hincome - 14) + children, family = binomial(link = &quot;logit&quot;), data = Womenlf_clean) Deviance Residuals: Min 1Q Median 3Q Max -2.4047 -0.8678 0.3949 0.6213 1.7641 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.97602 0.43024 4.593 4.37e-06 *** I(hincome - 14) -0.10727 0.03915 -2.740 0.00615 ** childrenpresent -2.65146 0.54108 -4.900 9.57e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 144.34 on 107 degrees of freedom Residual deviance: 104.49 on 105 degrees of freedom (155 observations deleted due to missingness) AIC: 110.49 Number of Fisher Scoring iterations: 5 The texreg package uses an intermediate function called extract() to extract information for the model and then put it in the right places in the table. We can invervene by writing our own extract_exp() function to use instead. texreg::knitreg(list(extract_glm_exp(fit_glm_2), extract_glm_exp(fit_glm_4)), custom.coef.names = c(&quot;BL: No children, Husband Earns $14,000/yr&quot;, &quot;Husband&#39;s Income, $1000&#39;s&quot;, &quot;Children in the Home&quot;), custom.model.names = c(&quot;Working at All&quot;, &quot;Full vs. Part-Time&quot;), single.row = TRUE, custom.note = &quot;* The value of &#39;1&#39; is outside the confidence interval for the OR&quot;) Statistical models   Working at All Full vs. Part-Time BL: No children, Husband Earns $14,000/yr 2.10 [1.32; 3.44]* 7.21 [3.34; 18.45]* Husbands Income, $1000s 0.96 [0.92; 1.00]* 0.90 [0.83; 0.97]* Children in the Home 0.21 [0.12; 0.36]* 0.07 [0.02; 0.19]* The value of 1 is outside the confidence interval for the OR Interpretation: Among working women without children in the home and a husband making $14,000 annually, there is more than 7:1 odds she is working full time verses part time. For each additional thousand dollars the husband makes, the odds ratio decreases by about 10 percent. If there are children in the home, the odds of being in the workforce is drastically reduced. interactions::interact_plot(model = fit_glm_4, pred = hincome, modx = children, interval = TRUE) interactions::interact_plot(model = fit_glm_4, pred = hincome, modx = children, interval = TRUE, x.label = &quot;Husband&#39;s Income, in $1,000&#39;s&quot;, y.label = &quot;Predicted Probability of\\nWomen Being in the Workforce Full Time\\nif they are working&quot;, legend.main = &quot;Children in the Home:&quot;, modx.labels = c(&quot;Absent&quot;, &quot;Present&quot;), colors = rep(&quot;black&quot;, 2)) + geom_vline(xintercept = 14, color = &quot;gray25&quot;) + # reference line for intercept theme_bw() + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) effects::allEffects(fit_glm_4) model: working_type ~ I(hincome - 14) + children hincome effect hincome 1 10 20 30 40 0.8829045 0.7416987 0.4955346 0.2515165 0.1031024 children effect children absent present 0.8830579 0.3475686 effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_glm_4) %&gt;% data.frame() %&gt;% ggplot(aes(x = hincome, y = fit, color = children, linetype = children)) + geom_vline(xintercept = 14, color = &quot;gray25&quot;) + # reference line for intercept geom_line(size = 1) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability of\\nWomen Being in the Workforce Full Time\\nif they are working&quot;, color = &quot;Children in\\nthe Home:&quot;, linetype = &quot;Children in\\nthe Home:&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;)) + coord_cartesian(ylim = c(0, 1)) 11.4 Multinomial (nominal) Logistic Regression Multinomial Logistic Regression fits a single model by specifing a reference level of the outcome and comparing each additional level to it. In our case we will choose not working as the reference category adn get a set of parameter estimates (betas) for each of the two options part time and full time. 11.4.1 Fit Model 1: main effects only Use multinom() function in the base \\(R\\) nnet package. You will also need the MASS and \\(R\\) package (only to compute MLEs). Make sure to remove cases with missing data on predictors before modeling or use the na.action = na.omit optin in the multinom() model command. fit_multnom_1 &lt;- nnet::multinom(working_ord ~ I(hincome - 14) + children + region, data = Womenlf_clean) # weights: 24 (14 variable) initial value 288.935032 iter 10 value 208.470682 iter 20 value 207.732796 iter 20 value 207.732796 iter 20 value 207.732796 final value 207.732796 converged summary(fit_multnom_1, corr = FALSE, wald = TRUE) Call: nnet::multinom(formula = working_ord ~ I(hincome - 14) + children + region, data = Womenlf_clean) Coefficients: (Intercept) I(hincome - 14) childrenpresent regionBC regionOntario Part Time -1.7521373 0.005261435 0.1462009 1.0863549 0.2856917 Full Time 0.7240428 -0.100034170 -2.6977927 -0.4599247 0.1135573 regionPrairie regionQuebec Part Time 0.5747258 -0.1105184 Full Time 0.4681016 -0.3116829 Std. Errors: (Intercept) I(hincome - 14) childrenpresent regionBC regionOntario Part Time 0.7204798 0.02468887 0.4901621 0.7193077 0.6175050 Full Time 0.6102008 0.02901623 0.3876731 0.7837044 0.6175128 regionPrairie regionQuebec Part Time 0.7259118 0.6873048 Full Time 0.7332449 0.6515172 Residual Deviance: 415.4656 AIC: 443.4656 11.4.2 Fit Model 2: only significant predictors Reduce the model by removing the region variable. fit_multnom_2 &lt;- nnet::multinom(working_ord ~ I(hincome - 14) + children, data = Womenlf_clean) # weights: 12 (6 variable) initial value 288.935032 iter 10 value 211.456740 final value 211.440963 converged summary(fit_multnom_2, corr = FALSE, wald = TRUE) Call: nnet::multinom(formula = working_ord ~ I(hincome - 14) + children, data = Womenlf_clean) Coefficients: (Intercept) I(hincome - 14) childrenpresent Part Time -1.3357927 0.00688932 0.02149927 Full Time 0.6215469 -0.09723492 -2.55867912 Std. Errors: (Intercept) I(hincome - 14) childrenpresent Part Time 0.4340062 0.02345463 0.4690285 Full Time 0.2585136 0.02809670 0.3622077 Residual Deviance: 422.8819 AIC: 434.8819 11.4.3 Compre model fit Check if we need to keep the region variable in our model. anova(fit_multnom_1, fit_multnom_2) # A tibble: 2 x 7 Model `Resid. df` `Resid. Dev` Test ` Df` `LR stat.` `Pr(Chi)` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 I(hincome - 14) ~ 520 423. &quot;&quot; NA NA NA 2 I(hincome - 14) ~ 512 415. &quot;1 vs~ 8 7.42 0.492 performance::compare_performance(fit_multnom_1, fit_multnom_2, rank = TRUE) # A tibble: 2 x 8 Name Model AIC BIC R2_Nagelkerke RMSE Sigma Performance_Score &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 fit_multnom_2 multinom 435. 456. 0.300 0.397 1.28 0.6 2 fit_multnom_1 multinom 443. 493. 0.325 0.392 1.29 0.4 11.4.4 Extract parameters 11.4.4.1 Logit Scale Here is one way to extract the parameter estimates, but recall they are in terms of the logit or log-odds, not probability. broom::tidy(fit_multnom_2) %&gt;% dplyr::mutate(p.value = round(p.value, 4)) # A tibble: 6 x 6 y.level term estimate std.error statistic p.value &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Part Time (Intercept) -1.34 0.434 -3.08 0.0021 2 Part Time I(hincome - 14) 0.00689 0.0235 0.294 0.769 3 Part Time childrenpresent 0.0215 0.469 0.0458 0.963 4 Full Time (Intercept) 0.622 0.259 2.40 0.0162 5 Full Time I(hincome - 14) -0.0972 0.0281 -3.46 0.0005 6 Full Time childrenpresent -2.56 0.362 -7.06 0 11.4.4.2 Odds-Ratio Scale The effects::allEffects() function provides probability estimates for each outcome level for different levels of the predictors. effects::allEffects(fit_multnom_2) model: working_ord ~ I(hincome - 14) + children hincome effect (probability) for Not at All hincome 1 10 20 30 40 0.4265713 0.5819837 0.6888795 0.7333353 0.7439897 hincome effect (probability) for Part Time hincome 1 10 20 30 40 0.1041120 0.1511290 0.1916462 0.2185644 0.2375547 hincome effect (probability) for Full Time hincome 1 10 20 30 40 0.46931674 0.26688731 0.11947430 0.04810031 0.01845552 children effect (probability) for Not at All children absent present 0.3339937 0.7122698 children effect (probability) for Part Time children absent present 0.08828253 0.19236144 children effect (probability) for Full Time children absent present 0.57772376 0.09536878 11.4.5 Tabulate parameters The texreg package know how to handle this type of model and displays the parameters estimates in two separate columns. texreg::extract(fit_multnom_2) coef. s.e. p Part Time: (Intercept) -1.33579274 0.43400624 2.085214e-03 Part Time: I(hincome - 14) 0.00688932 0.02345463 7.689645e-01 Part Time: childrenpresent 0.02149927 0.46902849 9.634395e-01 Full Time: (Intercept) 0.62154688 0.25851357 1.620301e-02 Full Time: I(hincome - 14) -0.09723492 0.02809670 5.387245e-04 Full Time: childrenpresent -2.55867912 0.36220774 1.616364e-12 GOF dec. places AIC 434.8819 TRUE BIC 456.3149 TRUE Log Likelihood -211.4410 TRUE Deviance 422.8819 TRUE Num. obs. 263.0000 FALSE K 3.0000 FALSE texreg::knitreg(fit_multnom_2, custom.model.name = c(&quot;b (SE)&quot;), custom.coef.map = list(&quot;Part Time: (Intercept)&quot; = &quot;PT-BL: No children, Husband Earns $14,000/yr&quot;, &quot;Part Time: I(hincome - 14)&quot; = &quot;PT-Husband&#39;s Income, in $1,000&#39;s&quot;, &quot;Part Time: childrenpresent&quot; = &quot;PT-Children in the Home&quot;, &quot;Full Time: (Intercept)&quot; = &quot;FT-BL: No children, Husband Earns $14,000/yr&quot;, &quot;Full Time: I(hincome - 14)&quot; = &quot;FT-Husband&#39;s Income, in $1,000&#39;s&quot;, &quot;Full Time: childrenpresent&quot; = &quot;FT-Children in the Home&quot;), groups = list(&quot;Part Time&quot; = 1:3, &quot;Full Time&quot; = 4:6), single.row = TRUE) Statistical models   b (SE) Part Time        PT-BL: No children, Husband Earns $14,000/yr -1.34 (0.43)**      PT-Husbands Income, in $1,000s 0.01 (0.02)      PT-Children in the Home 0.02 (0.47) Full Time        FT-BL: No children, Husband Earns $14,000/yr 0.62 (0.26)*      FT-Husbands Income, in $1,000s -0.10 (0.03)***      FT-Children in the Home -2.56 (0.36)*** AIC 434.88 BIC 456.31 Log Likelihood -211.44 Deviance 422.88 Num. obs. 263 K 3 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 fit_multnom_2 %&gt;% coef() (Intercept) I(hincome - 14) childrenpresent Part Time -1.3357927 0.00688932 0.02149927 Full Time 0.6215469 -0.09723492 -2.55867912 fit_multnom_2 %&gt;% coef() %&gt;% exp() (Intercept) I(hincome - 14) childrenpresent Part Time 0.2629496 1.0069131 1.02173205 Full Time 1.8618058 0.9073428 0.07740692 fit_multnom_2 %&gt;% confint() %&gt;% exp() , , Part Time 2.5 % 97.5 % (Intercept) 0.1123171 0.6156011 I(hincome - 14) 0.9616729 1.0542816 childrenpresent 0.4074734 2.5619744 , , Full Time 2.5 % 97.5 % (Intercept) 1.12172715 3.0901640 I(hincome - 14) 0.85872767 0.9587102 childrenpresent 0.03805993 0.1574315 11.4.6 Plot Predicted Probabilities NOTE: Im not sure how to use the interactions::interact_plot() function with multinomial models. 11.4.6.1 Manually Compute estimates for probabilities and the associated 95% confidence intervals effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = c(20, 30, 40)), mod = fit_multnom_2) hincome*children effect (probability) for Not at All children hincome absent present 20 0.4323538 0.7350680 30 0.5929483 0.7516619 40 0.6834699 0.7502613 hincome*children effect (probability) for Part Time children hincome absent present 20 0.1184851 0.2058207 30 0.1740850 0.2254779 40 0.2149730 0.2411093 hincome*children effect (probability) for Full Time children hincome absent present 20 0.4491611 0.059111252 30 0.2329667 0.022860160 40 0.1015571 0.008629454 11.4.6.2 Wrange effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = c(20, 30, 40)), mod = fit_multnom_2) %&gt;% data.frame() # A tibble: 6 x 26 hincome children prob.Not.at.All prob.Part.Time prob.Full.Time &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 20 absent 0.432 0.118 0.449 2 30 absent 0.593 0.174 0.233 3 40 absent 0.683 0.215 0.102 4 20 present 0.735 0.206 0.0591 5 30 present 0.752 0.225 0.0229 6 40 present 0.750 0.241 0.00863 # ... with 21 more variables: logit.Not.at.All &lt;dbl&gt;, logit.Part.Time &lt;dbl&gt;, # logit.Full.Time &lt;dbl&gt;, se.prob.Not.at.All &lt;dbl&gt;, se.prob.Part.Time &lt;dbl&gt;, # se.prob.Full.Time &lt;dbl&gt;, se.logit.Not.at.All &lt;dbl&gt;, # se.logit.Part.Time &lt;dbl&gt;, se.logit.Full.Time &lt;dbl&gt;, # L.prob.Not.at.All &lt;dbl&gt;, L.prob.Part.Time &lt;dbl&gt;, L.prob.Full.Time &lt;dbl&gt;, # U.prob.Not.at.All &lt;dbl&gt;, U.prob.Part.Time &lt;dbl&gt;, U.prob.Full.Time &lt;dbl&gt;, # L.logit.Not.at.All &lt;dbl&gt;, L.logit.Part.Time &lt;dbl&gt;, ... effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = c(20, 30, 40)), mod = fit_multnom_2) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;), starts_with(&quot;L.prob&quot;), starts_with(&quot;U.prob&quot;)) %&gt;% dplyr::rename(est_fit_none = prob.Not.at.All, est_fit_part = prob.Part.Time, est_fit_full = prob.Full.Time, est_lower_none = L.prob.Not.at.All, est_lower_part = L.prob.Part.Time, est_lower_full = L.prob.Full.Time, est_upper_none = U.prob.Not.at.All, est_upper_part = U.prob.Part.Time, est_upper_full = U.prob.Full.Time) # A tibble: 6 x 11 hincome children est_fit_none est_fit_part est_fit_full est_lower_none &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 20 absent 0.432 0.118 0.449 0.306 2 30 absent 0.593 0.174 0.233 0.399 3 40 absent 0.683 0.215 0.102 0.418 4 20 present 0.735 0.206 0.0591 0.655 5 30 present 0.752 0.225 0.0229 0.597 6 40 present 0.750 0.241 0.00863 0.486 # ... with 5 more variables: est_lower_part &lt;dbl&gt;, est_lower_full &lt;dbl&gt;, # est_upper_none &lt;dbl&gt;, est_upper_part &lt;dbl&gt;, est_upper_full &lt;dbl&gt; effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = c(20, 30, 40)), mod = fit_multnom_2) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;), starts_with(&quot;L.prob&quot;), starts_with(&quot;U.prob&quot;)) %&gt;% dplyr::rename(est_fit_none = prob.Not.at.All, est_fit_part = prob.Part.Time, est_fit_full = prob.Full.Time, est_lower_none = L.prob.Not.at.All, est_lower_part = L.prob.Part.Time, est_lower_full = L.prob.Full.Time, est_upper_none = U.prob.Not.at.All, est_upper_part = U.prob.Part.Time, est_upper_full = U.prob.Full.Time) %&gt;% tidyr::pivot_longer(cols = starts_with(&quot;est&quot;), names_to = c(&quot;.value&quot;, &quot;work_level&quot;), names_pattern = &quot;est_(.*)_(.*)&quot;, values_to = &quot;fit&quot;) # A tibble: 18 x 6 hincome children work_level fit lower upper &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 20 absent none 0.432 0.306 0.568 2 20 absent part 0.118 0.0568 0.231 3 20 absent full 0.449 0.318 0.588 4 30 absent none 0.593 0.399 0.762 5 30 absent part 0.174 0.0728 0.361 6 30 absent full 0.233 0.104 0.443 7 40 absent none 0.683 0.418 0.866 8 40 absent part 0.215 0.0692 0.502 9 40 absent full 0.102 0.0256 0.327 10 20 present none 0.735 0.655 0.802 11 20 present part 0.206 0.145 0.283 12 20 present full 0.0591 0.0319 0.107 13 30 present none 0.752 0.597 0.861 14 30 present part 0.225 0.120 0.383 15 30 present full 0.0229 0.00782 0.0649 16 40 present none 0.750 0.486 0.905 17 40 present part 0.241 0.0887 0.509 18 40 present full 0.00863 0.00176 0.0411 11.4.6.3 Plot, version 1 effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_multnom_2) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;), starts_with(&quot;L.prob&quot;), starts_with(&quot;U.prob&quot;)) %&gt;% dplyr::rename(est_fit_none = prob.Not.at.All, est_fit_part = prob.Part.Time, est_fit_full = prob.Full.Time, est_lower_none = L.prob.Not.at.All, est_lower_part = L.prob.Part.Time, est_lower_full = L.prob.Full.Time, est_upper_none = U.prob.Not.at.All, est_upper_part = U.prob.Part.Time, est_upper_full = U.prob.Full.Time) %&gt;% tidyr::pivot_longer(cols = starts_with(&quot;est&quot;), names_to = c(&quot;.value&quot;, &quot;work_level&quot;), names_pattern = &quot;est_(.*)_(.*)&quot;, values_to = &quot;fit&quot;) %&gt;% dplyr::mutate(work_level = work_level %&gt;% factor() %&gt;% forcats::fct_recode(&quot;Not at All&quot; = &quot;none&quot;, &quot;Part Time&quot; = &quot;part&quot;, &quot;Full Time&quot; = &quot;full&quot;) %&gt;% forcats::fct_rev()) %&gt;% ggplot(aes(x = hincome, y = fit)) + geom_vline(xintercept = 14, color = &quot;gray50&quot;) + # reference line for intercept geom_hline(yintercept = 0.5, color = &quot;gray50&quot;) + # 50% chance line for reference geom_ribbon(aes(ymin = lower, ymax = upper, fill = work_level), alpha = .3) + geom_line(aes(color = work_level, linetype = work_level), size = 1) + facet_grid(. ~ children, labeller = label_both) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability&quot;, color = &quot;Woman Works:&quot;, fill = &quot;Woman Works:&quot;, linetype = &quot;Woman Works:&quot;) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) + coord_cartesian(ylim = c(0, 1)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) 11.4.6.4 Plot, version 2 effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_multnom_2) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;), starts_with(&quot;L.prob&quot;), starts_with(&quot;U.prob&quot;)) %&gt;% dplyr::rename(est_fit_none = prob.Not.at.All, est_fit_part = prob.Part.Time, est_fit_full = prob.Full.Time, est_lower_none = L.prob.Not.at.All, est_lower_part = L.prob.Part.Time, est_lower_full = L.prob.Full.Time, est_upper_none = U.prob.Not.at.All, est_upper_part = U.prob.Part.Time, est_upper_full = U.prob.Full.Time) %&gt;% tidyr::pivot_longer(cols = starts_with(&quot;est&quot;), names_to = c(&quot;.value&quot;, &quot;work_level&quot;), names_pattern = &quot;est_(.*)_(.*)&quot;, values_to = &quot;fit&quot;) %&gt;% dplyr::mutate(work_level = factor(work_level, levels = c(&quot;none&quot;, &quot;part&quot;, &quot;full&quot;), labels = c(&quot;Not at All&quot;, &quot;Part Time&quot;, &quot;Full Time&quot;))) %&gt;% ggplot(aes(x = hincome, y = fit)) + geom_vline(xintercept = 14, color = &quot;gray50&quot;) + # reference line for intercept geom_hline(yintercept = 0.5, color = &quot;gray50&quot;) + # 50% chance line for reference geom_ribbon(aes(ymin = lower, ymax = upper, fill = children), alpha = .3) + geom_line(aes(color = children, linetype = children), size = 1) + facet_grid(. ~ work_level) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability&quot;, color = &quot;Children in the Home:&quot;, fill = &quot;Children in the Home:&quot;, linetype = &quot;Children in the Home:&quot;) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) + coord_cartesian(ylim = c(0, 1)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + scale_fill_manual(values = c(&quot;dodgerblue&quot;, &quot;coral3&quot;))+ scale_color_manual(values = c(&quot;dodgerblue&quot;, &quot;coral3&quot;)) 11.4.7 Interpretation Among women without children in the home and a husband making $14,000 annually, there is about 1:4 odds she is working part time verses not at all. and a 1.8:1 odds she is working full time. For each additional thousand dollars the husband makes, the odds ratio decreases by about 10 percent that she is working full time, yet stay the same that she works part time. If there are children in the home, the odds of working part time increase by 2 percent and there is a very unlikely change she works full time. 11.5 Proportional-odds (ordinal) Logistic Regression This type of logisit regression model forces the predictors to have similar relationship with the outcome (slopes), but different means (intercepts). This is called the proportional odds assumption. 11.5.1 Fit the Model Use polr() function in the base \\(R\\) MASS package. While outcome variable (dependent variable, Y) may be a regular factor, it is preferable to specify it as an ordered factor. fit_polr_1 &lt;- MASS::polr(working_ord ~ hincome + children, data = Womenlf_clean) summary(fit_polr_1) Call: MASS::polr(formula = working_ord ~ hincome + children, data = Womenlf_clean) Coefficients: Value Std. Error t value hincome -0.0539 0.01949 -2.766 childrenpresent -1.9720 0.28695 -6.872 Intercepts: Value Std. Error t value Not at All|Part Time -1.8520 0.3863 -4.7943 Part Time|Full Time -0.9409 0.3699 -2.5435 Residual Deviance: 441.663 AIC: 449.663 11.5.2 Extract Parameters 11.5.2.1 Logit Scale fit_polr_1$zeta Not at All|Part Time Part Time|Full Time -1.8520378 -0.9409261 fit_polr_1 %&gt;% confint() 2.5 % 97.5 % hincome -0.09323274 -0.0166527 childrenpresent -2.54479931 -1.4177487 11.5.2.2 Odds-Ratio Scale fit_polr_1 %&gt;% coef() %&gt;% exp() hincome childrenpresent 0.9475262 0.1391841 fit_polr_1 %&gt;% confint() %&gt;% exp() 2.5 % 97.5 % hincome 0.9109815 0.9834852 childrenpresent 0.0784888 0.2422588 11.5.2.3 Predicted Probabilities effects::allEffects(fit_polr_1) model: working_ord ~ hincome + children hincome effect (probability) for Not at All hincome 1 10 20 30 40 0.3968718 0.5166413 0.6469356 0.7585238 0.8433820 hincome effect (probability) for Part Time hincome 1 10 20 30 40 0.22384583 0.21001062 0.17311759 0.12800001 0.08713914 hincome effect (probability) for Full Time hincome 1 10 20 30 40 0.37928237 0.27334810 0.17994676 0.11347618 0.06947888 children effect (probability) for Not at All children absent present 0.2579513 0.7140863 children effect (probability) for Part Time children absent present 0.2057297 0.1472491 children effect (probability) for Full Time children absent present 0.5363190 0.1386646 11.5.3 Tabulate parameters texreg::knitreg(fit_polr_1, custom.model.name = &quot;b (SE)&quot;, custom.coef.map = list(&quot;hincome&quot; = &quot;Husband&#39;s Income, $1000&#39;s&quot;, &quot;childrenpresent&quot; = &quot;Children in the Home&quot;, &quot;Not at All|Part Time&quot; = &quot;Not at All vs.Part Time&quot;, &quot;Part Time|Full Time&quot; = &quot;Part Time vs.Full Time&quot;), groups = list(&quot;Coefficents&quot; = 1:2, &quot;Intercepts&quot; = 3:4), caption = &quot;Proportional-odds (ordinal) Logistic Regression&quot;, caption.above = TRUE, single.row = TRUE) Proportional-odds (ordinal) Logistic Regression   b (SE) Coefficents        Husbands Income, $1000s -0.05 (0.02)**      Children in the Home -1.97 (0.29)*** Intercepts        Not at All vs.Part Time -1.85 (0.39)***      Part Time vs.Full Time -0.94 (0.37)* AIC 449.66 BIC 463.95 Log Likelihood -220.83 Deviance 441.66 Num. obs. 263 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 11.5.4 Plot Predicted Probabilities 11.5.4.1 Manually Compute estimates for probabilities and the associated 95% confidence intervals effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = c(20, 30, 40)), mod = fit_polr_1) hincome*children effect (probability) for Not at All children hincome absent present 20 0.3156092 0.7681570 30 0.4415146 0.8502980 40 0.5754175 0.9068653 hincome*children effect (probability) for Part Time children hincome absent present 20 0.2186091 0.12362226 30 0.2213518 0.08359281 40 0.1957828 0.05347906 hincome*children effect (probability) for Full Time children hincome absent present 20 0.4657817 0.10822076 30 0.3371336 0.06610916 40 0.2287997 0.03965563 11.5.4.2 Wrangle effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = c(20, 30, 40)), mod = fit_polr_1) %&gt;% data.frame() # A tibble: 6 x 26 hincome children prob.Not.at.All prob.Part.Time prob.Full.Time &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 20 absent 0.316 0.219 0.466 2 30 absent 0.442 0.221 0.337 3 40 absent 0.575 0.196 0.229 4 20 present 0.768 0.124 0.108 5 30 present 0.850 0.0836 0.0661 6 40 present 0.907 0.0535 0.0397 # ... with 21 more variables: logit.Not.at.All &lt;dbl&gt;, logit.Part.Time &lt;dbl&gt;, # logit.Full.Time &lt;dbl&gt;, se.prob.Not.at.All &lt;dbl&gt;, se.prob.Part.Time &lt;dbl&gt;, # se.prob.Full.Time &lt;dbl&gt;, se.logit.Not.at.All &lt;dbl&gt;, # se.logit.Part.Time &lt;dbl&gt;, se.logit.Full.Time &lt;dbl&gt;, # L.prob.Not.at.All &lt;dbl&gt;, L.prob.Part.Time &lt;dbl&gt;, L.prob.Full.Time &lt;dbl&gt;, # U.prob.Not.at.All &lt;dbl&gt;, U.prob.Part.Time &lt;dbl&gt;, U.prob.Full.Time &lt;dbl&gt;, # L.logit.Not.at.All &lt;dbl&gt;, L.logit.Part.Time &lt;dbl&gt;, ... effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = c(20, 30, 40)), mod = fit_polr_1) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;), starts_with(&quot;L.prob&quot;), starts_with(&quot;U.prob&quot;)) %&gt;% dplyr::rename(est_fit_none = prob.Not.at.All, est_fit_part = prob.Part.Time, est_fit_full = prob.Full.Time, est_lower_none = L.prob.Not.at.All, est_lower_part = L.prob.Part.Time, est_lower_full = L.prob.Full.Time, est_upper_none = U.prob.Not.at.All, est_upper_part = U.prob.Part.Time, est_upper_full = U.prob.Full.Time) # A tibble: 6 x 11 hincome children est_fit_none est_fit_part est_fit_full est_lower_none &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 20 absent 0.316 0.219 0.466 0.217 2 30 absent 0.442 0.221 0.337 0.274 3 40 absent 0.575 0.196 0.229 0.320 4 20 present 0.768 0.124 0.108 0.692 5 30 present 0.850 0.0836 0.0661 0.741 6 40 present 0.907 0.0535 0.0397 0.775 # ... with 5 more variables: est_lower_part &lt;dbl&gt;, est_lower_full &lt;dbl&gt;, # est_upper_none &lt;dbl&gt;, est_upper_part &lt;dbl&gt;, est_upper_full &lt;dbl&gt; effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = c(20, 30, 40)), mod = fit_polr_1) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;), starts_with(&quot;L.prob&quot;), starts_with(&quot;U.prob&quot;)) %&gt;% dplyr::rename(est_fit_none = prob.Not.at.All, est_fit_part = prob.Part.Time, est_fit_full = prob.Full.Time, est_lower_none = L.prob.Not.at.All, est_lower_part = L.prob.Part.Time, est_lower_full = L.prob.Full.Time, est_upper_none = U.prob.Not.at.All, est_upper_part = U.prob.Part.Time, est_upper_full = U.prob.Full.Time) %&gt;% tidyr::pivot_longer(cols = starts_with(&quot;est&quot;), names_to = c(&quot;.value&quot;, &quot;work_level&quot;), names_pattern = &quot;est_(.*)_(.*)&quot;, values_to = &quot;fit&quot;) # A tibble: 18 x 6 hincome children work_level fit lower upper &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 20 absent none 0.316 0.217 0.435 2 20 absent part 0.219 0.163 0.286 3 20 absent full 0.466 0.346 0.589 4 30 absent none 0.442 0.274 0.623 5 30 absent part 0.221 0.165 0.291 6 30 absent full 0.337 0.195 0.516 7 40 absent none 0.575 0.320 0.796 8 40 absent part 0.196 0.122 0.299 9 40 absent full 0.229 0.0926 0.463 10 20 present none 0.768 0.692 0.830 11 20 present part 0.124 0.0875 0.172 12 20 present full 0.108 0.0716 0.160 13 30 present none 0.850 0.741 0.919 14 30 present part 0.0836 0.0467 0.145 15 30 present full 0.0661 0.0327 0.129 16 40 present none 0.907 0.775 0.965 17 40 present part 0.0535 0.0210 0.129 18 40 present full 0.0397 0.0138 0.109 11.5.4.3 Plot, version 1 effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_polr_1) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;), starts_with(&quot;L.prob&quot;), starts_with(&quot;U.prob&quot;)) %&gt;% dplyr::rename(est_fit_none = prob.Not.at.All, est_fit_part = prob.Part.Time, est_fit_full = prob.Full.Time, est_lower_none = L.prob.Not.at.All, est_lower_part = L.prob.Part.Time, est_lower_full = L.prob.Full.Time, est_upper_none = U.prob.Not.at.All, est_upper_part = U.prob.Part.Time, est_upper_full = U.prob.Full.Time) %&gt;% tidyr::pivot_longer(cols = starts_with(&quot;est&quot;), names_to = c(&quot;.value&quot;, &quot;work_level&quot;), names_pattern = &quot;est_(.*)_(.*)&quot;, values_to = &quot;fit&quot;) %&gt;% dplyr::mutate(work_level = work_level %&gt;% factor() %&gt;% forcats::fct_recode(&quot;Not at All&quot; = &quot;none&quot;, &quot;Part Time&quot; = &quot;part&quot;, &quot;Full Time&quot; = &quot;full&quot;) %&gt;% forcats::fct_rev()) %&gt;% ggplot(aes(x = hincome, y = fit)) + geom_vline(xintercept = 14, color = &quot;gray50&quot;) + # reference line for intercept geom_hline(yintercept = 0.5, color = &quot;gray50&quot;) + # 50% chance line for reference geom_ribbon(aes(ymin = lower, ymax = upper, fill = work_level), alpha = .3) + geom_line(aes(color = work_level, linetype = work_level), size = 1) + facet_grid(. ~ children, labeller = label_both) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability&quot;, color = &quot;Woman Works:&quot;, fill = &quot;Woman Works:&quot;, linetype = &quot;Woman Works:&quot;) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) + coord_cartesian(ylim = c(0, 1)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) 11.5.4.4 Plot, version 2 effects::Effect(focal.predictors = c(&quot;hincome&quot;, &quot;children&quot;), xlevels = list(hincome = seq(from = 1, to = 45, by = .1)), mod = fit_polr_1) %&gt;% data.frame() %&gt;% dplyr::select(hincome, children, starts_with(&quot;prob&quot;), starts_with(&quot;L.prob&quot;), starts_with(&quot;U.prob&quot;)) %&gt;% dplyr::rename(est_fit_none = prob.Not.at.All, est_fit_part = prob.Part.Time, est_fit_full = prob.Full.Time, est_lower_none = L.prob.Not.at.All, est_lower_part = L.prob.Part.Time, est_lower_full = L.prob.Full.Time, est_upper_none = U.prob.Not.at.All, est_upper_part = U.prob.Part.Time, est_upper_full = U.prob.Full.Time) %&gt;% tidyr::pivot_longer(cols = starts_with(&quot;est&quot;), names_to = c(&quot;.value&quot;, &quot;work_level&quot;), names_pattern = &quot;est_(.*)_(.*)&quot;, values_to = &quot;fit&quot;) %&gt;% dplyr::mutate(work_level = factor(work_level, levels = c(&quot;none&quot;, &quot;part&quot;, &quot;full&quot;), labels = c(&quot;Not at All&quot;, &quot;Part Time&quot;, &quot;Full Time&quot;))) %&gt;% ggplot(aes(x = hincome, y = fit)) + geom_vline(xintercept = 14, color = &quot;gray50&quot;) + # reference line for intercept geom_hline(yintercept = 0.5, color = &quot;gray50&quot;) + # 50% chance line for reference geom_ribbon(aes(ymin = lower, ymax = upper, fill = children), alpha = .3) + geom_line(aes(color = children, linetype = children), size = 1) + facet_grid(. ~ work_level) + theme_bw() + labs(x = &quot;Husband&#39;s Income, in $1000&#39;s&quot;, y = &quot;Predicted Probability&quot;, color = &quot;Children in the Home:&quot;, fill = &quot;Children in the Home:&quot;, linetype = &quot;Children in the Home:&quot;) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) + coord_cartesian(ylim = c(0, 1)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + scale_fill_manual(values = c(&quot;dodgerblue&quot;, &quot;coral3&quot;))+ scale_color_manual(values = c(&quot;dodgerblue&quot;, &quot;coral3&quot;)) 11.5.5 Interpretation Among women without children in the home and a husband making $14,000 annually, there is a 26% chance she is not working, 21% change she is working part time and just over a 53% change she is working full time. For each additional thousand dollars the husband makes, the odds ratio decreases by about 5 percent that she is working part time vs not at all and 5% that she is working full time vs part time. If there are children in the home, the odds ratio of working part time vs not at all decreases by 86% and similartly the odds ratio fo working full time vs part time also decreases by 86%. 11.6 Compare Model Fits: Multinomial vs. Ordinal The multinomail and proportional-odds models arent truely nested, so you can NOT conduct a Likelihood-Ratio Test (aka Deviance Difference Test) with the anova() command. You can use the performance::compare_performance() command to compare overal model performance via the Bayes factor (BF). performance::compare_performance(fit_multnom_2, fit_polr_1, rank = TRUE) # A tibble: 2 x 8 Name Model AIC BIC R2_Nagelkerke RMSE Sigma Performance_Score &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 fit_multnom_2 multinom 435. 456. 0.300 0.397 1.28 1 2 fit_polr_1 polr 450. 464. 0.236 1.60 1.30 0 11.6.1 Interpretation The multinomial model looks to fit these data better than the proportional-odds (aka ordinal) logisic model. "],["ordered-logistic-regression---ex-spaking.html", "12 Ordered Logistic Regression - Ex: Spaking 12.1 Background 12.2 Exploratory Data Analysis 12.3 Linear Regression 12.4 Ordered Logistic Regression 12.5 Proportional-odds (ordinal) Logistic Regression 12.6 Hoffmanns Example 4.8 (continuesapproximated)", " 12 Ordered Logistic Regression - Ex: Spaking library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(sjPlot) # Quick plots and tables for models library(car) # Companion to Applied Regression (a text book - includes datasets) library(MASS) # Support Functions and Datasets library(nnet) # Multinomial Log-Linear Models library(pscl) # Political Science Computational Laboratory (ZIP) 12.1 Background This dataset comes from John Hoffmans textbook: Regression Models for Categorical, Count, and Related Variables: An Applied Approach (2004) Amazon link, 2014 edition Chapter 4: Ordered Logistic and Probit Regression Models Dataset: The following example uses the SPSS data set gss.sav. The dependent variable of interest is labeled spanking. \" The pertinent question (sapnking) asks Do you strongly agree, agree, disagree, or strongly disagree that it is sometimes necessary to discipline a child with a good, hard spanking? The possible answers are coded as 1 = strongly agree, 2 = agree, 3 = disagree, and 4 = strongly disagree. A common hypothesis is that support for corporal punishment of children decreases at higher levels of education.\" 12.1.1 Raw Dataset data_gss &lt;- haven::read_spss(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/Hoffmann_datasets/gss.sav&quot;) %&gt;% haven::as_factor() data_gss %&gt;% dplyr::select(spanking, female, nonwhite, educate, income) %&gt;% dplyr::filter(!is.na(spanking)) %&gt;% # about 1/3 of participants are missing this head() # A tibble: 6 x 5 spanking female nonwhite educate income &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 agree male white 16 12 2 agree female white 11 2 3 disagree male white 15 12 4 disagree male white 14 NA 5 agree female non-white 16 12 6 agree male white 12 NA 12.1.2 Wrangle Data data_gss_model &lt;- data_gss %&gt;% dplyr::mutate(spankingN = as.numeric(spanking)) %&gt;% # numeric version: 1, 2, 3, 4 dplyr::mutate(polviewsN = as.numeric(polviews)) %&gt;% dplyr::filter(complete.cases(educate, spanking)) # only include complete cases 12.2 Exploratory Data Analysis 12.2.1 Entire Sample data_gss %&gt;% furniture::table1(spanking, na.rm = FALSE, output = &quot;markdown&quot;, caption = &quot;Hoffmann&#39;s Example 4.1 Summary of the Spanking Variable&quot;) Table 12.1: Hoffmanns Example 4.1 Summary of the Spanking Variable Mean/Count (SD/%) n = 2903 spanking strongly agree 512 (17.6%) agree 890 (30.7%) disagree 357 (12.3%) strongly disagree 164 (5.6%) NA 980 (33.8%) data_gss %&gt;% ggplot(aes(spanking)) + geom_bar() 12.2.2 By Education data_gss %&gt;% dplyr::group_by(forcats::fct_explicit_na(spanking)) %&gt;% furniture::table1(&quot;Educations, years&quot; = educate, &quot;Education, factor&quot; = factor(educate), na.rm = FALSE, digits = 2, output = &quot;markdown&quot;) strongly agree agree disagree strongly disagree (Missing) n = 512 n = 890 n = 357 n = 164 n = 980 Educations, years 12.64 (2.96) 13.40 (2.84) 14.00 (2.74) 14.24 (3.00) 13.32 (2.95) Education, factor 0 0 (0%) 2 (0.2%) 0 (0%) 0 (0%) 2 (0.2%) 3 5 (1%) 0 (0%) 0 (0%) 0 (0%) 3 (0.3%) 4 1 (0.2%) 0 (0%) 0 (0%) 1 (0.6%) 4 (0.4%) 5 4 (0.8%) 5 (0.6%) 0 (0%) 1 (0.6%) 3 (0.3%) 6 5 (1%) 5 (0.6%) 1 (0.3%) 0 (0%) 3 (0.3%) 7 7 (1.4%) 8 (0.9%) 0 (0%) 1 (0.6%) 7 (0.7%) 8 25 (4.9%) 15 (1.7%) 5 (1.4%) 2 (1.2%) 33 (3.4%) 9 15 (2.9%) 20 (2.2%) 7 (2%) 4 (2.4%) 23 (2.3%) 10 27 (5.3%) 44 (4.9%) 11 (3.1%) 4 (2.4%) 35 (3.6%) 11 39 (7.6%) 42 (4.7%) 16 (4.5%) 5 (3%) 53 (5.4%) 12 152 (29.7%) 270 (30.3%) 97 (27.2%) 33 (20.1%) 297 (30.3%) 13 56 (10.9%) 90 (10.1%) 41 (11.5%) 19 (11.6%) 90 (9.2%) 14 56 (10.9%) 111 (12.5%) 41 (11.5%) 19 (11.6%) 112 (11.4%) 15 19 (3.7%) 47 (5.3%) 14 (3.9%) 14 (8.5%) 59 (6%) 16 59 (11.5%) 105 (11.8%) 68 (19%) 31 (18.9%) 129 (13.2%) 17 13 (2.5%) 45 (5.1%) 14 (3.9%) 6 (3.7%) 42 (4.3%) 18 17 (3.3%) 43 (4.8%) 15 (4.2%) 11 (6.7%) 39 (4%) 19 3 (0.6%) 15 (1.7%) 8 (2.2%) 2 (1.2%) 13 (1.3%) 20 8 (1.6%) 20 (2.2%) 18 (5%) 11 (6.7%) 29 (3%) NA 1 (0.2%) 3 (0.3%) 1 (0.3%) 0 (0%) 4 (0.4%) 12.2.3 Spanking by Sex data_gss %&gt;% dplyr::filter(complete.cases(female, spanking)) %&gt;% dplyr::select(female, spanking) %&gt;% table() %&gt;% addmargins() spanking female strongly agree agree disagree strongly disagree Sum male 243 388 156 56 843 female 269 502 201 108 1080 Sum 512 890 357 164 1923 data_gss %&gt;% dplyr::filter(complete.cases(female, spanking)) %&gt;% furniture::tableX(female, spanking, type = &quot;count&quot;) spanking female strongly agree agree disagree strongly disagree Total male 243 388 156 56 843 female 269 502 201 108 1080 Total 512 890 357 164 1923 data_gss %&gt;% dplyr::filter(complete.cases(female, spanking)) %&gt;% furniture::tableX(female, spanking, type = &quot;row_perc&quot;) spanking female strongly agree agree disagree strongly disagree Total male 28.83 46.03 18.51 6.64 100.00 female 24.91 46.48 18.61 10.00 100.00 All 26.63 46.28 18.56 8.53 100.00 data_gss %&gt;% dplyr::filter(complete.cases(female, spanking)) %&gt;% furniture::tableX(female, spanking, type = &quot;col_perc&quot;) spanking female strongly agree agree disagree strongly disagree All male 47.46 43.60 43.70 34.15 43.84 female 52.54 56.40 56.30 65.85 56.16 Total 100.00 100.00 100.00 100.00 100.00 data_gss %&gt;% dplyr::filter(complete.cases(female, spanking)) %&gt;% furniture::tableX(female, spanking, type = &quot;cell_perc&quot;) spanking female strongly agree agree disagree strongly disagree Total male 12.64 20.18 8.11 2.91 43.84 female 13.99 26.11 10.45 5.62 56.16 Total 26.63 46.28 18.56 8.53 100.00 data_gss %&gt;% dplyr::filter(complete.cases(female, spanking)) %&gt;% dplyr::group_by(spanking) %&gt;% furniture::table1(female) -------------------------------------------------------------------- spanking strongly agree agree disagree strongly disagree n = 512 n = 890 n = 357 n = 164 female male 243 (47.5%) 388 (43.6%) 156 (43.7%) 56 (34.1%) female 269 (52.5%) 502 (56.4%) 201 (56.3%) 108 (65.9%) -------------------------------------------------------------------- 12.3 Linear Regression Linear regression is often ill-suited to fitting a likert rating, such as agreement. 12.3.1 Visualization data_gss_model %&gt;% ggplot(aes(x = educate, y = spankingN)) + geom_count() + # point size relative to over-plotting geom_smooth(method = &quot;lm&quot;) + # add linear regression line (OLS) theme_bw() + labs(x = &quot;Years of Formal Education&quot;, y = &quot;Spanking&quot;) Figure 12.1: Hoffmanns Figure 4.1 12.3.2 Fit the Model fit_lm &lt;- lm(spankingN ~ educate, data = data_gss_model) summary(fit_lm) Call: lm(formula = spankingN ~ educate, data = data_gss_model) Residuals: Min 1Q Median 3Q Max -1.44768 -0.79947 -0.06955 0.66036 2.41660 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.367326 0.093670 14.597 &lt; 2e-16 *** educate 0.054018 0.006839 7.898 4.73e-15 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.8729 on 1916 degrees of freedom Multiple R-squared: 0.03153, Adjusted R-squared: 0.03103 F-statistic: 62.38 on 1 and 1916 DF, p-value: 4.73e-15 anova(fit_lm) # A tibble: 2 x 5 Df `Sum Sq` `Mean Sq` `F value` `Pr(&gt;F)` &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 47.5 47.5 62.4 4.73e-15 2 1916 1460. 0.762 NA NA 12.3.3 Tabulate Parameters texreg::knitreg(fit_lm, custom.model.name = &quot;Linear Regression&quot;, caption = &quot;Hoffmann&#39;s Example 4.2&quot;, caption.above = TRUE, single.row = TRUE, digits = 4) Hoffmanns Example 4.2   Linear Regression (Intercept) 1.3673 (0.0937)*** educate 0.0540 (0.0068)*** R2 0.0315 Adj. R2 0.0310 Num. obs. 1918 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 12.3.4 Model Fit and Variance Explained performance::performance(fit_lm) # A tibble: 1 x 6 AIC BIC R2 R2_adjusted RMSE Sigma &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4926. 4942. 0.0315 0.0310 0.872 0.873 performance::r2(fit_lm) # R2 for Linear Regression R2: 0.032 adj. R2: 0.031 12.3.5 Residual Diagnostics sjPlot::plot_model(fit_lm, type = &quot;diag&quot;) [[1]] Figure 12.2: Hoffmans Figures 4.2 adn 4.3 Residual Diagnostics for a linear model on likery dependent variable - YUCK! [[2]] Figure 12.3: Hoffmans Figures 4.2 adn 4.3 Residual Diagnostics for a linear model on likery dependent variable - YUCK! [[3]] Figure 12.4: Hoffmans Figures 4.2 adn 4.3 Residual Diagnostics for a linear model on likery dependent variable - YUCK! 12.4 Ordered Logistic Regression data_gss_model %&gt;% dplyr::group_by(forcats::fct_explicit_na(spanking)) %&gt;% furniture::table1(&quot;Sex&quot; = female, caption = &quot;Hoffmann&#39;s Example 4.3 Crosstabulate DV with Sex&quot;, na.rm = FALSE, digits = 2, total = TRUE, output = &quot;markdown&quot;) Table 12.2: Hoffmanns Example 4.3 Crosstabulate DV with Sex Total strongly agree agree disagree strongly disagree n = 1918 n = 511 n = 887 n = 356 n = 164 Sex male 841 (43.8%) 243 (47.6%) 387 (43.6%) 155 (43.5%) 56 (34.1%) female 1077 (56.2%) 268 (52.4%) 500 (56.4%) 201 (56.5%) 108 (65.9%) NA 0 (0%) 0 (0%) 0 (0%) 0 (0%) 0 (0%) data_gss_model %&gt;% furniture::tableX(female, spanking) spanking female strongly agree agree disagree strongly disagree Total male 243 387 155 56 841 female 268 500 201 108 1077 Total 511 887 356 164 1918 12.5 Proportional-odds (ordinal) Logistic Regression This type of logisit regression model forces the predictors to have similar relationship with the outcome (slopes), but different means (intercepts). This is called the proportional odds assumption. 12.5.1 Fit Model 1: Sex Use polr() function in the base \\(R\\) MASS package. While outcome variable (dependent variable, Y) may be a regular factor, it is preferable to specify it as an ordered factor. fit_polr_1 &lt;- MASS::polr(spanking ~ female, data = data_gss_model) summary(fit_polr_1) Call: MASS::polr(formula = spanking ~ female, data = data_gss_model) Coefficients: Value Std. Error t value femalefemale 0.2116 0.08532 2.48 Intercepts: Value Std. Error t value strongly agree|agree -0.8967 0.0694 -12.9114 agree|disagree 1.1094 0.0711 15.6078 disagree|strongly disagree 2.4922 0.0958 26.0026 Residual Deviance: 4719.394 AIC: 4727.394 12.5.2 Extract Parameters 12.5.2.1 Logit Scale fit_polr_1$zeta strongly agree|agree agree|disagree -0.8966862 1.1093754 disagree|strongly disagree 2.4921855 fit_polr_1 %&gt;% coef() femalefemale 0.2116244 fit_polr_1 %&gt;% confint() 2.5 % 97.5 % 0.04451894 0.37901780 12.5.2.2 Odds-Ratio Scale fit_polr_1$zeta %&gt;% exp() strongly agree|agree agree|disagree 0.4079192 3.0324638 disagree|strongly disagree 12.0876653 fit_polr_1 %&gt;% coef() %&gt;% exp() femalefemale 1.235684 fit_polr_1 %&gt;% confint() %&gt;% exp() 2.5 % 97.5 % 1.045525 1.460849 12.5.2.3 Predicted Probabilities effects::allEffects(fit_polr_1) model: spanking ~ female female effect (probability) for strongly agree female male female 0.289732 0.248186 female effect (probability) for agree female male female 0.4622807 0.4623011 female effect (probability) for disagree female male female 0.1715795 0.1967672 female effect (probability) for strongly disagree female male female 0.07640782 0.09274572 12.5.3 Tabulate parameters texreg::knitreg(fit_polr_1, custom.model.name = c(&quot;b (SE)&quot;), custom.coef.map = list(&quot;femalefemale&quot; = &quot;Female vs. Male&quot;, &quot;strongly agree|agree&quot; = &quot;strongly agree|agree&quot;, &quot;agree|disagree&quot; = &quot;agree|disagree&quot;, &quot;disagree|strongly disagree&quot; = &quot;disagree|strongly disagree&quot;), groups = list(&quot;Predictors&quot; = 1, &quot;Cut Values (i.e. threasholds)&quot; = 2:4), caption = &quot;Hoffmann&#39;s Example 4.4 Ordered Logistic Regression&quot;, caption.above = TRUE, single.row = TRUE, digits = 4) Hoffmanns Example 4.4 Ordered Logistic Regression   b (SE) Predictors        Female vs. Male 0.2116 (0.0853)* Cut Values (i.e. threasholds)        strongly agree|agree -0.8967 (0.0694)***      agree|disagree 1.1094 (0.0711)***      disagree|strongly disagree 2.4922 (0.0958)*** AIC 4727.3944 BIC 4749.6306 Log Likelihood -2359.6972 Deviance 4719.3944 Num. obs. 1918 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 12.5.4 Predicted Probabilities ggeffects::ggeffect(model = fit_polr_1, terms = c(&quot;female&quot;)) # A tibble: 8 x 7 x response.level predicted std.error conf.low conf.high group &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 male strongly.agree 0.290 0.0694 0.263 0.319 1 2 female strongly.agree 0.248 0.0647 0.225 0.273 1 3 male agree 0.462 0.0460 0.440 0.485 1 4 female agree 0.462 0.0459 0.440 0.485 1 5 male disagree 0.172 0.0708 0.153 0.192 1 6 female disagree 0.197 0.0653 0.177 0.218 1 7 male strongly.disagree 0.0764 0.0958 0.0642 0.0908 1 8 female strongly.disagree 0.0927 0.0889 0.0791 0.108 1 ggeffects::ggeffect(model = fit_polr_1, terms = c(&quot;female&quot;)) %&gt;% dplyr::filter(x == &quot;female&quot;) # A tibble: 4 x 7 x response.level predicted std.error conf.low conf.high group &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 female strongly.agree 0.248 0.0647 0.225 0.273 1 2 female agree 0.462 0.0459 0.440 0.485 1 3 female disagree 0.197 0.0653 0.177 0.218 1 4 female strongly.disagree 0.0927 0.0889 0.0791 0.108 1 12.5.5 Plot Predicted Probabilities ggeffects::ggeffect(model = fit_polr_1, terms = c(&quot;female&quot;)) %&gt;% # x-axis data.frame() %&gt;% ggplot(aes(x = x, y = predicted, group = response.level, color = response.level)) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .25) + geom_point(size = 4) + geom_line(aes(linetype = response.level)) ggeffects::ggeffect(model = fit_polr_1, terms = c(&quot;female&quot;)) %&gt;% # x-axis data.frame() %&gt;% dplyr::mutate(response.level = response.level %&gt;% forcats::fct_reorder(predicted) %&gt;% forcats::fct_rev()) %&gt;% ggplot(aes(x = x, y = predicted, group = response.level, color = response.level)) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .25) + geom_point(size = 4) + geom_line(aes(linetype = response.level)) + theme_bw() + labs(x = NULL, y = &quot;Predicted Probability&quot;, color = &quot;Spanking:&quot;, shape = &quot;Spanking:&quot;, linetype = &quot;Spanking:&quot;) + theme(legend.key.width = unit(2, &quot;cm&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotdash&quot;, &quot;dotted&quot;)) + scale_shape_manual(values = c(0, 1, 2, 8)) 12.5.6 Model Fit and Variance Explained fit_polr_0 &lt;- MASS::polr(spanking ~ 1, data = data_gss_model) anova(fit_polr_1, fit_polr_0) # A tibble: 2 x 7 Model `Resid. df` `Resid. Dev` Test ` Df` `LR stat.` `Pr(Chi)` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 1915 4726. &quot;&quot; NA NA NA 2 female 1914 4719. &quot;1 vs 2&quot; 1 6.16 0.0130 performance::performance(fit_polr_1) Can&#39;t calculate log-loss. Can&#39;t calculate proper scoring rules for ordinal, multinomial or cumulative link models. # A tibble: 1 x 5 AIC BIC R2_Nagelkerke RMSE Sigma &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4727. 4750. 0.00351 2.05 1.57 performance::r2(fit_polr_1) Nagelkerke&#39;s R2: 0.004 12.5.7 Assumptions 12.5.7.1 Proportional Odds: Brant Test The poTest function implements tests proposed by Brant (1990) for proportional odds for logistic models fit by the polr() function in the MASS package. # Hoffmann&#39;s Examle 4.5 (continued...) car::poTest(fit_polr_1) Tests for Proportional Odds MASS::polr(formula = spanking ~ female, data = data_gss_model) b[polr] b[&gt;strongly agree] b[&gt;agree] b[&gt;disagree] Chisquare df Overall 3.01 2 femalefemale 0.212 0.204 0.183 0.446 3.01 2 Pr(&gt;Chisq) Overall 0.22 femalefemale 0.22 A significant test statistics provides evidence that the parallel regression assumption has been violated! 12.5.8 Fit Model 2: Sex + Covars fit_polr_2 &lt;- MASS::polr(spanking ~ female + educate + polviewsN, data = data_gss_model) summary(fit_polr_2) Call: MASS::polr(formula = spanking ~ female + educate + polviewsN, data = data_gss_model) Coefficients: Value Std. Error t value femalefemale 0.2532 0.08825 2.869 educate 0.1153 0.01564 7.374 polviewsN -0.2215 0.03248 -6.818 Intercepts: Value Std. Error t value strongly agree|agree -0.2977 0.2671 -1.1146 agree|disagree 1.7845 0.2706 6.5935 disagree|strongly disagree 3.1926 0.2793 11.4312 Residual Deviance: 4396.504 AIC: 4408.504 (97 observations deleted due to missingness) 12.5.9 Extract Parameters 12.5.9.1 Logit Scale fit_polr_2$zeta strongly agree|agree agree|disagree -0.2976843 1.7844863 disagree|strongly disagree 3.1926342 fit_polr_2 %&gt;% coef() femalefemale educate polviewsN 0.2532132 0.1152980 -0.2214577 fit_polr_2 %&gt;% confint() 2.5 % 97.5 % femalefemale 0.08039420 0.4263963 educate 0.08472724 0.1460403 polviewsN -0.28526298 -0.1579046 12.5.9.2 Odds-Ratio Scale fit_polr_2$zeta %&gt;% exp() strongly agree|agree agree|disagree 0.7425358 5.9565193 disagree|strongly disagree 24.3524926 fit_polr_2 %&gt;% coef() %&gt;% exp() femalefemale educate polviewsN 1.2881578 1.1222079 0.8013498 fit_polr_2 %&gt;% confint() %&gt;% exp() 2.5 % 97.5 % femalefemale 1.0837142 1.5317277 educate 1.0884202 1.1572429 polviewsN 0.7518165 0.8539312 12.5.10 Tabulate parameters texreg::knitreg(fit_polr_2, custom.model.name = c(&quot;b (SE)&quot;), custom.coef.map = list(&quot;femalefemale&quot; = &quot;Female vs. Male&quot;, &quot;educate&quot; = &quot;Years of Education&quot;, &quot;polviewsN&quot; = &quot;Level of Polytical Views&quot;, &quot;strongly agree|agree&quot; = &quot;strongly agree|agree&quot;, &quot;agree|disagree&quot; = &quot;agree|disagree&quot;, &quot;disagree|strongly disagree&quot; = &quot;disagree|strongly disagree&quot;), groups = list(&quot;Predictors&quot; = 1:3, &quot;Cut Values&quot; = 4:6), caption = &quot;Hoffmann&#39;s Example 4.7 Ordered Logistic Regression&quot;, caption.above = TRUE, single.row = TRUE, digits = 4) Hoffmanns Example 4.7 Ordered Logistic Regression   b (SE) Predictors        Female vs. Male 0.2532 (0.0883)**      Years of Education 0.1153 (0.0156)***      Level of Polytical Views -0.2215 (0.0325)*** Cut Values        strongly agree|agree -0.2977 (0.2671)      agree|disagree 1.7845 (0.2706)***      disagree|strongly disagree 3.1926 (0.2793)*** AIC 4408.5038 BIC 4441.5466 Log Likelihood -2198.2519 Deviance 4396.5038 Num. obs. 1821 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 12.5.11 Predicted Probabilities The ggeffects package computes estimated marginal means (predicted values) for the response, at the margin of specific values or levels from certain model terms, i.e. it generates predictions by a model by holding the non-focal variables constant and varying the focal variable(s). ggpredict() uses predict() for generating predictions factors: uses the reference level ggeffect() computes marginal effects by internally calling effects::Effect() factors: compute a kind of average value, which represents the proportions of each factors category ggemmeans() uses emmeans::emmeans() factors: compute a kind of average value, which represents the proportions of each factors category Use condition to set a specific level for factors in ggemmeans(), so factors are not averaged over their categories, but held constant at a given level. ggeffects::ggpredict() Adjusted for: * educate = 13.51 The grand mean value * polviewsN = 4.17 The grand mean value ## Hoffmann&#39;s Example 4.8 (continues...approximated) ggeffects::ggpredict(model = fit_polr_2, terms = c(&quot;female&quot;)) # A tibble: 8 x 7 x predicted std.error conf.low conf.high response.level group &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; 1 male 0.283 0.253 0.193 0.393 strongly agree 1 2 male 0.477 0.253 0.357 0.600 agree 1 3 male 0.169 0.253 0.110 0.250 disagree 1 4 male 0.0718 0.253 0.0450 0.113 strongly disagree 1 5 female 0.234 0.276 0.151 0.344 strongly agree 1 6 female 0.476 0.276 0.346 0.610 agree 1 7 female 0.199 0.276 0.126 0.299 disagree 1 8 female 0.0906 0.276 0.0549 0.146 strongly disagree 1 ggeffects::ggpredict(model = fit_polr_2, terms = c(&quot;female&quot;)) %&gt;% data.frame() # A tibble: 8 x 7 x predicted std.error conf.low conf.high response.level group &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; 1 male 0.283 0.253 0.193 0.393 strongly agree 1 2 male 0.477 0.253 0.357 0.600 agree 1 3 male 0.169 0.253 0.110 0.250 disagree 1 4 male 0.0718 0.253 0.0450 0.113 strongly disagree 1 5 female 0.234 0.276 0.151 0.344 strongly agree 1 6 female 0.476 0.276 0.346 0.610 agree 1 7 female 0.199 0.276 0.126 0.299 disagree 1 8 female 0.0906 0.276 0.0549 0.146 strongly disagree 1 12.6 Hoffmanns Example 4.8 (continuesapproximated) ggeffects::ggpredict() Adjusted for: * female = male The reference category * polviewsN = 4.17 The grand mean value ggeffects::ggpredict(model = fit_polr_2, terms = c(&quot;educate [10, 16]&quot;, # 1st = x &quot;female&quot;)) %&gt;% # 2nd = group data.frame() %&gt;% dplyr::filter(group == &quot;male&quot;) # A tibble: 8 x 7 x predicted std.error conf.low conf.high response.level group &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; 1 10 0.371 0.209 0.282 0.471 strongly agree male 2 10 0.454 0.209 0.356 0.556 agree male 3 10 0.125 0.209 0.0868 0.177 disagree male 4 10 0.0491 0.209 0.0331 0.0721 strongly disagree male 5 16 0.228 0.287 0.144 0.342 strongly agree male 6 16 0.475 0.287 0.340 0.614 agree male 7 16 0.203 0.287 0.127 0.309 disagree male 8 16 0.0935 0.287 0.0555 0.153 strongly disagree male ggeffects::ggeffect() Adjusted for: * female computed a kind of average value, which represents the proportions of male/female * polviewsN = 4.17 The grand mean value ggeffects::ggeffect(model = fit_polr_2, terms = c(&quot;educate [10, 16]&quot;)) %&gt;% data.frame() # A tibble: 8 x 7 x response.level predicted std.error conf.low conf.high group &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 10 strongly.agree 0.339 0.0744 0.307 0.372 1 2 16 strongly.agree 0.204 0.0692 0.183 0.227 1 3 10 agree 0.465 0.0498 0.441 0.490 1 4 16 agree 0.469 0.0487 0.445 0.493 1 5 10 disagree 0.139 0.0793 0.122 0.159 1 6 16 disagree 0.221 0.0663 0.199 0.244 1 7 10 strongly.disagree 0.0561 0.105 0.0461 0.0682 1 8 16 strongly.disagree 0.106 0.0885 0.0908 0.124 1 ggeffects::ggemmeans(model = fit_polr_2, terms = c(&quot;educate [10, 16]&quot;), condition = c(female = &quot;female&quot;)) %&gt;% data.frame() # A tibble: 8 x 7 x predicted std.error conf.low conf.high response.level group &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; 1 10 0.314 0.0178 0.279 0.349 strongly agree 1 2 10 0.472 0.0124 0.448 0.496 agree 1 3 10 0.151 0.0107 0.130 0.172 disagree 1 4 10 0.0624 0.00641 0.0498 0.0749 strongly disagree 1 5 16 0.187 0.0124 0.162 0.211 strongly agree 1 6 16 0.461 0.0125 0.437 0.486 agree 1 7 16 0.235 0.0130 0.209 0.260 disagree 1 8 16 0.117 0.0100 0.0976 0.137 strongly disagree 1 Predictions for specific values: females with 10 or 16 years education ggeffects::ggeffect(model = fit_polr_2, terms = c(&quot;female&quot;, # 1st var = `x` &quot;educate [10, 16]&quot;)) %&gt;% # 2nd var = `group` data.frame() # A tibble: 16 x 7 x group response.level predicted std.error conf.low conf.high &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 male 10 strongly.agree 0.371 0.0909 0.331 0.414 2 female 10 strongly.agree 0.314 0.0825 0.281 0.350 3 male 16 strongly.agree 0.228 0.0818 0.201 0.258 4 female 16 strongly.agree 0.187 0.0820 0.164 0.212 5 male 10 agree 0.454 0.0544 0.428 0.481 6 female 10 agree 0.472 0.0496 0.448 0.496 7 male 16 agree 0.475 0.0489 0.451 0.499 8 female 16 agree 0.461 0.0504 0.437 0.486 9 male 10 disagree 0.125 0.0937 0.106 0.147 10 female 10 disagree 0.151 0.0834 0.132 0.174 11 male 16 disagree 0.203 0.0756 0.180 0.228 12 female 16 disagree 0.235 0.0721 0.210 0.261 13 male 10 strongly.disagree 0.0491 0.120 0.0392 0.0613 14 female 10 strongly.disagree 0.0624 0.110 0.0509 0.0762 15 male 16 strongly.disagree 0.0935 0.101 0.0780 0.112 16 female 16 strongly.disagree 0.117 0.0968 0.0990 0.138 ggeffects::ggemmeans(model = fit_polr_2, terms = &quot;female&quot;, condition = c(educate = 12, polviewsN = 4.5)) # A tibble: 8 x 7 x predicted std.error conf.low conf.high response.level group &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; 1 male 0.335 0.0169 0.302 0.368 strongly agree 1 2 male 0.467 0.0124 0.442 0.491 agree 1 3 male 0.141 0.00969 0.122 0.160 disagree 1 4 male 0.0570 0.00571 0.0458 0.0682 strongly disagree 1 5 female 0.281 0.0141 0.254 0.309 strongly agree 1 6 female 0.477 0.0121 0.453 0.501 agree 1 7 female 0.169 0.0101 0.149 0.189 disagree 1 8 female 0.0723 0.00653 0.0595 0.0851 strongly disagree 1 12.6.1 Plot Predicted Probabilites ggeffects::ggeffect(model = fit_polr_2, terms = c(&quot;educate [10, 16]&quot;, # x-axis &quot;female&quot;)) %&gt;% # lines by group data.frame() %&gt;% ggplot(aes(x = x, y = predicted, color = group, shape = group)) + geom_point(size = 4) + geom_line(aes(linetype = group)) + facet_wrap(~ response.level) ggeffects::ggeffect(model = fit_polr_2, terms = c(&quot;educate [10, 16]&quot;, # x-axis &quot;female&quot;)) %&gt;% # lines by group data.frame() %&gt;% dplyr::filter(response.level == &quot;strongly.agree&quot;) %&gt;% ggplot(aes(x = x, y = predicted, color = group)) + geom_point(size = 4) + geom_line(aes(linetype = group)) ggeffects::ggeffect(model = fit_polr_2, terms = c(&quot;educate [10, 16]&quot;, # x-axis &quot;female&quot;)) %&gt;% # lines by group data.frame() %&gt;% dplyr::filter(response.level == &quot;strongly.agree&quot;) %&gt;% ggplot(aes(x = x, y = predicted, color = group, shape = group)) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .5, position = position_dodge(width =.25)) + geom_point(size = 4, position = position_dodge(width =.25)) + geom_line(aes(linetype = group), position = position_dodge(width =.25)) + theme_bw() + labs(x = &quot;Education, years&quot;, y = &quot;Predicted Probability for Strongly Agree&quot;, color = NULL, shape = NULL, linetype = NULL) ggeffects::ggeffect(model = fit_polr_2, terms = c(&quot;educate [10, 16]&quot;, # x-axis &quot;female&quot;)) %&gt;% # lines by group data.frame() %&gt;% dplyr::mutate(group = forcats::fct_rev(group)) %&gt;% dplyr::filter(response.level == &quot;strongly.agree&quot;) %&gt;% ggplot(aes(x = x, y = predicted, shape = group)) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .25, position = position_dodge(.2)) + geom_point(size = 4, position = position_dodge(.2)) + geom_line(aes(linetype = group), size = 1, position = position_dodge(.2)) + theme_bw() + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;black&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + labs(x = &quot;Years of Formal Education&quot;, y = &quot;Predicted Probabilit for\\nResponding &#39;Strongly Agree&#39;&quot;, color = NULL, shape = NULL, linetype = NULL, title = &quot;Adjusted Predictions: Strongly Agree Spanking is Appropriate&quot;) Figure 12.5: Hoffmanns Figure 4.4 ggeffects::ggeffect(model = fit_polr_2, terms = c(&quot;female&quot;)) %&gt;% # lines by group data.frame() %&gt;% dplyr::filter(response.level %in% c(&quot;strongly.agree&quot;, &quot;strongly.disagree&quot;)) %&gt;% dplyr::mutate(resonse.level = factor(response.level)) %&gt;% ggplot(aes(x = x, y = predicted, fill = resonse.level)) + geom_col(position = position_dodge()) ggeffects::ggeffect(model = fit_polr_2, terms = c(&quot;female&quot;)) %&gt;% # lines by group data.frame() %&gt;% dplyr::filter(response.level %in% c(&quot;strongly.agree&quot;, &quot;strongly.disagree&quot;)) %&gt;% dplyr::mutate(resonse.level = factor(response.level)) %&gt;% ggplot(aes(x = forcats::fct_rev(x), y = predicted, fill = resonse.level)) + geom_col(position = position_dodge()) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + scale_fill_manual(values = c(&quot;gray30&quot;, &quot;gray70&quot;)) + labs(x = NULL, y = &quot;Predicted Probability of Response&quot;, fill = NULL, title = &quot;Attitues towareds Spanking, by Sex&quot;) Figure 12.6: Hoffmanns Figure 4.5 12.6.2 Model Fit and Variance Explained fit_polr_1redeo &lt;- MASS::polr(spanking ~ female, data = data_gss_model %&gt;% dplyr::filter(complete.cases(educate, polviewsN))) anova(fit_polr_2, fit_polr_1redeo) # A tibble: 2 x 7 Model `Resid. df` `Resid. Dev` Test ` Df` `LR stat.` `Pr(Chi)` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 female 1817 4501. &quot;&quot; NA NA NA 2 female + educate~ 1815 4397. &quot;1 vs~ 2 105. 0 performance::compare_performance(fit_polr_2, fit_polr_1redeo, rank = TRUE) # A tibble: 2 x 8 Name Model AIC BIC R2_Nagelkerke RMSE Sigma Performance_Score &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 fit_polr_2 polr 4409. 4442. 0.179 2.06 1.56 0.8 2 fit_polr_1redeo polr 4509. 4531. 0.00384 2.06 1.57 0.2 12.6.3 Assumptions 12.6.3.1 Proportional Odds: Brant Test The poTest function implements tests proposed by Brant (1990) for proportional odds for logistic models fit by the polr() function in the MASS package. # Hoffmann&#39;s Example 4.8 car::poTest(fit_polr_2) Tests for Proportional Odds MASS::polr(formula = spanking ~ female + educate + polviewsN, data = data_gss_model) b[polr] b[&gt;strongly agree] b[&gt;agree] b[&gt;disagree] Chisquare df Overall 8.80 6 femalefemale 0.2532 0.2342 0.2180 0.4719 2.64 2 educate 0.1153 0.1248 0.1036 0.0927 1.17 2 polviewsN -0.2215 -0.1622 -0.2558 -0.2872 4.85 2 Pr(&gt;Chisq) Overall 0.185 femalefemale 0.268 educate 0.558 polviewsN 0.089 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A significant test statistics provides evidence that the parallel regression assumption has been violated! "],["count-outcome-regression---ex.html", "13 Count Outcome Regression - Ex: 13.1 Background 13.2 Exploratory Data Analysis 13.3 Poisson Reression 13.4 Negative Binomial Regression 13.5 Zero Inflated Poisson 13.6 Zero Inflated Negative Binomial", " 13 Count Outcome Regression - Ex: library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(sjPlot) # Quick plots and tables for models library(glue) # Interpreted String Literals library(DescTools) # Tools for Descriptive Statistics library(texreghelpr) # Helper Functions for generalized models library(pscl) # Political Science Computational Laboratory (ZIP) 13.1 Background This dataset comes from John Hoffmans textbook: Regression Models for Categorical, Count, and Related Variables: An Applied Approach (2004) Amazon link, 2014 edition 13.1.1 Raw Dataset data_gss &lt;- haven::read_spss(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/Hoffmann_datasets/gss.sav&quot;) %&gt;% haven::as_factor() data_gss %&gt;% dplyr::select(volteer, female, nonwhite, educate, income) %&gt;% head() # A tibble: 6 x 5 volteer female nonwhite educate income &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 female non-white 12 10 2 0 male white 17 2 3 0 female non-white 8 NA 4 1 female white 12 NA 5 1 male white 12 12 6 0 female white NA NA 13.2 Exploratory Data Analysis 13.2.1 Entire Sample data_gss %&gt;% furniture::tableF(volteer) -------------------------------------- volteer Freq CumFreq Percent CumPerc 0 2376 2376 81.85% 81.85% 1 286 2662 9.85% 91.70% 2 133 2795 4.58% 96.28% 3 64 2859 2.20% 98.48% 4 19 2878 0.65% 99.14% 5 11 2889 0.38% 99.52% 6 7 2896 0.24% 99.76% 7 6 2902 0.21% 99.97% 9 1 2903 0.03% 100.00% -------------------------------------- data_gss %&gt;% dplyr::select(volteer) %&gt;% summary() volteer Min. :0.0000 1st Qu.:0.0000 Median :0.0000 Mean :0.3334 3rd Qu.:0.0000 Max. :9.0000 data_gss %&gt;% ggplot(aes(volteer)) + geom_histogram() 13.2.2 By Sex data_gss %&gt;% dplyr::group_by(female) %&gt;% furniture::table1(factor(volteer), digits = 4, total = TRUE) -------------------------------------------------------- female Total male female n = 2903 n = 1285 n = 1618 factor(volteer) 0 2376 (81.8%) 1057 (82.3%) 1319 (81.5%) 1 286 (9.9%) 132 (10.3%) 154 (9.5%) 2 133 (4.6%) 50 (3.9%) 83 (5.1%) 3 64 (2.2%) 26 (2%) 38 (2.3%) 4 19 (0.7%) 10 (0.8%) 9 (0.6%) 5 11 (0.4%) 4 (0.3%) 7 (0.4%) 6 7 (0.2%) 5 (0.4%) 2 (0.1%) 7 6 (0.2%) 1 (0.1%) 5 (0.3%) 9 1 (0%) 0 (0%) 1 (0.1%) -------------------------------------------------------- data_gss %&gt;% dplyr::group_by(female) %&gt;% furniture::table1(volteer, digits = 4, total = TRUE, test = TRUE) ----------------------------------------------------------------- female Total male female P-Value n = 2903 n = 1285 n = 1618 volteer 0.365 0.3334 (0.8858) 0.3167 (0.8493) 0.3467 (0.9139) ----------------------------------------------------------------- data_gss %&gt;% ggplot(aes(volteer, fill = female)) + geom_histogram(position = &quot;dodge&quot;) 13.3 Poisson Reression 13.3.1 Single Predictor: Sex 13.3.1.1 Fit the model glm_possion_1 &lt;- glm(volteer ~ female, data = data_gss, family = poisson(link = &quot;log&quot;)) summary(glm_possion_1) Call: glm(formula = volteer ~ female, family = poisson(link = &quot;log&quot;), data = data_gss) Deviance Residuals: Min 1Q Median 3Q Max -0.8327 -0.8327 -0.7959 -0.7959 6.4273 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.14970 0.04957 -23.19 &lt;2e-16 *** femalefemale 0.09048 0.06511 1.39 0.165 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 3658.1 on 2902 degrees of freedom Residual deviance: 3656.2 on 2901 degrees of freedom AIC: 4924.1 Number of Fisher Scoring iterations: 6 Note: The deviance residuals range as high as 6.47!!! 13.3.1.2 Marginal Estimates Note: Results are given on the log (not the response) scale glm_possion_1 %&gt;% emmeans::emmeans(~ female) female emmean SE df asymp.LCL asymp.UCL male -1.15 0.0496 Inf -1.25 -1.053 female -1.06 0.0422 Inf -1.14 -0.976 Results are given on the log (not the response) scale. Confidence level used: 0.95 Note: These means are on the original scale (number of volunteer activities in the past year). These standard errors are called delta-method standard errors # Hoffmann Example 6.4 (continued...) ggeffects::ggpredict(model = glm_possion_1, terms = c(&quot;female&quot;)) %&gt;% data.frame() # A tibble: 2 x 6 x predicted std.error conf.low conf.high group &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 male 0.317 0.0496 0.287 0.349 1 2 female 0.347 0.0422 0.319 0.377 1 13.3.1.3 Pairwise Post Hoc Test glm_possion_1 %&gt;% emmeans::emmeans(~ female) %&gt;% pairs() contrast estimate SE df z.ratio p.value male - female -0.0905 0.0651 Inf -1.390 0.1647 Results are given on the log (not the response) scale. 13.3.1.4 Parameter Estimates Coefficients are in terms of the LOG of the number of times a person volunteers per year. glm_possion_1 %&gt;% coef() (Intercept) femalefemale -1.14970081 0.09047562 Exponentiating the coefficients (betas) returns the values to the original scale (number of times a person volunteers per year) and is refered to as the incident rate ratio IRR. glm_possion_1 %&gt;% coef() %&gt;% exp() (Intercept) femalefemale 0.3167315 1.0946948 # Hoffmann Example 6.4 texreg::knitreg(list(glm_possion_1, texreghelpr::extract_glm_exp(glm_possion_1, include.aic = FALSE, include.bic = FALSE, include.loglik = FALSE, include.deviance = FALSE, include.nobs = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;RR [95% CI]&quot;), custom.coef.map = list(&quot;(Intercept)&quot; =&quot;Intercept&quot;, femalefemale = &quot;Female vs. Male&quot;), caption = &quot;GLM: Simple Possion Regression&quot;, single.row = TRUE, digits = 3) GLM: Simple Possion Regression   b (SE) RR [95% CI] Intercept -1.150 (0.050)*** 0.317 [0.287; 0.349]* Female vs. Male 0.090 (0.065) 1.095 [0.964; 1.244]* AIC 4924.135   BIC 4936.082   Log Likelihood -2460.068   Deviance 3656.198   Num. obs. 2903   p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 13.3.2 Multiple Predictors 13.3.2.1 Fit the model # Hoffmann Example 6.5 glm_possion_2 &lt;- glm(volteer ~ female + nonwhite + educate + income, data = data_gss, family = poisson(link = &quot;log&quot;)) summary(glm_possion_2) Call: glm(formula = volteer ~ female + nonwhite + educate + income, family = poisson(link = &quot;log&quot;), data = data_gss) Deviance Residuals: Min 1Q Median 3Q Max -1.3061 -0.8864 -0.7597 -0.6064 5.8489 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.15830 0.24479 -12.902 &lt; 2e-16 *** femalefemale 0.26132 0.07785 3.357 0.000789 *** nonwhitenon-white -0.28038 0.10838 -2.587 0.009681 ** educate 0.10280 0.01443 7.123 1.05e-12 *** income 0.05683 0.01566 3.628 0.000286 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 2566.8 on 1943 degrees of freedom Residual deviance: 2465.5 on 1939 degrees of freedom (959 observations deleted due to missingness) AIC: 3380.9 Number of Fisher Scoring iterations: 6 13.3.2.2 Parameter Estimates Coefficients are in terms of the LOG of the number of times a person volunteers per year. glm_possion_2 %&gt;% coef() (Intercept) femalefemale nonwhitenon-white educate -3.15830174 0.26132182 -0.28037733 0.10280183 income 0.05682778 Exponentiating the coefficients (betas) returns the values to the original scale (number of times a person volunteers per year) and is refered to as the incident rate ratio IRR. glm_possion_2 %&gt;% coef() %&gt;% exp() (Intercept) femalefemale nonwhitenon-white educate 0.04249785 1.29864553 0.75549861 1.10827176 income 1.05847350 texreg::knitreg(list(glm_possion_2, texreghelpr::extract_glm_exp(glm_possion_2, include.aic = FALSE, include.bic = FALSE, include.loglik = FALSE, include.deviance = FALSE, include.nobs = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;IRR [95% CI]&quot;), custom.coef.map = list(&quot;(Intercept)&quot; =&quot;Intercept&quot;, femalefemale = &quot;Female vs. Male&quot;, &quot;nonwhitenon-white&quot; = &quot;Non-white vs. White&quot;, educate = &quot;Education, Years&quot;, income = &quot;Income&quot;), caption = &quot;GLM: Multiple Possion Regression&quot;, single.row = TRUE, digits = 3) GLM: Multiple Possion Regression   b (SE) IRR [95% CI] Intercept -3.158 (0.245)*** 0.042 [0.026; 0.068]* Female vs. Male 0.261 (0.078)*** 1.299 [1.115; 1.513]* Non-white vs. White -0.280 (0.108)** 0.755 [0.608; 0.930]* Education, Years 0.103 (0.014)*** 1.108 [1.077; 1.140]* Income 0.057 (0.016)*** 1.058 [1.027; 1.092]* AIC 3380.860   BIC 3408.722   Log Likelihood -1685.430   Deviance 2465.514   Num. obs. 1944   p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 13.3.2.3 Interpretation female: Adjusting for the effects of rate/ethnicity, education, and income, FEMALES are expected to volunteer about 30% MORE activities per year than males, exp(b) = 1.29, p = .001. nonwhite: Adjusting for the effects of sex, education, and income, NON-WHITES are expected to volunteer for about 24% LESS activities per year than males, exp(b) = 0.76, p = .010. educate: Each one-year increase in education is associated with an 11% increase in the number of volunteer activities per year, adjusting for the effects of sex, race/ethnicity, and income, exp(b) = 1.11, p &lt;.001. 13.3.2.4 Predictions Note: These means are on the original scale (number of volunteer activities in the past year). These standard errors are called delta-method standard errors in Stata, but they are not calculated in R. ggeffects::ggemmeans(model = glm_possion_2, terms = c(&quot;female&quot;), condition = c(nonwhite = &quot;white&quot;, educate = 12, income = 5)) # A tibble: 2 x 6 x predicted std.error conf.low conf.high group &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 male 0.194 0.109 0.157 0.240 1 2 female 0.252 0.0952 0.209 0.303 1 Interpretation: The expected number of volunteer activities among females is 31.5% higher ((0.25 - 0.19)/0.19) than among males, for white high school graduates with low income. Note: income = 5 is the 10th percentile of the income distribution. Alternative: * 0.25/0.19 = 1.315 * (1.315 - 1)x100% = 31.5% 13.3.2.5 Assess Model Fit DescTools::PseudoR2(glm_possion_2) McFadden 0.02918402 DescTools::PseudoR2(glm_possion_2, which = &quot;all&quot;) %&gt;% round(3) McFadden McFaddenAdj CoxSnell Nagelkerke AldrichNelson 0.029 0.026 0.051 0.061 0.050 VeallZimmermann Efron McKelveyZavoina Tjur AIC 0.077 0.020 NA NA 3380.860 BIC logLik logLik0 G2 3408.722 -1685.430 -1736.096 101.333 13.3.2.6 Residual Diagnostics par(mfrow = c(2, 2)) plot(glm_possion_2) par(mfrow = c(1, 1)) These residuals do NOT look good, especially the Q-Q plot for normality. 13.3.2.7 Marginal Plot data_gss %&gt;% dplyr::select(educate, income) %&gt;% psych::describe(skew = FALSE) # A tibble: 2 x 8 vars n mean sd min max range se &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 2894 13.4 2.93 0 20 20 0.0544 2 2 1947 9.86 2.99 1 12 11 0.0677 data_gss %&gt;% dplyr::select(educate, income) %&gt;% summary() educate income Min. : 0.00 Min. : 1.000 1st Qu.:12.00 1st Qu.: 9.000 Median :13.00 Median :11.000 Mean :13.36 Mean : 9.862 3rd Qu.:16.00 3rd Qu.:12.000 Max. :20.00 Max. :12.000 NA&#39;s :9 NA&#39;s :956 ggeffects::ggemmeans(model = glm_possion_2, terms = &quot;educate&quot;, condition = c(female = &quot;male&quot;, nonwhite = &quot;white&quot;)) %&gt;% data.frame %&gt;% ggplot(aes(x = x, y = predicted)) + geom_line() + labs(x = &quot;Years of Formal Education&quot;, y = &quot;Predicted Number of Volunteer Activities&quot;) Figure 13.1: Hoffmanns Figure 6.5 effects::Effect(focal.predictors = c(&quot;female&quot;, &quot;nonwhite&quot;, &quot;educate&quot;, &quot;income&quot;), mod = glm_possion_2, xlevels = list(educate = seq(from = 0, to = 20, by = .1), income = c(8, 10, 12))) %&gt;% data.frame() %&gt;% dplyr::mutate(income = factor(income) %&gt;% forcats::fct_recode(&quot;Lower Income (8)&quot; = &quot;8&quot;, &quot;Middle Income (10)&quot; = &quot;10&quot;, &quot;Higher Income (12)&quot; = &quot;12&quot;)) %&gt;% ggplot(aes(x = educate, y = fit)) + geom_ribbon(aes(ymin = fit - se, # bands = +/- 1 SEM ymax = fit + se, fill = female), alpha = .2) + geom_line(aes(linetype = female, color = female), size = 1) + theme_bw() + labs(x = &quot;Education, Years&quot;, y = &quot;Predicted Mean Number of Volunteer Activities&quot;, color = NULL, fill = NULL, linetype = NULL) + theme(legend.position = c(0, 1), legend.justification = c(-.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + facet_grid(nonwhite ~ income) effects::Effect(focal.predictors = c(&quot;female&quot;, &quot;nonwhite&quot;, &quot;educate&quot;, &quot;income&quot;), mod = glm_possion_2, xlevels = list(educate = seq(from = 0, to = 20, by = .1), income = c(5, 8, 12))) %&gt;% data.frame() %&gt;% dplyr::mutate(income = factor(income)) %&gt;% ggplot(aes(x = educate, y = fit)) + geom_line(aes(linetype = fct_rev(income), color = fct_rev(income)), size = 1) + theme_bw() + labs(x = &quot;Education, Years&quot;, y = &quot;Predicted Mean Number of Volunteer Activities&quot;, color = &quot;Income:&quot;, fill = &quot;Income:&quot;, linetype = &quot;Income:&quot;) + theme(legend.position = c(0, 1), legend.justification = c(-.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + facet_grid(nonwhite ~ female) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) effects::Effect(focal.predictors = c(&quot;female&quot;, &quot;nonwhite&quot;, &quot;educate&quot;, &quot;income&quot;), mod = glm_possion_2, xlevels = list(educate = seq(from = 0, to = 20, by = .1), income = c(8, 10, 12))) %&gt;% data.frame() %&gt;% dplyr::mutate(income = factor(income) %&gt;% forcats::fct_recode(&quot;Lower Income (8)&quot; = &quot;8&quot;, &quot;Middle Income (10)&quot; = &quot;10&quot;, &quot;Higher Income (12)&quot; = &quot;12&quot;)) %&gt;% ggplot(aes(x = educate, y = fit)) + geom_ribbon(aes(ymin = fit - se, # bands = +/- 1 SEM ymax = fit + se, fill = nonwhite), alpha = .2) + geom_line(aes(linetype = nonwhite, color = nonwhite), size = 1) + theme_bw() + labs(x = &quot;Education, Years&quot;, y = &quot;Predicted Mean Number of Volunteer Activities&quot;, color = NULL, fill = NULL, linetype = NULL) + theme(legend.position = c(0, .5), legend.justification = c(-.05, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + facet_grid(female ~ income) + scale_color_manual(values = c(&quot;darkgreen&quot;, &quot;orange&quot;)) + scale_fill_manual(values = c(&quot;darkgreen&quot;, &quot;orange&quot;)) effects::Effect(focal.predictors = c(&quot;female&quot;, &quot;educate&quot;), mod = glm_possion_2, xlevels = list(educate = seq(from = 0, to = 20, by = .1), income = 11)) %&gt;% #Median Income data.frame() %&gt;% ggplot(aes(x = educate, y = fit)) + geom_ribbon(aes(ymin = fit - se, # bands = +/- 1 SEM ymax = fit + se, fill = female), alpha = .2) + geom_line(aes(linetype = female, color = female), size = 1) + theme_bw() + labs(x = &quot;Education, Years&quot;, y = &quot;Predicted Mean Number of Volunteer Activities&quot;, color = NULL, fill = NULL, linetype = NULL) + theme(legend.position = c(0, 1), legend.justification = c(-.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) 13.4 Negative Binomial Regression 13.4.1 Multiple Predictors 13.4.1.1 Fit the model glm_negbin_1 &lt;- MASS::glm.nb(volteer ~ female + nonwhite + educate + income, data = data_gss) summary(glm_negbin_1) Call: MASS::glm.nb(formula = volteer ~ female + nonwhite + educate + income, data = data_gss, init.theta = 0.2559648877, link = log) Deviance Residuals: Min 1Q Median 3Q Max -0.8798 -0.6897 -0.6141 -0.5211 2.7019 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.24738 0.37283 -8.710 &lt; 2e-16 *** femalefemale 0.28441 0.12312 2.310 0.0209 * nonwhitenon-white -0.31107 0.16203 -1.920 0.0549 . educate 0.11200 0.02321 4.825 1.4e-06 *** income 0.05193 0.02264 2.293 0.0218 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Negative Binomial(0.256) family taken to be 1) Null deviance: 1068.5 on 1943 degrees of freedom Residual deviance: 1024.3 on 1939 degrees of freedom (959 observations deleted due to missingness) AIC: 2851.6 Number of Fisher Scoring iterations: 1 Theta: 0.2560 Std. Err.: 0.0251 2 x log-likelihood: -2839.5640 Note: the deviance residuals all have absolute values less than 3-4ishbetter than before Theta in R = 1/alpha in Stata # Hoffmann Example 6.5 texreg::knitreg(list(glm_possion_2, texreghelpr::extract_glm_exp(glm_possion_2, include.aic = FALSE, include.bic = FALSE, include.loglik = FALSE, include.deviance = FALSE, include.nobs = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;IRR [95% CI]&quot;), custom.coef.map = list(&quot;(Intercept)&quot; =&quot;Intercept&quot;, femalefemale = &quot;Female vs. Male&quot;, &quot;nonwhitenon-white&quot; = &quot;Non-white vs. White&quot;, educate = &quot;Education, years&quot;, income = &quot;Income, 1000&#39;s&quot;), caption = &quot;GLM: Multiple Possion Regression&quot;, single.row = TRUE, digits = 3) GLM: Multiple Possion Regression   b (SE) IRR [95% CI] Intercept -3.158 (0.245)*** 0.042 [0.026; 0.068]* Female vs. Male 0.261 (0.078)*** 1.299 [1.115; 1.513]* Non-white vs. White -0.280 (0.108)** 0.755 [0.608; 0.930]* Education, years 0.103 (0.014)*** 1.108 [1.077; 1.140]* Income, 1000s 0.057 (0.016)*** 1.058 [1.027; 1.092]* AIC 3380.860   BIC 3408.722   Log Likelihood -1685.430   Deviance 2465.514   Num. obs. 1944   p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 13.4.1.2 Predictions Note: These means are on the original scale (number of volunteer activities in the past year). These standard errors are called delta-method standard errors effects::Effect(focal.predictors = c(&quot;female&quot;), mod = glm_negbin_1, xlevels = list(nonwhite = &quot;non-white&quot;, educate = 5, income = 12)) %&gt;% data.frame() # A tibble: 2 x 5 female fit se lower upper &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 male 0.289 0.0257 0.243 0.344 2 female 0.384 0.0322 0.326 0.453 ggeffects::ggemmeans(model = glm_negbin_1, terms = c(&quot;female&quot;), condition = c(nonwhite = &quot;white&quot;, educate = 12, income = 5)) # A tibble: 2 x 6 x predicted std.error conf.low conf.high group &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 male 0.193 0.157 0.142 0.263 1 2 female 0.257 0.137 0.196 0.336 1 Compare to the Poisson: ggeffects::ggemmeans(model = glm_possion_2, terms = c(&quot;female&quot;), condition = c(nonwhite = &quot;white&quot;, educate = 12, income = 5)) # A tibble: 2 x 6 x predicted std.error conf.low conf.high group &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 male 0.194 0.109 0.157 0.240 1 2 female 0.252 0.0952 0.209 0.303 1 Note: The predictions are very similar for Poisson and Negative Binomialtherefor the overdisperssion does not affect the sex difference much, but it may affect other things 13.4.1.3 Parameter Estimates Coefficients are in terms of the LOG of the number of times a person volunteers per year. glm_negbin_1 %&gt;% coef() (Intercept) femalefemale nonwhitenon-white educate -3.24738340 0.28440826 -0.31107286 0.11199528 income 0.05193102 Exponentiating the coefficients (betas) returns the values to the original scale (number of times a person volunteers per year) and is refered to as the incident rate ratio IRR. glm_negbin_1 %&gt;% coef() %&gt;% exp() (Intercept) femalefemale nonwhitenon-white educate 0.0388758 1.3289754 0.7326605 1.1185076 income 1.0533031 texreg::knitreg(list(glm_negbin_1, texreghelpr::extract_glm_exp(glm_negbin_1, include.aic = FALSE, include.bic = FALSE, include.loglik = FALSE, include.deviance = FALSE, include.nobs = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;IRR [95% CI]&quot;), custom.coef.map = list(&quot;(Intercept)&quot; =&quot;Intercept&quot;, femalefemale = &quot;Female vs. Male&quot;, &quot;nonwhitenon-white&quot; = &quot;Non-white vs. White&quot;, educate = &quot;Education, Years&quot;, income = &quot;Income&quot;), caption = &quot;GLM: Negitive Binomial Regression&quot;, single.row = TRUE, digits = 3) GLM: Negitive Binomial Regression   b (SE) IRR [95% CI] Intercept -3.247 (0.373)*** 0.039 [0.018; 0.081]* Female vs. Male 0.284 (0.123)* 1.329 [1.043; 1.696]* Non-white vs. White -0.311 (0.162) 0.733 [0.533; 1.008]* Education, Years 0.112 (0.023)*** 1.119 [1.067; 1.173]* Income 0.052 (0.023)* 1.053 [1.008; 1.101]* AIC 2851.564   BIC 2884.999   Log Likelihood -1419.782   Deviance 1024.343   Num. obs. 1944   p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 13.4.1.4 Residual Diagnostics par(mfrow = c(2, 2)) plot(glm_negbin_1) par(mfrow = c(1, 1)) These still dont look very good :( 13.4.1.5 Compare models performance::compare_performance(glm_possion_2, glm_negbin_1, rank = TRUE) # A tibble: 2 x 10 Name Model AIC BIC R2_Nagelkerke RMSE Sigma Score_log Score_spherical &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 glm_negbin_1 negbin 2852. 2885. 0.0531 0.919 0.727 -0.808 0.0210 2 glm_possion_2 glm 3381. 3409. 0.0693 0.918 1.13 -0.867 0.0210 # ... with 1 more variable: Performance_Score &lt;dbl&gt; 13.5 Zero Inflated Poisson 13.5.0.1 Fit the model glm_zip_1 &lt;- pscl::zeroinfl(volteer ~ female + nonwhite + educate + income | educate, data = data_gss) summary(glm_zip_1) Call: pscl::zeroinfl(formula = volteer ~ female + nonwhite + educate + income | educate, data = data_gss) Pearson residuals: Min 1Q Median 3Q Max -0.5778 -0.4416 -0.3908 -0.3382 9.4776 Count model coefficients (poisson with log link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.09676 0.34808 -3.151 0.00163 ** femalefemale 0.20623 0.09452 2.182 0.02912 * nonwhitenon-white -0.20099 0.13444 -1.495 0.13491 educate 0.05649 0.02140 2.639 0.00832 ** income 0.04888 0.01882 2.597 0.00940 ** Zero-inflation model coefficients (binomial with logit link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 2.01175 0.41193 4.884 1.04e-06 *** educate -0.07180 0.02769 -2.594 0.0095 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Number of iterations in BFGS optimization: 12 Log-likelihood: -1434 on 7 Df glm_zip_1 %&gt;% coef() %&gt;% exp() count_(Intercept) count_femalefemale count_nonwhitenon-white 0.3339504 1.2290310 0.8179193 count_educate count_income zero_(Intercept) 1.0581113 1.0500956 7.4763533 zero_educate 0.9307151 Compares two models fit to the same data that do not nest via Vuongs non-nested test. pscl::vuong(glm_zip_1, glm_possion_2) Vuong Non-Nested Hypothesis Test-Statistic: (test-statistic is asymptotically distributed N(0,1) under the null that the models are indistinguishible) ------------------------------------------------------------- Vuong z-statistic H_A p-value Raw 8.000316 model1 &gt; model2 6.6613e-16 AIC-corrected 7.936582 model1 &gt; model2 9.9920e-16 BIC-corrected 7.759003 model1 &gt; model2 4.3299e-15 13.6 Zero Inflated Negative Binomial 13.6.0.1 Fit the model glm_zinb_1 &lt;- pscl::zeroinfl(volteer ~ female + nonwhite + educate + income | educate, data = data_gss, dist = &quot;negbin&quot;) summary(glm_zinb_1) Call: pscl::zeroinfl(formula = volteer ~ female + nonwhite + educate + income | educate, data = data_gss, dist = &quot;negbin&quot;) Pearson residuals: Min 1Q Median 3Q Max -0.5146 -0.4122 -0.3704 -0.3222 8.8088 Count model coefficients (negbin with log link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.79283 0.54745 -3.275 0.00106 ** femalefemale 0.26245 0.11745 2.235 0.02544 * nonwhitenon-white -0.28519 0.15603 -1.828 0.06758 . educate 0.06817 0.03317 2.055 0.03986 * income 0.05292 0.02159 2.451 0.01426 * Log(theta) 0.05055 0.46241 0.109 0.91295 Zero-inflation model coefficients (binomial with logit link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.28315 0.71437 1.796 0.0725 . educate -0.07208 0.04684 -1.539 0.1239 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Theta = 1.0518 Number of iterations in BFGS optimization: 27 Log-likelihood: -1416 on 8 Df glm_zinb_1 %&gt;% coef() %&gt;% exp() count_(Intercept) count_femalefemale count_nonwhitenon-white 0.1664876 1.3001178 0.7518694 count_educate count_income zero_(Intercept) 1.0705453 1.0543420 3.6079992 zero_educate 0.9304589 pscl::vuong(glm_zinb_1, glm_negbin_1) Vuong Non-Nested Hypothesis Test-Statistic: (test-statistic is asymptotically distributed N(0,1) under the null that the models are indistinguishible) ------------------------------------------------------------- Vuong z-statistic H_A p-value Raw 1.3486684 model1 &gt; model2 0.088722 AIC-corrected 0.5592184 model1 &gt; model2 0.288006 BIC-corrected -1.6403877 model2 &gt; model1 0.050462 pscl::vuong(glm_zip_1, glm_zinb_1) Vuong Non-Nested Hypothesis Test-Statistic: (test-statistic is asymptotically distributed N(0,1) under the null that the models are indistinguishible) ------------------------------------------------------------- Vuong z-statistic H_A p-value Raw -2.428631 model2 &gt; model1 0.007578 AIC-corrected -2.428631 model2 &gt; model1 0.007578 BIC-corrected -2.428631 model2 &gt; model1 0.007578 The best model is the zero-inflated negative binomial "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
