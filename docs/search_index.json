[
["index.html", "Encyclopedia of Quantitative Methods in R, vol. 4: Multiple Linear Regression Welcome Blocked Notes Code and Output The Authors", " Encyclopedia of Quantitative Methods in R, vol. 4: Multiple Linear Regression Sarah Schwartz &amp; Tyson Barrett Last updated: 2018-09-17 Welcome Backgroup and links to other volumes of this encyclopedia may be found at the Encyclopedia’s Home Website. Blocked Notes Thoughout all the eBooks in this encyclopedia, several small secitons will be blocked out in the following ways: These blocks denote an area UNDER CONSTRUCTION, so check back often. This massive undertaking started during the summer of 2018 and is far from complete. The outline of seven volumes is given above despite any one being complete. Feedback is welcome via either author’s email. These blocks denote something EXTREMELY IMPORTANT. Do NOT skip these notes as they will be used very sparingly. These blocks denote something to DOWNLOAD. This may include software installations, example datasets, or notebook code files. These blocks denote something INTERESTING. These point out information we found of interest or added value. These blocks denote LINKS to other websites. This may include instructional video clips, articles, or blog posts. We are all about NOT re-creating the wheel. If somebody else has described or illustrated a topic well, we celebrate it! Code and Output This is how \\(R\\) code is shown: 1 + 1 This is what the output of the \\(R\\) code above will look: ## [1] 2 The Authors Dr. Sarah Schwartz Dr. Tyson Barrett www.SarahSchwartzStats.com www.TysonBarrett.com Sarah.Schwartz@usu.edu Tyson.Barrett@usu.edu Statistical Consulting Studio Data Science and Discover Unit Why choose R ? Check it out: an article from Fall 2016… No more excuses: R is better than SPSS for psychology undergrads, and students agree FYI This entire encyclopedia is written in \\(R Markdown\\), using \\(R Studio\\) as the text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The book’s source code is hosted on GitHub. If you notice typos or other issues, feel free to email either of the authors. This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. "],
["example-ventricular-shortening-velocity-simple-linear-regression.html", "1 Example: Ventricular Shortening Velocity (simple linear regression) 1.1 Purpose 1.2 Exploratory Data Analysis 1.3 Regression Analysis 1.4 Conclusion", " 1 Example: Ventricular Shortening Velocity (simple linear regression) library(tidyverse) # super helpful everything! library(magrittr) # includes other versions of the pipe library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools library(ISwR) # Introduction to Statistics with R (datasets) 1.1 Purpose 1.1.1 Research Question Is there a relationship between fasting blood flucose and shortening of ventricular velocity among type 1 diabetic patiences? If so, what is the nature of the association? 1.1.2 Data Description This dataset is included in the ISwR package (Dalgaard 2015), which was a companion to the texbook “Introductory Statistics with R, 2nd ed.” (Dalgaard 2008), although it was first published by Altman (1991) in table 11.6. The thuesen data frame has 24 rows and 2 columns. It contains ventricular shortening velocity and blood glucose for type 1 diabetic patients. blood.glucose a numeric vector, fasting blood glucose (mmol/l). short.velocity a numeric vector, mean circumferential shortening velocity (%/s). data(thuesen, package = &quot;ISwR&quot;) tibble::glimpse(thuesen) # view the class and 1st few values of each variable Observations: 24 Variables: 2 $ blood.glucose &lt;dbl&gt; 15.3, 10.8, 8.1, 19.5, 7.2, 5.3, 9.3, 11.1, 7.5... $ short.velocity &lt;dbl&gt; 1.76, 1.34, 1.27, 1.47, 1.27, 1.49, 1.31, 1.09,... 1.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 1.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). thuesen %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max blood.glucose 24 10.300 4.338 4.200 7.075 12.700 19.500 short.velocity 23 1.326 0.233 1.030 1.185 1.420 1.950 The stargazer() function has many handy options, should you wish to change the default settings. thuesen %&gt;% stargazer(type = &quot;html&quot;, digits = 4, flip = TRUE, summary.stat = c(&quot;n&quot;, &quot;mean&quot;, &quot;sd&quot;, &quot;min&quot;, &quot;median&quot;, &quot;max&quot;), title = &quot;Descriptives&quot;) Descriptives Statistic blood.glucose short.velocity N 24 23 Mean 10.3000 1.3257 St. Dev. 4.3375 0.2329 Min 4.2000 1.0300 Median 9.4000 1.2700 Max 19.5000 1.9500 Although the table1() function from the furniture package creates a nice summary table, it ‘hides’ the nubmer of missing values for each continuous variable (Barrett, Brignone, and Laxman 2018). thuesen %&gt;% furniture::table1() ---------------------------------- Mean/Count (SD/%) n = 24 blood.glucose 10.3 (4.3) short.velocity 1.3 (0.2) ---------------------------------- 1.2.2 Univariate Visualizations thuesen %&gt;% ggplot() + aes(blood.glucose) + # variable of interest (just one) geom_histogram(binwidth = 2) # specify the width of the bars thuesen %&gt;% ggplot() + aes(short.velocity) + # variable of interest (just one) geom_histogram(bins = 10) # specify the number of bars 1.2.3 Bivariate Statistics (Unadjusted Pearson’s correlation) The cor() fucntion in base \\(R\\) doesn’t like NA or missing values thuesen %&gt;% cor() blood.glucose short.velocity blood.glucose 1 NA short.velocity NA 1 You may specify how to handle cases that are missing on at least one of the variables of interest: use = &quot;everything&quot; NAs will propagate conceptually, i.e., a resulting value will be NA whenever one of its contributing observations is NA &lt;– DEFAULT *use = &quot;all.obs&quot; the presence of missing observations will produce an error use = &quot;complete.obs&quot; missing values are handled by casewise deletion (and if there are no complete cases, that gives an error). use = &quot;na.or.complete&quot; is the same as above unless there are no complete cases, that gives NA use = &quot;pairwise.complete.obs&quot; the correlation between each pair of variables is computed using all complete pairs of observations on those variables. This can result in covariance matrices which are not positive semi-definite, as well as NA entries if there are no complete pairs for that pair of variables. Commonly, we want listwise deletion: thuesen %&gt;% cor(use = &quot;complete.obs&quot;) # list-wise deletion blood.glucose short.velocity blood.glucose 1.0000000 0.4167546 short.velocity 0.4167546 1.0000000 It is also handy to specify the number of decimal places desired, but adding a rounding step: thuesen %&gt;% cor(use=&quot;complete.obs&quot;) %&gt;% round(2) # number od decimal places blood.glucose short.velocity blood.glucose 1.00 0.42 short.velocity 0.42 1.00 If you desire a correlation single value of a single PAIR of variables, instead of a matrix, then you must use a magrittr exposition pipe (%$%) thuesen %$% # notice the special kind of pipe cor(blood.glucose, short.velocity, # specify exactly TWO variables use=&quot;complete.obs&quot;) [1] 0.4167546 In addition to the cor() funciton, the base \\(R\\) stats package also includes the cor.test() function to test if the correlation is zero (\\(H_0: R = 0\\)) This TESTS if the cor == 0 thuesen %$% # notice the special kind of pipe cor.test(blood.glucose, short.velocity, # specify exactly TWO variables use=&quot;complete.obs&quot;) Pearson&#39;s product-moment correlation data: blood.glucose and short.velocity t = 2.101, df = 21, p-value = 0.0479 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.005496682 0.707429479 sample estimates: cor 0.4167546 The default correltaion type for cor()is Pearson’s \\(R\\), which assesses linear relationships. Spearman’s correlation assesses monotonic relationships. thuesen %$% # notice the special kind of pipe cor(blood.glucose, short.velocity, # specify exactly TWO variables use = &#39;complete&#39;, method = &#39;spearman&#39;) # spearman&#39;s (rho) [1] 0.318002 1.2.4 Bivariate Visualization Scatterplots show the relationship between two continuous measures (one on the \\(x-axis\\) and the other on the \\(y-axis\\)), with one point for each observation. thuesen %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = short.velocity)) + # y-axis variable geom_point() + # place a point for each observation theme_bw() # black-and-white theme Both the code chunk above and below produce the same plot. ggplot(thuesen, aes(x = blood.glucose, # x-axis variable y = short.velocity)) + # y-axis variable geom_point() + # place a point for each observation theme_bw() # black-and-white theme 1.3 Regression Analysis 1.3.1 Fit A Simple Linear Model \\[ Y = \\beta_0 + \\beta_1 \\times X \\] short.velocity dependent variable or outcome (\\(Y\\)) blood.glucose independent variable or predictor (\\(X\\)) The lm() function must be supplied with at least two options: a formula: Y ~ X a dataset: data = XXXXXXX When a model is fit and directly saved as a named object via the assignment opperator (&lt;-), no output is produced. fit_vel_glu &lt;- lm(short.velocity ~ blood.glucose, data = thuesen) Running the name of the fit object yields very little output: fit_vel_glu Call: lm(formula = short.velocity ~ blood.glucose, data = thuesen) Coefficients: (Intercept) blood.glucose 1.09781 0.02196 Appling the summary() funciton produced a good deal more output: summary(fit_vel_glu) Call: lm(formula = short.velocity ~ blood.glucose, data = thuesen) Residuals: Min 1Q Median 3Q Max -0.40141 -0.14760 -0.02202 0.03001 0.43490 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.09781 0.11748 9.345 6.26e-09 *** blood.glucose 0.02196 0.01045 2.101 0.0479 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2167 on 21 degrees of freedom (1 observation deleted due to missingness) Multiple R-squared: 0.1737, Adjusted R-squared: 0.1343 F-statistic: 4.414 on 1 and 21 DF, p-value: 0.0479 You may request specific pieces of the output: Coefficients or beta estimates: coef(fit_vel_glu) (Intercept) blood.glucose 1.09781488 0.02196252 95% confidence intervals for the coefficients or beta estimates: confint(fit_vel_glu) 2.5 % 97.5 % (Intercept) 0.8534993816 1.34213037 blood.glucose 0.0002231077 0.04370194 The F-test for overall modle fit vs. a \\(null\\) or empty model having only an intercept and no predictors. anova(fit_vel_glu) # A tibble: 2 x 5 Df `Sum Sq` `Mean Sq` `F value` `Pr(&gt;F)` * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0.207 0.207 4.41 0.0479 2 21 0.986 0.0470 NA NA Various other model fit indicies: logLik(fit_vel_glu) &#39;log Lik.&#39; 3.583612 (df=3) AIC(fit_vel_glu) [1] -1.167223 BIC(fit_vel_glu) [1] 2.239259 1.3.2 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_vel_glu, which = 1) plot(fit_vel_glu, which = 2) plot(fit_vel_glu, which = 5) plot(fit_vel_glu, which = 6) Viewing potentially influencial or outlier points based on plots above: thuesen %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::filter(id == c(13, 20, 24)) # A tibble: 3 x 3 blood.glucose short.velocity id &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 19 1.95 13 2 16.1 1.05 20 3 9.5 1.7 24 The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2018). car::residualPlots(fit_vel_glu) Test stat Pr(&gt;|Test stat|) blood.glucose 0.9289 0.3640 Tukey test 0.9289 0.3529 Here is a fancy way to visulaize ‘potential problem cases’ with ggplot2: thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases ggplot() + # name the FULL dataset aes(x = blood.glucose, # x-axis variable name y = short.velocity) + # y-axis variable name geom_point() + # do a scatterplot stat_smooth(method = &quot;lm&quot;) + # smooth: linear model theme_bw() + # black-and-while theme geom_point(data = thuesen %&gt;% # override the dataset from above filter(row_number() == c(13, 20, 24)), # with a reduced subset of cases size = 4, # make the points bigger in size color = &quot;red&quot;) # give the points a different color 1.3.3 Manually checking residual diagnostics You may extract values from the model in dataset form and then you can maually plot the residuals. thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) # residual values # A tibble: 23 x 4 blood.glucose short.velocity pred resid &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 15.3 1.76 1.43 0.326 2 10.8 1.34 1.34 0.00499 3 8.1 1.27 1.28 -0.00571 4 19.5 1.47 1.53 -0.0561 5 7.2 1.27 1.26 0.0141 6 5.3 1.49 1.21 0.276 7 9.3 1.31 1.30 0.00793 8 11.1 1.09 1.34 -0.252 9 7.5 1.18 1.26 -0.0825 10 12.2 1.22 1.37 -0.146 # ... with 13 more rows Check for equal spread of points along the \\(y=0\\) horizontal line: thuesen %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) %&gt;% # residual values ggplot() + aes(x = id, y = resid) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, size = 1, linetype = &quot;dashed&quot;) + theme_classic() + labs(title = &quot;Looking for homogeneity of residuals&quot;, subtitle = &quot;want to see equal spread all across&quot;) Check for normality: thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) %&gt;% # residual values ggplot() + aes(resid) + geom_histogram(bins = 12, color = &quot;blue&quot;, fill = &quot;blue&quot;, alpha = 0.3) + geom_vline(xintercept = 0, size = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + theme_classic() + labs(title = &quot;Looking for normality of residuals&quot;, subtitle = &quot;want to see roughly a bell curve&quot;) 1.4 Conclusion 1.4.1 Tabulate the Final Model Summary You may also present the output in a table using two different packages: The stargazer package has stargazer() function: stargazer::stargazer(fit_vel_glu, type = &quot;html&quot;) Dependent variable: short.velocity blood.glucose 0.022** (0.010) Constant 1.098*** (0.117) Observations 23 R2 0.174 Adjusted R2 0.134 Residual Std. Error 0.217 (df = 21) F Statistic 4.414** (df = 1; 21) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 The stargazer package can produce the regression table in various output types: type = “latex Default Use when knitting your .Rmd file to a .pdf via LaTeX type = “text Default Use when working on a project and viewing tables on your computer screen type = “html Default Use when knitting your .Rmd file to a .html document The texreg package has the texreg() fucntion: texreg::htmlreg(fit_vel_glu) Statistical models Model 1 (Intercept) 1.10*** (0.12) blood.glucose 0.02* (0.01) R2 0.17 Adj. R2 0.13 Num. obs. 23 RMSE 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 The texreg package contains three version of the regression table function. screenreg() Use when working on a project and viewing tables on your computer screen htmlreg() Use when knitting your .Rmd file to a .html document texreg() Use when knitting your .Rmd file to a .pdf via LaTeX 1.4.2 Plot the Model When a model only contains main effects, a plot is not important for interpretation, but can help understand the relationship between multiple predictors. The Effect() function from the effects package chooses ‘5 or 6 nice values’ for your continuous independent variable (\\(X\\)) based on the range of values found in the dataset on which the model was fit and plugs them into the regression equation \\(Y = \\beta_0 + \\beta_1 \\times X\\) to compute the predicted mean value of the outcome (\\(Y\\)) (Fox et al. 2018). effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), # IV variable name mod = fit_vel_glu) # fitted model name blood.glucose effect blood.glucose 4.2 8 12 16 20 1.190057 1.273515 1.361365 1.449215 1.537065 You may override the ‘nice values’ using the xlevels = list(var_name = c(#, #, ...#) option. effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = c(5, 10, 15, 20))) blood.glucose effect blood.glucose 5 10 15 20 1.207627 1.317440 1.427253 1.537065 Adding a piped data frame step (%&gt;% data.frame()) will arrange the predicted \\(Y\\) values into a column called fit. This tidy data format is ready for plotting. effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu) %&gt;% data.frame() # A tibble: 5 x 5 blood.glucose fit se lower upper * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4.2 1.19 0.0788 1.03 1.35 2 8 1.27 0.0516 1.17 1.38 3 12 1.36 0.0483 1.26 1.46 4 16 1.45 0.0742 1.29 1.60 5 20 1.54 0.110 1.31 1.77 effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = c(5, 12, 20))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .5) + # ribbon transparency level geom_line() + theme_bw() Notice that although the regression line is smooth, the ribbon is choppy. This is because we are basing it on only THREE values of \\(X\\). c(5, 12, 20) [1] 5 12 20 Use the seq() function in base \\(R\\) to request many values of \\(X\\) seq(from = 5, to = 20, by = 5) [1] 5 10 15 20 seq(from = 5, to = 20, by = 2) [1] 5 7 9 11 13 15 17 19 seq(from = 5, to = 20, by = 1) [1] 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 seq(from = 5, to = 20, by = .5) [1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 11.5 [15] 12.0 12.5 13.0 13.5 14.0 14.5 15.0 15.5 16.0 16.5 17.0 17.5 18.0 18.5 [29] 19.0 19.5 20.0 effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .5) + # ribbon transparency level geom_line() + theme_bw() Now that we are basing our ribbon on MANY more points of \\(X\\), the ribbon is much smoother. For publication, you would of course want to clean up the plot a bit more: effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, # x-axis variable y = fit) + # y-axis variable geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon ymax = upper), # top edge of the ribbon alpha = .3) + # ribbon transparency level geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) # axis labels The above plot has a ribbon that represents a 95% confidence interval (lower toupper) for the MEAN (fit) outcome. Sometimes we would rather display a ribbon for only the MEAN (fit) plus-or-minus ONE STANDARD ERROR (se) for the mean. You would do that by changing the variables that define the min and max edges of the ribbon (notice the range of the y-axis has changed): effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, y = fit) + geom_ribbon(aes(ymin = fit - se, # bottom edge of the ribbon ymax = fit + se), # top edge of the ribbon alpha = .3) + geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) Of course, you could do both ribbons together: effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 5, to = 20, by = .5))) %&gt;% data.frame() %&gt;% ggplot() + aes(x = blood.glucose, y = fit) + geom_ribbon(aes(ymin = lower, # bottom edge of the ribbon = lower of the 95% CI ymax = upper), # top edge of the ribbon = upper of the 95% CI alpha = .3) + geom_ribbon(aes(ymin = fit - se, # bottom edge of the ribbon = mean - SE ymax = fit + se), # top edge of the ribbon = Mean + SE alpha = .3) + geom_line() + theme_bw() + labs(x = &quot;Fasting Blood Glucose (mmol/l)&quot;, y = &quot;Mean Circumferential Shortening Velocity (%/s)&quot;) # axis labels "],
["example-obesity-and-blood-pressure-interaction-between-a-continuous-and-categorical-ivs.html", "2 Example: Obesity and Blood Pressure (interaction between a continuous and categorical IVs) 2.1 Purpose 2.2 Exploratory Data Analysis 2.3 Regression Analysis 2.4 Conclusion", " 2 Example: Obesity and Blood Pressure (interaction between a continuous and categorical IVs) library(tidyverse) # super helpful everything! library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools library(GGally) # extensions to ggplot2 library(ISwR) # Introduction to Statistics with R (datasets) 2.1 Purpose 2.1.1 Research Question Is obsesity associated with higher blood pressure and is that relationship the same among men and women? 2.1.2 Data Description This dataset is included in the ISwR package (Dalgaard 2015), which was a companion to the texbook “Introductory Statistics with R, 2nd ed.” (Dalgaard 2008), although it was first published by Brown and Hollander (1977). To view the documentation for the dataset, type ?bp.obese in the console and enter or search the help tab for `bp.obese’. The bp.obese data frame has 102 rows and 3 columns. It contains data from a random sample of Mexican-American adults in a small California town. This data frame contains the following columns: sex a numeric vector code, 0: male, 1: female obese a numeric vector, ratio of actual weight to ideal weight from New York Metropolitan Life Tables bp a numeric vector,systolic blood pressure (mm Hg) data(bp.obese, package = &quot;ISwR&quot;) bp.obese &lt;- bp.obese %&gt;% dplyr::mutate(sex = factor(sex, labels = c(&quot;Male&quot;, &quot;Female&quot;))) tibble::glimpse(bp.obese) Observations: 102 Variables: 3 $ sex &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Ma... $ obese &lt;dbl&gt; 1.31, 1.31, 1.19, 1.11, 1.34, 1.17, 1.56, 1.18, 1.04, 1.... $ bp &lt;int&gt; 130, 148, 146, 122, 140, 146, 132, 110, 124, 150, 120, 1... 2.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 2.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). bp.obese %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max obese 102 1.313 0.258 0.810 1.143 1.430 2.390 bp 102 127.020 18.184 94 116 137.5 208 2.2.2 Bivariate Relationships The furniture package’s table1() function is a clean way to create a descriptive table that compares distinct subgroups of your sample (Barrett, Brignone, and Laxman 2018). bp.obese %&gt;% furniture::table1(obese, bp, splitby = ~ sex, test = TRUE, output = &quot;html&quot;) Male Female P-Value n = 44 n = 58 obese &lt;.001 1.2 (0.2) 1.4 (0.3) bp 0.646 128.0 (16.6) 126.3 (19.4) The ggpairs() function in the GGally package is helpful for showing all pairwise relationships in raw data, especially seperating out two or three groups (Schloerke et al. 2018). GGally::ggpairs(bp.obese, mapping = aes(fill = sex, col = sex, alpha = 0.1), upper = list(continuous = &quot;smooth&quot;, combo = &quot;facethist&quot;, discrete = &quot;ratio&quot;), lower = list(continuous = &quot;cor&quot;, combo = &quot;box&quot;, discrete = &quot;facetbar&quot;), title = &quot;Very Useful for Exploring Data&quot;) bp.obese %&gt;% ggplot() + aes(x = sex, y = bp, fill = sex) + geom_boxplot(alpha = 0.6) + scale_fill_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;)) + labs(x = &quot;Gender&quot;, y = &quot;Blood Pressure (mmHg)&quot;) + guides(fill = FALSE) + theme_bw() Visual inspection for an interaction (is gender a moderator?) bp.obese %&gt;% ggplot(aes(x = obese, y = bp, color = sex)) + geom_point(size = 3) + geom_smooth(aes(fill = sex), alpha = 0.2, method = &quot;lm&quot;) + scale_color_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;), breaks = c(&quot;male&quot;, &quot;female&quot;), labels = c(&quot;Men&quot;, &quot;Women&quot;)) + scale_fill_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;), breaks = c(&quot;male&quot;, &quot;female&quot;), labels = c(&quot;Men&quot;, &quot;Women&quot;)) + labs(title = &quot;Does Gender Moderate the Association Between Obesity and Blood Pressure?&quot;, x = &quot;Ratio: Actual Weight vs. Ideal Weight (NYM Life Tables)&quot;, y = &quot;Systolic Blood Pressure (mmHg)&quot;) + theme_bw() + scale_x_continuous(breaks = seq(from = 0, to = 3, by = 0.25 )) + scale_y_continuous(breaks = seq(from = 75, to = 300, by = 25)) + theme(legend.title = element_blank(), legend.key = element_rect(fill = &quot;white&quot;), legend.background = element_rect(color = &quot;black&quot;), legend.justification = c(1, 0), legend.position = c(1, 0)) bp.obese %&gt;% dplyr::mutate(sex = as.numeric(sex)) %&gt;% # cor needs only numeric cor() %&gt;% round(3) sex obese bp sex 1.000 0.405 -0.045 obese 0.405 1.000 0.326 bp -0.045 0.326 1.000 Often it is easier to digest a correlation matrix if it is visually presented, instead of just given as a table of many numbers. The corrplot package has a useful function called corrplot.mixed() for doing just that (Wei and Simko 2017). bp.obese %&gt;% dplyr::mutate(sex = as.numeric(sex)) %&gt;% # cor needs only numeric cor() %&gt;% corrplot::corrplot.mixed(lower = &quot;ellipse&quot;, upper = &quot;number&quot;, tl.col = &quot;black&quot;) 2.3 Regression Analysis 2.3.1 Fit Nested Models The bottom-up approach consists of starting with an initial NULL model with only an intercept term and them building additional models that are nested. Two models are considered nested if one is conains a subset of the terms (predictors or IV) compared to the other. fit_bp_null &lt;- lm(bp ~ 1, data = bp.obese) # intercept only or NULL model fit_bp_sex &lt;- lm(bp ~ sex, data = bp.obese) fit_bp_obe &lt;- lm(bp ~ obese, data = bp.obese) fit_bp_obesex &lt;- lm(bp ~ obese + sex, data = bp.obese) fit_bp_inter &lt;- lm(bp ~ obese*sex, data = bp.obese) 2.3.2 Comparing Nested Models 2.3.2.1 Model Comparison Table In single level, multiple linear regression significance of predictors (independent variables, IV) is usually based on both the Wald tests of significance for each beta estimate (shown with stars here) and comparisons in the model fit via the \\(R^2\\) values. Again the texreg package comes in handy to display several models in the same tal e (Leifeld 2017). texreg::htmlreg(list(fit_bp_null, fit_bp_sex, fit_bp_obe, fit_bp_obesex, fit_bp_inter), custom.model.names = c(&quot;No Predictors&quot;, &quot;Only Sex Quiz&quot;, &quot;Only Obesity&quot;, &quot;Both IVs&quot;, &quot;Add Interaction&quot;)) Statistical models No Predictors Only Sex Quiz Only Obesity Both IVs Add Interaction (Intercept) 127.02*** 127.95*** 96.82*** 93.29*** 102.11*** (1.80) (2.75) (8.92) (8.94) (18.23) sexFemale -1.64 -7.73* -19.60 (3.65) (3.72) (21.66) obese 23.00*** 29.04*** 21.65 (6.67) (7.17) (15.12) obese:sexFemale 9.56 (17.19) R2 0.00 0.00 0.11 0.14 0.15 Adj. R2 0.00 -0.01 0.10 0.13 0.12 Num. obs. 102 102 102 102 102 RMSE 18.18 18.26 17.28 17.00 17.05 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 2.3.2.2 Likelihood Ratio Test of Nested Models An alternative method for determing model fit and variable importance is the likelihood ratio test. This involves comparing the \\(-2LL\\) or inverse of twice the log of the likelihood value for the model. The difference in these values follows a Chi Squared distribution with degrees of freedom equal to the difference in the number of parameters estimated (number of betas). Test the main effect of math quiz: anova(fit_bp_null, fit_bp_sex) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 101 33398. NA NA NA NA 2 100 33330. 1 67.6 0.203 0.653 Test the main effect of math phobia anova(fit_bp_null, fit_bp_obe) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 101 33398. NA NA NA NA 2 100 29846. 1 3552. 11.9 0.000822 Test the main effect of math phobia, after controlling for math test anova(fit_bp_obe, fit_bp_obesex) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 100 29846. NA NA NA NA 2 99 28595. 1 1250. 4.33 0.0401 Test the interaction between math test and math phobia (i.e. moderation) anova(fit_bp_obesex, fit_bp_inter) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 99 28595. NA NA NA NA 2 98 28505. 1 89.9 0.309 0.579 2.3.3 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_bp_obesex, which = 1) plot(fit_bp_obesex, which = 4, id.n = 10) # Change the number labeled The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2018). car::residualPlots(fit_bp_obesex) Test stat Pr(&gt;|Test stat|) obese -0.2759 0.7832 sex Tukey test -0.6141 0.5391 you can adjust any part of a ggplot bp.obese %&gt;% dplyr::mutate(e_bp = resid(fit_bp_obesex)) %&gt;% # add the resid to the dataset ggplot(aes(x = sex, # x-axis variable name y = e_bp, # y-axis variable name color = sex, # color is the outline fill = sex)) + # fill is the inside geom_hline(yintercept = 0, # set at a meaningful value size = 1, # adjust line thickness linetype = &quot;dashed&quot;, # set type of line color = &quot;purple&quot;) + # color of line geom_boxplot(alpha = 0.5) + # level of transparency theme_bw() + # my favorite theme labs(title = &quot;Check Assumptions&quot;, # main title&#39;s text x = &quot;Gender&quot;, # x-axis text label y = &quot;Blood Pressure, Residual (bpm)&quot;) + # y-axis text label scale_y_continuous(breaks = seq(from = -40, # declare a sequence of to = 80, # values to make the by = 20)) + # tick marks at guides(color = FALSE, fill = FALSE) # no legends included bp.obese %&gt;% dplyr::mutate(e_bp = resid(fit_bp_obesex)) %&gt;% # add the resid to the dataset ggplot(aes(x = e_bp, # y-axis variable name color = sex, # color is the outline fill = sex)) + # fill is the inside geom_density(alpha = 0.5) + geom_vline(xintercept = 0, # set at a meaningful value size = 1, # adjust line thickness linetype = &quot;dashed&quot;, # set type of line color = &quot;purple&quot;) + # color of line theme_bw() + # my favorite theme labs(title = &quot;Check Assumptions&quot;, # main title&#39;s text x = &quot;Blood Pressure, Residual (bpm)&quot;) + # y-axis text label scale_x_continuous(breaks = seq(from = -40, # declare a sequence of to = 80, # values to make the by = 20)) # tick marks at 2.4 Conclusion Violations to the assumtions call the reliabity of the regression results into question. The data should be further investigated, specifically the \\(102^{nd}\\) case. "],
["example-ihnos-experiment-interaction-between-two-continuous-ivs.html", "3 Example: Ihno’s Experiment (interaction between two continuous IVs) 3.1 Purpose 3.2 Exploratory Data Analysis 3.3 Regression Analysis 3.4 Conclusion 3.5 Write-up", " 3 Example: Ihno’s Experiment (interaction between two continuous IVs) library(tidyverse) # super helpful everything! library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(corrplot) # visualize correlations library(car) # companion for applied regression library(effects) # effect displays for models library(psych) # lots of handy tools 3.1 Purpose 3.1.1 Research Question Does math phobia moderate the relationship between math and statistics performance? That is, does the assocation between math and stat quiz performance differ at variaous levels of math phobia? 3.1.2 Data Description Inho’s dataset is included in the textbook “Explaining Psychological Statistics” (Cohen 2013) and details regarding the sample and measures is describe in this Encyclopedia’s Vol. 2 - Ihno’s Dataset. data_ihno &lt;- haven::read_spss(&quot;http://www.psych.nyu.edu/cohen/Ihno_dataset.sav&quot;) %&gt;% dplyr::rename_all(tolower) %&gt;% dplyr::mutate(gender = factor(gender, levels = c(1, 2), labels = c(&quot;Female&quot;, &quot;Male&quot;))) %&gt;% dplyr::mutate(major = factor(major, levels = c(1, 2, 3, 4,5), labels = c(&quot;Psychology&quot;, &quot;Premed&quot;, &quot;Biology&quot;, &quot;Sociology&quot;, &quot;Economics&quot;))) %&gt;% dplyr::mutate(reason = factor(reason, levels = c(1, 2, 3), labels = c(&quot;Program requirement&quot;, &quot;Personal interest&quot;, &quot;Advisor recommendation&quot;))) %&gt;% dplyr::mutate(exp_cond = factor(exp_cond, levels = c(1, 2, 3, 4), labels = c(&quot;Easy&quot;, &quot;Moderate&quot;, &quot;Difficult&quot;, &quot;Impossible&quot;))) %&gt;% dplyr::mutate(coffee = factor(coffee, levels = c(0, 1), labels = c(&quot;Not a regular coffee drinker&quot;, &quot;Regularly drinks coffee&quot;))) 3.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 3.2.1 Univariate Statistics Summary Statistics for all three variables of interest (Hlavac 2018). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% data.frame() %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max phobia 100 3.310 2.444 0 1 4 10 mathquiz 85 29.071 9.480 9.000 22.000 35.000 49.000 statquiz 100 6.860 1.700 1 6 8 10 3.2.2 Bivariate Relationships The furniture package’s table1() function is a clean way to create a descriptive table that compares distinct subgroups of your sample (Barrett, Brignone, and Laxman 2018). Although categorizing continuous variables results in a loss of information (possible signal or noise), it is often done to investigate relationships in an exploratory way. data_ihno %&gt;% dplyr::mutate(phobia_cut3 = cut(phobia, breaks = c(0, 2, 4, 10), include.lowest = TRUE)) %&gt;% furniture::table1(mathquiz, statquiz, splitby = ~ phobia_cut3, test = TRUE, output = &quot;html&quot;) [0,2] (2,4] (4,10] P-Value n = 39 n = 37 n = 24 mathquiz 0.014 32.6 (8.5) 26.5 (9.8) 26.8 (8.9) statquiz 0.001 7.6 (1.3) 6.6 (1.6) 6.1 (2.0) One of the quickest ways to get a feel for all the pairwise relationships in your dataset (provided there aren’t too many variables) is with the pairs.panels() function in the psych package (Revelle 2018). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% data.frame() %&gt;% psych::pairs.panels(lm = TRUE, ci = TRUE, stars = TRUE) When two variables are both continuous, correlations (Pearson’s \\(R\\)) are an important measure of association. Notice the discrepincy between the correlation between statquiz and phobia. Above, the psych::pairs.panels() function uses pairwise complete cases by default, so \\(r=-.39\\) is computed on all \\(n=100\\) subjects. Below, we specified use = &quot;complete.obs&quot; in the cor() fucntion, so all correlations will be based on the same \\(n=85\\) students, making it listwise complete. The choice of which method to you will vary by situation. Often it is easier to digest a correlation matrix if it is visually presented, instead of just given as a table of many numbers. The corrplot package has a useful function called corrplot.mixed() for doing just that (Wei and Simko 2017). data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% cor(use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot.mixed(lower = &quot;ellipse&quot;, upper = &quot;number&quot;, tl.col = &quot;black&quot;) 3.3 Regression Analysis 3.3.1 Subset the Sample All regression models can only be fit to complete observations regarding the variables included in the model (dependent and independent). Removing any case that is incomplete with respect to even one variables is called “list-wise deletion”. In this analysis, models including the mathquiz variable will be fit on only 85 students (sincle 15 students did not take the math quiz), where as models not including this variable will be fit to all 100 studnets. This complicates model comparisons, which require nested models be fit to the same data (exactly). For this reason, the dataset has been reduced to the subset of students that are complete regarding the three variables utilized throughout the set of five nested models. data_ihno_fitting &lt;- data_ihno %&gt;% dplyr::filter(complete.cases(mathquiz, statquiz, phobia)) dim(data_ihno_fitting) [1] 85 18 3.3.2 Fit Nested Models The bottom-up approach consists of starting with an initial NULL model with only an intercept term and them building additional models that are nested. Two models are considered nested if one is conains a subset of the terms (predictors or IV) compared to the other. fit_ihno_lm_0 &lt;- lm(statquiz ~ 1, # null model: intercept only data = data_ihno_fitting) fit_ihno_lm_1 &lt;- lm(statquiz ~ mathquiz, # only main effect of mathquiz data = data_ihno_fitting) fit_ihno_lm_2 &lt;- lm(statquiz ~ phobia, # only mian effect of phobia data = data_ihno_fitting) fit_ihno_lm_3 &lt;- lm(statquiz ~ mathquiz + phobia, # both main effects data = data_ihno_fitting) fit_ihno_lm_4 &lt;- lm(statquiz ~ mathquiz*phobia, # additional interaction data = data_ihno_fitting) 3.3.3 Comparing Nested Models 3.3.3.1 Model Comparison Table In single level, multiple linear regression significance of predictors (independent variables, IV) is usually based on both the Wald tests of significance for each beta estimate (shown with stars here) and comparisons in the model fit via the \\(R^2\\) values. Again the texreg package comes in handy to display several models in the same tal e (Leifeld 2017). texreg::htmlreg(list(fit_ihno_lm_0, fit_ihno_lm_1, fit_ihno_lm_2, fit_ihno_lm_3, fit_ihno_lm_4), custom.model.names = c(&quot;No Predictors&quot;, &quot;Only Math Quiz&quot;, &quot;Only Phobia&quot;, &quot;Both IVs&quot;, &quot;Add Interaction&quot;)) Statistical models No Predictors Only Math Quiz Only Phobia Both IVs Add Interaction (Intercept) 6.85*** 4.14*** 7.65*** 5.02*** 5.60*** (0.19) (0.53) (0.29) (0.63) (0.91) mathquiz 0.09*** 0.08*** 0.06* (0.02) (0.02) (0.03) phobia -0.25*** -0.16* -0.34 (0.07) (0.07) (0.21) mathquiz:phobia 0.01 (0.01) R2 0.00 0.26 0.13 0.31 0.31 Adj. R2 0.00 0.25 0.12 0.29 0.29 Num. obs. 85 85 85 85 85 RMSE 1.74 1.50 1.63 1.46 1.46 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.3.3.2 Likelihood Ratio Test of Nested Models An alternative method for determing model fit and variable importance is the likelihood ratio test. This involves comparing the \\(-2LL\\) or inverse of twice the log of the likelihood value for the model. The difference in these values follows a Chi Squared distribution with degrees of freedom equal to the difference in the number of parameters estimated (number of betas). Test the main effect of math quiz: anova(fit_ihno_lm_0, fit_ihno_lm_1) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 84 253. NA NA NA NA 2 83 188. 1 65.3 28.8 0.000000700 Test the main effect of math phobia anova(fit_ihno_lm_0, fit_ihno_lm_2) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 84 253. NA NA NA NA 2 83 221. 1 32.3 12.1 0.000791 Test the main effect of math phobia, after controlling for math test anova(fit_ihno_lm_1, fit_ihno_lm_3) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 83 188. NA NA NA NA 2 82 175. 1 12.6 5.88 0.0175 Test the interaction between math test and math phobia (i.e. moderation) anova(fit_ihno_lm_3, fit_ihno_lm_4) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 82 175. NA NA NA NA 2 81 173. 1 1.69 0.789 0.377 3.3.4 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_ihno_lm_3, which = 1) plot(fit_ihno_lm_3, which = 2) The car package has a handy function called residualPlots() for displaying residual plots quickly (Fox, Weisberg, and Price 2018). car::residualPlots(fit_ihno_lm_3) Test stat Pr(&gt;|Test stat|) mathquiz -1.7778 0.07918 . phobia 0.5004 0.61813 Tukey test -1.5749 0.11527 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 While the model tables give starts to denote significance, you may print the actual p-values with the summary() function applied to the model name. summary(fit_ihno_lm_3) Call: lm(formula = statquiz ~ mathquiz + phobia, data = data_ihno_fitting) Residuals: Min 1Q Median 3Q Max -4.3436 -0.8527 0.2805 0.9857 2.7370 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.01860 0.62791 7.993 7.23e-12 *** mathquiz 0.08097 0.01754 4.617 1.42e-05 *** phobia -0.16176 0.06670 -2.425 0.0175 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.462 on 82 degrees of freedom Multiple R-squared: 0.3076, Adjusted R-squared: 0.2907 F-statistic: 18.21 on 2 and 82 DF, p-value: 2.849e-07 summary(fit_ihno_lm_4) Call: lm(formula = statquiz ~ mathquiz * phobia, data = data_ihno_fitting) Residuals: Min 1Q Median 3Q Max -4.1634 -0.8433 0.2832 0.9685 2.9434 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.600183 0.907824 6.169 2.57e-08 *** mathquiz 0.061216 0.028334 2.161 0.0337 * phobia -0.339426 0.210907 -1.609 0.1114 mathquiz:phobia 0.006485 0.007303 0.888 0.3771 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.464 on 81 degrees of freedom Multiple R-squared: 0.3143, Adjusted R-squared: 0.2889 F-statistic: 12.37 on 3 and 81 DF, p-value: 9.637e-07 3.4 Conclusion 3.4.1 Tabulate the Final Model Summary Many journals prefer that regression tables include 95% confidence intervals, rater than standard errors for the beta estimates. The texreg package contains three version of the regression table function (Leifeld 2017). screenreg() Use when working on a project and viewing tables on your computer screen htmlreg() Use when knitting your .Rmd file to a .html document texreg() Use when knitting your .Rmd file to a .pdf via LaTeX texreg::htmlreg(fit_ihno_lm_3, custom.model.names = &quot;Main Effects Model&quot;, ci.force = TRUE, # request 95% conf interv caption = &quot;Final Model for Stat&#39;s Quiz&quot;, single.row = TRUE) Final Model for Stat’s Quiz Main Effects Model (Intercept) 5.02 [3.79; 6.25]* mathquiz 0.08 [0.05; 0.12]* phobia -0.16 [-0.29; -0.03]* R2 0.31 Adj. R2 0.29 Num. obs. 85 RMSE 1.46 * 0 outside the confidence interval 3.4.2 Plot the Model When a model only contains main effects, a plot is not important for interpretation, but can help understand the relationship between multiple predictors. The Effect() function from the effects package chooses ‘5 or 6 nice values’ for each of your continuous independent variable (\\(X&#39;s\\)) based on the range of values found in the dataset on which the model and plugs all possible combinations of them into the regression equation \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 \\dots \\beta_k X_k\\) to compute the predicted mean value of the outcome (\\(Y\\)) (Fox et al. 2018). When plotting a regression model the outcome (dependent variable) is always on the y-axis (fit) and only one predictor (independent variable) may be used on the x-axis. You may incorporate additional predictor using colors, shapes, linetypes, or facets. For these predictors, you will want to specify only 2-4 values for illustration and then declare them as factors prior to plotting. effects::Effect(focal.predictors = c(&quot;mathquiz&quot;, &quot;phobia&quot;), mod = fit_ihno_lm_3, xlevels = list(phobia = c(0, 5, 10))) %&gt;% # values for illustration data.frame %&gt;% dplyr::mutate(phobia = factor(phobia)) %&gt;% # factor for illustration ggplot() + aes(x = mathquiz, y = fit, fill = phobia, color = phobia) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se), alpha = .3) + geom_point() + geom_line() + theme_bw() + labs(x = &quot;Score on Math Quiz&quot;, y = &quot;Estimated Marginal Mean\\nScore on Stat Quiz&quot;, fill = &quot;Self Rated\\nMath Phobia&quot;, color = &quot;Self Rated\\nMath Phobia&quot;) + theme(legend.background = element_rect(color = &quot;black&quot;), legend.position = c(0, 1), legend.justification = c(0, 1)) 3.5 Write-up There is evidence both mathquiz and phobia are associated with statquiz and that the relationship is addative (i.e. no interaction). There is a strong association between math and stats quiz scores, \\(r = .51\\). Math phobia is associated with lower math, \\(r = -.28\\), and stats quiz scores, \\(r = -.36\\). When considered togehter, the combined effects of math phobia and math score account for 31% of the variance in statistical achievement. Not surprizingly, while higher self-reported math phobia was associated with lower statists scores, \\(b = -0.162\\), \\(p=.018\\), \\(95CI = [-0.29, -0.03]\\), higher math quiz scores were associated with higher stats score, \\(b = -0.081\\), \\(p&lt;.001\\), \\(95CI = [0.05, 0.12]\\). There was no evidence that math phobia moderated the relationship between math and quiz performance, \\(p=.377\\). "],
["references.html", "References", " References "]
]
