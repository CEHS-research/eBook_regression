[
["index.html", "Encyclopedia of Quantitative Methods in R, vol. 4: Multiple Linear Regression Welcome Blocked Notes Code and Output The Authors", " Encyclopedia of Quantitative Methods in R, vol. 4: Multiple Linear Regression Sarah Schwartz &amp; Tyson Barrett Last updated: 2018-09-07 Welcome Backgroup and links to other volumes of this encyclopedia may be found at the Encyclopedia’s Home Website. Blocked Notes Thoughout all the eBooks in this encyclopedia, several small secitons will be blocked out in the following ways: These blocks denote an area UNDER CONSTRUCTION, so check back often. This massive undertaking started during the summer of 2018 and is far from complete. The outline of seven volumes is given above despite any one being complete. Feedback is welcome via either author’s email. These blocks denote something EXTREMELY IMPORTANT. Do NOT skip these notes as they will be used very sparingly. These blocks denote something to DOWNLOAD. This may include software installations, example datasets, or notebook code files. These blocks denote something INTERESTING. These point out information we found of interest or added value. These blocks denote LINKS to other websites. This may include instructional video clips, articles, or blog posts. We are all about NOT re-creating the wheel. If somebody else has described or illustrated a topic well, we celebrate it! Code and Output This is how \\(R\\) code is shown: 1 + 1 This is what the output of the \\(R\\) code above will look: ## [1] 2 The Authors Dr. Sarah Schwartz Dr. Tyson Barrett www.SarahSchwartzStats.com www.TysonBarrett.com Sarah.Schwartz@usu.edu Tyson.Barrett@usu.edu Statistical Consulting Studio Data Science and Discover Unit Why choose R ? Check it out: an article from Fall 2016… No more excuses: R is better than SPSS for psychology undergrads, and students agree FYI This entire encyclopedia is written in \\(R Markdown\\), using \\(R Studio\\) as the text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The book’s source code is hosted on GitHub. If you notice typos or other issues, feel free to email either of the authors. This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. "],
["example-ihnos-experiment.html", "1 Example: Ihno’s Experiment 1.1 Purpose 1.2 Exploratory Data Analysis 1.3 Regression Analysis 1.4 Conclusion 1.5 Write-up", " 1 Example: Ihno’s Experiment library(tidyverse) # super helpful everything! library(haven) # inporting SPSS data files library(furniture) # nice tables of descriptives library(texreg) # nice regression summary tables library(stargazer) # nice tables of descrip and regression library(car) # companion for applied regression library(psych) # lots of handy tools 1.1 Purpose 1.1.1 Research Question Does math phobia moderate the relationship between math and statistics performance? That is, does the assocation between math and stat quiz performance differ at variaous levels of math phobia? 1.1.2 Data Description Inho’s dataset is included in the textbook “Explaining Psychological Statistics” (Cohen 2013) and details regarding the sample and measures is describe in this Encyclopedia’s Vol. 2 - Ihno’s Dataset. data_ihno &lt;- haven::read_spss(&quot;http://www.psych.nyu.edu/cohen/Ihno_dataset.sav&quot;) %&gt;% dplyr::rename_all(tolower) %&gt;% dplyr::mutate(gender = factor(gender, levels = c(1, 2), labels = c(&quot;Female&quot;, &quot;Male&quot;))) %&gt;% dplyr::mutate(major = factor(major, levels = c(1, 2, 3, 4,5), labels = c(&quot;Psychology&quot;, &quot;Premed&quot;, &quot;Biology&quot;, &quot;Sociology&quot;, &quot;Economics&quot;))) %&gt;% dplyr::mutate(reason = factor(reason, levels = c(1, 2, 3), labels = c(&quot;Program requirement&quot;, &quot;Personal interest&quot;, &quot;Advisor recommendation&quot;))) %&gt;% dplyr::mutate(exp_cond = factor(exp_cond, levels = c(1, 2, 3, 4), labels = c(&quot;Easy&quot;, &quot;Moderate&quot;, &quot;Difficult&quot;, &quot;Impossible&quot;))) %&gt;% dplyr::mutate(coffee = factor(coffee, levels = c(0, 1), labels = c(&quot;Not a regular coffee drinker&quot;, &quot;Regularly drinks coffee&quot;))) 1.2 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 1.2.1 Univariate Statistics Summary Statistics for all three variables of interest. data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% data.frame() %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max phobia 100 3.310 2.444 0 1 4 10 mathquiz 85 29.071 9.480 9.000 22.000 35.000 49.000 statquiz 100 6.860 1.700 1 6 8 10 1.2.2 Bivariate Relationships Although categorizing continuous variables results in a loss of information (possible signal or noise), it is often done to investigate relationships in an exploratory way. data_ihno %&gt;% dplyr::mutate(phobia_cut3 = cut(phobia, breaks = c(0, 2, 4, 10), include.lowest = TRUE)) %&gt;% furniture::table1(mathquiz, statquiz, splitby = ~ phobia_cut3, test = TRUE, output = &quot;html&quot;) [0,2] (2,4] (4,10] P-Value n = 39 n = 37 n = 24 mathquiz 0.014 32.6 (8.5) 26.5 (9.8) 26.8 (8.9) statquiz 0.001 7.6 (1.3) 6.6 (1.6) 6.1 (2.0) data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% data.frame() %&gt;% psych::pairs.panels(lm = TRUE, ci = TRUE, stars = TRUE) When two variables are both continuous, correlations (Pearson’s \\(R\\)) are an important measure of association. Notice the discrepincy between the correlation between statquiz and phobia. Above, the psych::pairs.panels() function uses pairwise complete cases by default, so \\(r=-.39\\) is computed on all \\(n=100\\) subjects. Below, we specified use = &quot;complete.obs&quot; in the cor() fucntion, so all correlations will be based on the same \\(n=85\\) students, making it listwise complete. The choice of which method to you will vary by situation. data_ihno %&gt;% dplyr::select(phobia, mathquiz, statquiz) %&gt;% cor(use = &quot;complete.obs&quot;) %&gt;% corrplot::corrplot.mixed(lower = &quot;ellipse&quot;, upper = &quot;number&quot;, tl.col = &quot;black&quot;) 1.3 Regression Analysis 1.3.1 Subset the Sample All regression models can only be fit to complete observations regarding the variables included in the model (dependent and independent). Removing any case that is incomplete with respect to even one variables is called “list-wise deletion”. In this analysis, models including the mathquiz variable will be fit on only 85 students (sincle 15 students did not take the math quiz), where as models not including this variable will be fit to all 100 studnets. This complicates model comparisons, which require nested models be fit to the same data (exactly). For this reason, the dataset has been reduced to the subset of students that are complete regarding the three variables utilized throughout the set of five nested models. data_ihno_fitting &lt;- data_ihno %&gt;% dplyr::filter(complete.cases(mathquiz, statquiz, phobia)) dim(data_ihno_fitting) [1] 85 18 1.3.2 Fit Nested Models The bottom-up approach consists of starting with an initial NULL model with only an intercept term and them building additional models that are nested. Two models are considered nested if one is conains a subset of the terms (predictors or IV) compared to the other. fit_ihno_lm_0 &lt;- lm(statquiz ~ 1, # null model: intercept only data = data_ihno_fitting) fit_ihno_lm_1 &lt;- lm(statquiz ~ mathquiz, # only main effect of mathquiz data = data_ihno_fitting) fit_ihno_lm_2 &lt;- lm(statquiz ~ phobia, # only mian effect of phobia data = data_ihno_fitting) fit_ihno_lm_3 &lt;- lm(statquiz ~ mathquiz + phobia, # both main effects data = data_ihno_fitting) fit_ihno_lm_4 &lt;- lm(statquiz ~ mathquiz*phobia, # additional interaction data = data_ihno_fitting) 1.3.3 Comparing Nested Models 1.3.3.1 Model Comparison Table In single level, multiple linear regression significance of predictors (independent variables, IV) is usually based on both the Wald tests of significance for each beta estimate (shown with stars here) and comparisons in the model fit via the \\(R^2\\) values. texreg::htmlreg(list(fit_ihno_lm_0, fit_ihno_lm_1, fit_ihno_lm_2, fit_ihno_lm_3, fit_ihno_lm_4), custom.model.names = c(&quot;No Predictors&quot;, &quot;Only Math Quiz&quot;, &quot;Only Phobia&quot;, &quot;Both IVs&quot;, &quot;Add Interaction&quot;)) Statistical models No Predictors Only Math Quiz Only Phobia Both IVs Add Interaction (Intercept) 6.85*** 4.14*** 7.65*** 5.02*** 5.60*** (0.19) (0.53) (0.29) (0.63) (0.91) mathquiz 0.09*** 0.08*** 0.06* (0.02) (0.02) (0.03) phobia -0.25*** -0.16* -0.34 (0.07) (0.07) (0.21) mathquiz:phobia 0.01 (0.01) R2 0.00 0.26 0.13 0.31 0.31 Adj. R2 0.00 0.25 0.12 0.29 0.29 Num. obs. 85 85 85 85 85 RMSE 1.74 1.50 1.63 1.46 1.46 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 1.3.3.2 Likelihood Ratio Test of Nested Models An alternative method for determing model fit and variable importance is the likelihood ratio test. This involves comparing the \\(-2LL\\) or inverse of twice the log of the likelihood value for the model. The difference in these values follows a Chi Squared distribution with degrees of freedom equal to the difference in the number of parameters estimated (number of betas). Test the main effect of math quiz: anova(fit_ihno_lm_0, fit_ihno_lm_1) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 84 253. NA NA NA NA 2 83 188. 1 65.3 28.8 0.000000700 Test the main effect of math phobia anova(fit_ihno_lm_0, fit_ihno_lm_2) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 84 253. NA NA NA NA 2 83 221. 1 32.3 12.1 0.000791 Test the main effect of math phobia, after controlling for math test anova(fit_ihno_lm_1, fit_ihno_lm_3) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 83 188. NA NA NA NA 2 82 175. 1 12.6 5.88 0.0175 Test the interaction between math test and math phobia (i.e. moderation) anova(fit_ihno_lm_3, fit_ihno_lm_4) # A tibble: 2 x 6 Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 82 175. NA NA NA NA 2 81 173. 1 1.69 0.789 0.377 1.3.4 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_ihno_lm_3, which = 1) plot(fit_ihno_lm_3, which = 2) car::residualPlots(fit_ihno_lm_3) Test stat Pr(&gt;|Test stat|) mathquiz -1.7778 0.07918 . phobia 0.5004 0.61813 Tukey test -1.5749 0.11527 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(fit_ihno_lm_3) Call: lm(formula = statquiz ~ mathquiz + phobia, data = data_ihno_fitting) Residuals: Min 1Q Median 3Q Max -4.3436 -0.8527 0.2805 0.9857 2.7370 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.01860 0.62791 7.993 7.23e-12 *** mathquiz 0.08097 0.01754 4.617 1.42e-05 *** phobia -0.16176 0.06670 -2.425 0.0175 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.462 on 82 degrees of freedom Multiple R-squared: 0.3076, Adjusted R-squared: 0.2907 F-statistic: 18.21 on 2 and 82 DF, p-value: 2.849e-07 summary(fit_ihno_lm_4) Call: lm(formula = statquiz ~ mathquiz * phobia, data = data_ihno_fitting) Residuals: Min 1Q Median 3Q Max -4.1634 -0.8433 0.2832 0.9685 2.9434 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.600183 0.907824 6.169 2.57e-08 *** mathquiz 0.061216 0.028334 2.161 0.0337 * phobia -0.339426 0.210907 -1.609 0.1114 mathquiz:phobia 0.006485 0.007303 0.888 0.3771 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.464 on 81 degrees of freedom Multiple R-squared: 0.3143, Adjusted R-squared: 0.2889 F-statistic: 12.37 on 3 and 81 DF, p-value: 9.637e-07 1.4 Conclusion 1.4.1 Tabulate the Final Model Summary Many journals prefer that regression tables include 95% confidence intervals, rater than standard errors for the beta estimates. The texreg package contains three version of the regression table function. screenreg() Use when working on a project and viewing tables on your computer screen htmlreg() Use when knitting your .Rmd file to a .html document texreg() Use when knitting your .Rmd file to a .pdf via LaTeX texreg::htmlreg(fit_ihno_lm_3, custom.model.names = &quot;Main Effects Model&quot;, ci.force = TRUE, # request 95% conf interv caption = &quot;Final Model for Stat&#39;s Quiz&quot;, single.row = TRUE) Final Model for Stat’s Quiz Main Effects Model (Intercept) 5.02 [3.79; 6.25]* mathquiz 0.08 [0.05; 0.12]* phobia -0.16 [-0.29; -0.03]* R2 0.31 Adj. R2 0.29 Num. obs. 85 RMSE 1.46 * 0 outside the confidence interval 1.4.2 Plot the Model When a model only contains main effects, a plot is not important for interpretation, but can help understand the relationship between multiple predictors. When plotting a regression model the outcome (dependent variable) is always on the y-axis (fit) and only one predictor (independent variable) may be used on the x-axis. You may incorporate additional predictor using colors, shapes, linetypes, or facets. For these predictors, you will want to specify only 2-4 values for illustration and then declare them as factors prior to plotting. effects::Effect(focal.predictors = c(&quot;mathquiz&quot;, &quot;phobia&quot;), mod = fit_ihno_lm_3, xlevels = list(phobia = c(0, 5, 10))) %&gt;% # values for illustration data.frame %&gt;% dplyr::mutate(phobia = factor(phobia)) %&gt;% # factor for illustration ggplot() + aes(x = mathquiz, y = fit, fill = phobia, color = phobia) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se), alpha = .3) + geom_point() + geom_line() + theme_bw() + labs(x = &quot;Score on Math Quiz&quot;, y = &quot;Estimated Marginal Mean\\nScore on Stat Quiz&quot;, fill = &quot;Self Rated\\nMath Phobia&quot;, color = &quot;Self Rated\\nMath Phobia&quot;) + theme(legend.background = element_rect(color = &quot;black&quot;), legend.position = c(0, 1), legend.justification = c(0, 1)) 1.5 Write-up There is evidence both mathquiz and phobia are associated with statquiz and that the relationship is addative (i.e. no interaction). There is a strong association between math and stats quiz scores, \\(r = .51\\). Math phobia is associated with lower math, \\(r = -.28\\), and stats quiz scores, \\(r = -.36\\). When considered togehter, the combined effects of math phobia and math score account for 31% of the variance in statistical achievement. Not surprizingly, while higher self-reported math phobia was associated with lower statists scores, \\(b = -0.162\\), \\(p=.018\\), \\(95CI = [-0.29, -0.03]\\), higher math quiz scores were associated with higher stats score, \\(b = -0.081\\), \\(p&lt;.001\\), \\(95CI = [0.05, 0.12]\\). There was no evidence that math phobia moderated the relationship between math and quiz performance, \\(p=.377\\). "],
["example-obesity-and-blood-pressure.html", "2 Example: Obesity and Blood Pressure 2.1 Packages 2.2 Purpose 2.3 Exploratory Data Analysis 2.4 Regression Analysis 2.5 Conclusion", " 2 Example: Obesity and Blood Pressure 2.1 Packages library(tidyverse) library(magrittr) library(furniture) library(texreg) library(stargazer) library(psych) library(car) library(gpairs) library(GGally) library(corrplot) library(ggthemes) library(RColorBrewer) library(effects) library(ISwR) # Introduction to Statistics with R (datasets) 2.2 Purpose 2.2.1 Research Question 2.2.2 Data Description This dataset is included in the ISwR package (???), which was a companion to the texbook “Introductory Statistics with R, 2nd ed.” (Dalgaard 2008), although it was first published by Brown and Hollander (1977). To view the documentation for the dataset, type ?bp.obese in the console and enter or search the help tab for `bp.obese’. The bp.obese data frame has 102 rows and 3 columns. It contains data from a random sample of Mexican-American adults in a small California town. This data frame contains the following columns: sex a numeric vector code, 0: male, 1: female obese a numeric vector, ratio of actual weight to ideal weight from New York Metropolitan Life Tables bp a numeric vector,systolic blood pressure (mm Hg) data(bp.obese) bp.obese &lt;- bp.obese %&gt;% dplyr::mutate(sex = factor(sex, labels = c(&quot;Male&quot;, &quot;Female&quot;))) glimpse(bp.obese) ## Observations: 102 ## Variables: 3 ## $ sex &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Ma... ## $ obese &lt;dbl&gt; 1.31, 1.31, 1.19, 1.11, 1.34, 1.17, 1.56, 1.18, 1.04, 1.... ## $ bp &lt;int&gt; 130, 148, 146, 122, 140, 146, 132, 110, 124, 150, 120, 1... 2.3 Exploratory Data Analysis Before embarking on any inferencial anlaysis or modeling, always get familiar with your variables one at a time (univariate), as well as pairwise (bivariate). 2.3.1 Univariate Statistics Summary Statistics for all three variables of interest. bp.obese %&gt;% stargazer::stargazer(type = &quot;html&quot;) Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max obese 102 1.313 0.258 0.810 1.143 1.430 2.390 bp 102 127.020 18.184 94 116 137.5 208 2.3.2 Bivariate Relationships bp.obese %&gt;% furniture::table1(obese, bp, splitby = ~ sex, test = TRUE, output = &quot;html&quot;) Male Female P-Value n = 44 n = 58 obese &lt;.001 1.2 (0.2) 1.4 (0.3) bp 0.646 128.0 (16.6) 126.3 (19.4) GGally::ggpairs(bp.obese, mapping = aes(fill = sex, col = sex, alpha = 0.1), upper = list(continuous = &quot;smooth&quot;, combo = &quot;facethist&quot;, discrete = &quot;ratio&quot;), lower = list(continuous = &quot;cor&quot;, combo = &quot;box&quot;, discrete = &quot;facetbar&quot;), title = &quot;Very Useful for Exploring Data&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. bp.obese %&gt;% ggplot() + aes(x = sex, y = bp, fill = sex) + geom_boxplot(alpha = 0.6) + scale_fill_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;)) + labs(x = &quot;Gender&quot;, y = &quot;Blood Pressure (mmHg)&quot;) + guides(fill = FALSE) + theme_bw() Visual inspection for an interaction (is gender a moderator?) bp.obese %&gt;% ggplot(aes(x = obese, y = bp, color = sex)) + geom_point(size = 3) + geom_smooth(aes(fill = sex), alpha = 0.2, method = &quot;lm&quot;) + scale_color_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;), breaks = c(&quot;male&quot;, &quot;female&quot;), labels = c(&quot;Men&quot;, &quot;Women&quot;)) + scale_fill_manual(values = c(&quot;mediumblue&quot;, &quot;maroon3&quot;), breaks = c(&quot;male&quot;, &quot;female&quot;), labels = c(&quot;Men&quot;, &quot;Women&quot;)) + labs(title = &quot;Does Gender Moderate the Association Between Obesity and Blood Pressure?&quot;, x = &quot;Ratio: Actual Weight vs. Ideal Weight (NYM Life Tables)&quot;, y = &quot;Systolic Blood Pressure (mmHg)&quot;) + theme_bw() + scale_x_continuous(breaks = seq(from = 0, to = 3, by = 0.25 )) + scale_y_continuous(breaks = seq(from = 75, to = 300, by = 25)) + theme(legend.title = element_blank(), legend.key = element_rect(fill = &quot;white&quot;), legend.background = element_rect(color = &quot;black&quot;), legend.justification = c(1, 0), legend.position = c(1, 0)) bp.obese %&gt;% dplyr::mutate(sex = as.numeric(sex)) %&gt;% # cor needs only numeric cor() %&gt;% round(3) ## sex obese bp ## sex 1.000 0.405 -0.045 ## obese 0.405 1.000 0.326 ## bp -0.045 0.326 1.000 bp.obese %&gt;% dplyr::mutate(sex = as.numeric(sex)) %&gt;% # cor needs only numeric cor() %&gt;% corrplot::corrplot.mixed(lower = &quot;ellipse&quot;, upper = &quot;number&quot;, tl.col = &quot;black&quot;) 2.4 Regression Analysis 2.4.1 Fit Nested Models The bottom-up approach consists of starting with an initial NULL model with only an intercept term and them building additional models that are nested. Two models are considered nested if one is conains a subset of the terms (predictors or IV) compared to the other. fit_bp_null &lt;- lm(bp ~ 1, data = bp.obese) fit_bp_sex &lt;- lm(bp ~ sex, data = bp.obese) fit_bp_obe &lt;- lm(bp ~ obese, data = bp.obese) fit_bp_obesex &lt;- lm(bp ~ obese + sex, data = bp.obese) fit_bp_inter &lt;- lm(bp ~ obese + sex + obese*sex, data = bp.obese) 2.4.2 Comparing Nested Models 2.4.2.1 Model Comparison Table In single level, multiple linear regression significance of predictors (independent variables, IV) is usually based on both the Wald tests of significance for each beta estimate (shown with stars here) and comparisons in the model fit via the \\(R^2\\) values. texreg::htmlreg(list(fit_bp_null, fit_bp_sex, fit_bp_obe, fit_bp_obesex, fit_bp_inter), custom.model.names = c(&quot;No Predictors&quot;, &quot;Only Sex Quiz&quot;, &quot;Only Obesity&quot;, &quot;Both IVs&quot;, &quot;Add Interaction&quot;)) Statistical models No Predictors Only Sex Quiz Only Obesity Both IVs Add Interaction (Intercept) 127.02*** 127.95*** 96.82*** 93.29*** 102.11*** (1.80) (2.75) (8.92) (8.94) (18.23) sexFemale -1.64 -7.73* -19.60 (3.65) (3.72) (21.66) obese 23.00*** 29.04*** 21.65 (6.67) (7.17) (15.12) obese:sexFemale 9.56 (17.19) R2 0.00 0.00 0.11 0.14 0.15 Adj. R2 0.00 -0.01 0.10 0.13 0.12 Num. obs. 102 102 102 102 102 RMSE 18.18 18.26 17.28 17.00 17.05 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 2.4.2.2 Likelihood Ratio Test of Nested Models An alternative method for determing model fit and variable importance is the likelihood ratio test. This involves comparing the \\(-2LL\\) or inverse of twice the log of the likelihood value for the model. The difference in these values follows a Chi Squared distribution with degrees of freedom equal to the difference in the number of parameters estimated (number of betas). Test the main effect of math quiz: anova(fit_bp_null, fit_bp_sex) ## # A tibble: 2 x 6 ## Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 101 33398. NA NA NA NA ## 2 100 33330. 1 67.6 0.203 0.653 Test the main effect of math phobia anova(fit_bp_null, fit_bp_obe) ## # A tibble: 2 x 6 ## Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 101 33398. NA NA NA NA ## 2 100 29846. 1 3552. 11.9 0.000822 Test the main effect of math phobia, after controlling for math test anova(fit_bp_obe, fit_bp_obesex) ## # A tibble: 2 x 6 ## Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 29846. NA NA NA NA ## 2 99 28595. 1 1250. 4.33 0.0401 Test the interaction between math test and math phobia (i.e. moderation) anova(fit_bp_obesex, fit_bp_inter) ## # A tibble: 2 x 6 ## Res.Df RSS Df `Sum of Sq` F `Pr(&gt;F)` ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 99 28595. NA NA NA NA ## 2 98 28505. 1 89.9 0.309 0.579 2.4.3 Checking Assumptions via Residual Diagnostics Before reporting a model, ALWAYS make sure to check the residules to ensure that the model assumptions are not violated. plot(fit_bp_obesex, which = 1) plot(fit_bp_obesex, which = 4, id.n = 10) # Change the number labeled using the car package car::residualPlots(fit_bp_obesex) ## Test stat Pr(&gt;|Test stat|) ## obese -0.2759 0.7832 ## sex ## Tukey test -0.6141 0.5391 you can adjust any part of a ggplot bp.obese %&gt;% dplyr::mutate(e_bp = resid(fit_bp_obesex)) %&gt;% # add the resid to the dataset ggplot(aes(x = sex, # x-axis variable name y = e_bp, # y-axis variable name color = sex, # color is the outline fill = sex)) + # fill is the inside geom_hline(yintercept = 0, # set at a meaningful value size = 1, # adjust line thickness linetype = &quot;dashed&quot;, # set type of line color = &quot;purple&quot;) + # color of line geom_boxplot(alpha = 0.5) + # level of transparency theme_bw() + # my favorite theme labs(title = &quot;Check Assumptions&quot;, # main title&#39;s text x = &quot;Gender&quot;, # x-axis text label y = &quot;Blood Pressure, Residual (bpm)&quot;) + # y-axis text label scale_y_continuous(breaks = seq(from = -40, # declare a sequence of to = 80, # values to make the by = 20)) + # tick marks at guides(color = FALSE, fill = FALSE) # no legends included bp.obese %&gt;% dplyr::mutate(e_bp = resid(fit_bp_obesex)) %&gt;% # add the resid to the dataset ggplot(aes(x = e_bp, # y-axis variable name color = sex, # color is the outline fill = sex)) + # fill is the inside geom_density(alpha = 0.5) + geom_vline(xintercept = 0, # set at a meaningful value size = 1, # adjust line thickness linetype = &quot;dashed&quot;, # set type of line color = &quot;purple&quot;) + # color of line theme_bw() + # my favorite theme labs(title = &quot;Check Assumptions&quot;, # main title&#39;s text x = &quot;Blood Pressure, Residual (bpm)&quot;) + # y-axis text label scale_x_continuous(breaks = seq(from = -40, # declare a sequence of to = 80, # values to make the by = 20)) # tick marks at 2.5 Conclusion Violations to the assumtions call the reliabity of the regression results into question. The data should be furtherinvestigated, specifically the \\(102^{nd}\\) case. "],
["example-ventricular-shortening-velocity.html", "3 Example: Ventricular shortening velocity 3.1 Packages 3.2 VISUALIZATION: Raw Data 3.3 FIT REGRESSION: simple linear 3.4 VISUALIZATION: Model Fit 3.5 CHECK VALIDITY of the ASSUMPTIONS w/ residual diagnostics 3.6 Plot the model", " 3 Example: Ventricular shortening velocity 3.1 Packages library(tidyverse) library(magrittr) library(furniture) library(texreg) library(stargazer) library(psych) library(car) library(effects) library(ISwR) # Introduction to Statistics with R (datasets) load dataset that is included in the ISwR package data(thuesen) The thuesen data frame has 24 rows and 2 columns. It contains ventricular shortening velocity and blood glucose for type 1 diabetic patients. blood.glucose a numeric vector, fasting blood glucose (mmol/l). short.velocity a numeric vector, mean circumferential shortening velocity (%/s). 3.1.1 Get to know the data with ?thuesen dim(thuesen) # number of rows (subjects) &amp; columns (variables) ## [1] 24 2 names(thuesen) # names of the variables ## [1] &quot;blood.glucose&quot; &quot;short.velocity&quot; glimpse(thuesen) # view the class and 1st few values of each variable ## Observations: 24 ## Variables: 2 ## $ blood.glucose &lt;dbl&gt; 15.3, 10.8, 8.1, 19.5, 7.2, 5.3, 9.3, 11.1, 7.5... ## $ short.velocity &lt;dbl&gt; 1.76, 1.34, 1.27, 1.47, 1.27, 1.49, 1.31, 1.09,... summary(thuesen) # notice a missing value (NA) on velocity ## blood.glucose short.velocity ## Min. : 4.200 Min. :1.030 ## 1st Qu.: 7.075 1st Qu.:1.185 ## Median : 9.400 Median :1.270 ## Mean :10.300 Mean :1.326 ## 3rd Qu.:12.700 3rd Qu.:1.420 ## Max. :19.500 Max. :1.950 ## NA&#39;s :1 stargazer(thuesen, type = &quot;html&quot;, digits = 4, flip = TRUE, summary.stat = c(&quot;n&quot;, &quot;mean&quot;, &quot;sd&quot;, &quot;min&quot;, &quot;median&quot;, &quot;max&quot;), title = &quot;Descriptives&quot;) Descriptives Statistic blood.glucose short.velocity N 24 23 Mean 10.3000 1.3257 St. Dev. 4.3375 0.2329 Min 4.2000 1.0300 Median 9.4000 1.2700 Max 19.5000 1.9500 3.2 VISUALIZATION: Raw Data Base Graphics: let it determine the type of plot ggplot(thuesen, aes(x = blood.glucose, y = short.velocity)) + geom_point() ## Warning: Removed 1 rows containing missing values (geom_point). ggplot2: specify plot type thuesen %&gt;% ggplot(aes(x = blood.glucose, # x-axis variable name y = short.velocity)) + # y-axis variable name geom_point() + # scatterplot theme_bw() # black-and-white theme ## Warning: Removed 1 rows containing missing values (geom_point). 3.2.1 CORRELATION: un-adjusted cor doesn’t like NA values thuesen %&gt;% cor ## blood.glucose short.velocity ## blood.glucose 1 NA ## short.velocity NA 1 specify to do listwise deletion thuesen %&gt;% cor(use = &quot;complete.obs&quot;) ## blood.glucose short.velocity ## blood.glucose 1.0000000 0.4167546 ## short.velocity 0.4167546 1.0000000 you can abbreviate thuesen %&gt;% cor(use = &quot;complete&quot;) # ## blood.glucose short.velocity ## blood.glucose 1.0000000 0.4167546 ## short.velocity 0.4167546 1.0000000 same as above, but without the pipe cor(thuesen, use = &quot;complete&quot;) ## blood.glucose short.velocity ## blood.glucose 1.0000000 0.4167546 ## short.velocity 0.4167546 1.0000000 you can choose the number of decimal places thuesen %&gt;% cor(use=&quot;complete&quot;) %&gt;% round(2) ## blood.glucose short.velocity ## blood.glucose 1.00 0.42 ## short.velocity 0.42 1.00 this version give a single value instead of a matrix thuesen %$% cor(blood.glucose, short.velocity, use=&quot;complete&quot;) ## [1] 0.4167546 This TESTS if the cor == 0 thuesen %$% cor.test(blood.glucose, short.velocity, use = &#39;complete&#39;) ## ## Pearson&#39;s product-moment correlation ## ## data: blood.glucose and short.velocity ## t = 2.101, df = 21, p-value = 0.0479 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.005496682 0.707429479 ## sample estimates: ## cor ## 0.4167546 The default is Pearson’s R, which assesses linear relationships. Spearman’s correlation assesses monotonic relationships. thuesen %$% cor.test(blood.glucose, short.velocity, use = &#39;complete&#39;, method = &#39;spearman&#39;) # spearman&#39;s (rho) ## Warning in cor.test.default(blood.glucose, short.velocity, use = ## &quot;complete&quot;, : Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: blood.glucose and short.velocity ## S = 1380.4, p-value = 0.1392 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.318002 3.3 FIT REGRESSION: simple linear fit_vel_glu &lt;- lm(short.velocity ~ blood.glucose, data = thuesen) fit_vel_glu ## ## Call: ## lm(formula = short.velocity ~ blood.glucose, data = thuesen) ## ## Coefficients: ## (Intercept) blood.glucose ## 1.09781 0.02196 summary(fit_vel_glu) ## ## Call: ## lm(formula = short.velocity ~ blood.glucose, data = thuesen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.40141 -0.14760 -0.02202 0.03001 0.43490 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.09781 0.11748 9.345 6.26e-09 *** ## blood.glucose 0.02196 0.01045 2.101 0.0479 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2167 on 21 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.1737, Adjusted R-squared: 0.1343 ## F-statistic: 4.414 on 1 and 21 DF, p-value: 0.0479 coef(fit_vel_glu) ## (Intercept) blood.glucose ## 1.09781488 0.02196252 confint(fit_vel_glu) ## 2.5 % 97.5 % ## (Intercept) 0.8534993816 1.34213037 ## blood.glucose 0.0002231077 0.04370194 anova(fit_vel_glu) ## # A tibble: 2 x 5 ## Df `Sum Sq` `Mean Sq` `F value` `Pr(&gt;F)` ## * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.207 0.207 4.41 0.0479 ## 2 21 0.986 0.0470 NA NA model fit indicies logLik(fit_vel_glu) ## &#39;log Lik.&#39; 3.583612 (df=3) AIC(fit_vel_glu) ## [1] -1.167223 BIC(fit_vel_glu) ## [1] 2.239259 stargazer(fit_vel_glu, type = &quot;html&quot;) Dependent variable: short.velocity blood.glucose 0.022** (0.010) Constant 1.098*** (0.117) Observations 23 R2 0.174 Adjusted R2 0.134 Residual Std. Error 0.217 (df = 21) F Statistic 4.414** (df = 1; 21) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 texreg::htmlreg(fit_vel_glu) Statistical models Model 1 (Intercept) 1.10*** (0.12) blood.glucose 0.02* (0.01) R2 0.17 Adj. R2 0.13 Num. obs. 23 RMSE 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.4 VISUALIZATION: Model Fit Base Graphics: let it determine the type of plot ?plot.lm show all plots at once par(mfrow = c(2, 3)) plot(fit_vel_glu, which = 1:6) par(mfrow = c(1, 1)) potentially influencial or outlier points thuesen %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::filter(id == c(13, 20, 24)) ## # A tibble: 3 x 3 ## blood.glucose short.velocity id ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 19 1.95 13 ## 2 16.1 1.05 20 ## 3 9.5 1.7 24 ggplot2: specify plot type thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases ggplot(data = , # name the dataset 1st mapping = aes(x = blood.glucose, # x-axis variable name y = short.velocity)) + # y-axis variable name geom_point() + # do a scatterplot stat_smooth(method = &quot;lm&quot;) + # smooth: linear model theme_bw() + # black-and-while theme geom_point(data = thuesen %&gt;% filter(row_number() == c(13, 20, 24)), # pch = 19, size = 4, color = &quot;red&quot;) 3.5 CHECK VALIDITY of the ASSUMPTIONS w/ residual diagnostics store values from the model (into the dataset) thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) # residual values ## # A tibble: 23 x 4 ## blood.glucose short.velocity pred resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.3 1.76 1.43 0.326 ## 2 10.8 1.34 1.34 0.00499 ## 3 8.1 1.27 1.28 -0.00571 ## 4 19.5 1.47 1.53 -0.0561 ## 5 7.2 1.27 1.26 0.0141 ## 6 5.3 1.49 1.21 0.276 ## 7 9.3 1.31 1.30 0.00793 ## 8 11.1 1.09 1.34 -0.252 ## 9 7.5 1.18 1.26 -0.0825 ## 10 12.2 1.22 1.37 -0.146 ## # ... with 13 more rows thuesen %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) %&gt;% # residual values ggplot(aes(x = id, y = resid)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, size = 1, linetype = &quot;dashed&quot;) + theme_classic() + labs(title = &quot;Looking for homogeneity of residuals&quot;, subtitle = &quot;want to see equal spread all across&quot;) thuesen %&gt;% dplyr::filter(complete.cases(.)) %&gt;% # get ride fo the incomplete cases dplyr::mutate(pred = fitted(fit_vel_glu)) %&gt;% # fitted/prediction values dplyr::mutate(resid = residuals(fit_vel_glu)) %&gt;% # residual values ggplot(aes(x = resid)) + geom_histogram(bins = 12, color = &quot;blue&quot;, fill = &quot;blue&quot;, alpha = 0.3) + geom_vline(xintercept = 0, size = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + theme_classic() + labs(title = &quot;Looking for normality of residuals&quot;, subtitle = &quot;want to see roughly a bell curve&quot;) the residual plots residualPlots(fit_vel_glu) ## Test stat Pr(&gt;|Test stat|) ## blood.glucose 0.9289 0.3640 ## Tukey test 0.9289 0.3529 ggplot2: plotting confidence intervals for the mean outcome ggplot(thuesen, aes(x = blood.glucose, y = short.velocity)) + stat_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;) + geom_point(shape = 10, size = 2) + theme_bw() ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). 3.6 Plot the model effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu) ## ## blood.glucose effect ## blood.glucose ## 4.2 8 12 16 20 ## 1.190057 1.273515 1.361365 1.449215 1.537065 effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu) %&gt;% data.frame() ## # A tibble: 5 x 5 ## blood.glucose fit se lower upper ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4.2 1.19 0.0788 1.03 1.35 ## 2 8 1.27 0.0516 1.17 1.38 ## 3 12 1.36 0.0483 1.26 1.46 ## 4 16 1.45 0.0742 1.29 1.60 ## 5 20 1.54 0.110 1.31 1.77 effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = c(5, 10, 15, 20))) %&gt;% data.frame() ## # A tibble: 4 x 5 ## blood.glucose fit se lower upper ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 1.21 0.0721 1.06 1.36 ## 2 10 1.32 0.0454 1.22 1.41 ## 3 15 1.43 0.0662 1.29 1.56 ## 4 20 1.54 0.110 1.31 1.77 effects::Effect(focal.predictors = c(&quot;blood.glucose&quot;), mod = fit_vel_glu, xlevels = list(blood.glucose = seq(from = 4, to = 20, by = 1))) %&gt;% data.frame() %&gt;% ggplot(aes(x = blood.glucose, y = fit)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .5) + geom_line() "],
["references.html", "References", " References "]
]
