# Logistic Regression - Ex: Bronchopulmonary Dysplasia in Premature Infants

example walk through:

https://stats.idre.ucla.edu/r/dae/logit-regression/


info:

https://onlinecourses.science.psu.edu/stat504/node/216/ 

`sjPlot::tab_model` (HTML only)

http://www.strengejacke.de/sjPlot/articles/sjtlm.html#changing-summary-style-and-content 

`finafit`

https://www.r-bloggers.com/elegant-regression-results-tables-and-plots-in-r-the-finalfit-package/

```{r, include=FALSE}
knitr::opts_chunk$set(comment     = "",
                      echo        = TRUE, 
                      warning     = FALSE, 
                      message     = FALSE,
                      fig.align   = "center", # center all figures
                      fig.width   = 6,        # set default figure width to 4 inches
                      fig.height  = 4)        # set default figure height to 3 inches
```


Install a package Dr. Schwartz wrote:

```{r}
remotes::install_github("sarbearschwartz/texreghelpr")
```



```{r, message=FALSE, error=FALSE}
library(tidyverse)
library(haven)        # read in SPSS dataset
library(furniture)    # nice table1() descriptives
library(stargazer)    # display nice tables: summary & regression
library(texreg)       # Convert Regression Output to LaTeX or HTML tables
library(texreghelpr)  # Dr. Schwartz's helper funtcions for texreg tables
library(psych)        # contains some useful functions, like headTail
library(car)          # Companion to Applied Regression
library(pscl)         # psudo R-squared function
library(interactions) # interaction plots
library(sjPlot)       # various plots
library(performance)  # r-squared values
```

## Background

Simple example demonstrating basic modeling approach:  Data on **Bronchopulmonary Dysplasia** (BPD) from 223 low birth weight infants (weighing less than 1750 grams).


### Source

Data courtesy of Dr. Linda Van Marter.

### Reference

Van Marter, L.J., Leviton, A., Kuban, K.C.K., Pagano, M. & Allred, E.N. (1990).  *Maternal glucocorticoid therapy and reduced risk of bronchopulmonary dysplasia.* Pediatrics, 86, 331-336.

The data are from a study of low birth weight infants in a neonatal intensive care unit. The study was designed to examine the development of **bronchopulmonary dysplasia (BPD)**, a chronic lung disease, in a sample of 223 infants weighing less than 1750 grams. The response variable is binary, denoting whether an infant develops BPD by day 28 of life (where BPD is defined by both oxygen requirement and compatible chest radiograph).

### Variables    

* `bpd` 0 = no, 1 = yes    
* `brthwght` number of grams    
* `gestage` number of weeks    
* `toxemia` in mother, 0 = no, 1 = yes    

```{r}
bpd_raw <- read.table("https://raw.githubusercontent.com/CEHS-research/data/master/Regression/VanMarter_%20BPD.txt", 
                      header      = TRUE, 
                      strip.white = TRUE)
```

```{r}
n <- nrow(bpd_raw)
n
```


```{r}
tibble::glimpse(bpd_raw)
```

```{r}
head(bpd_raw)
```

> Note: For logistic regression, you need to leave the outcome (dependent variable) coded as zeros `0` and ones `1` and NOT apply lables.  You do want to apply labels to factors that function as predictors (independent varaibles).

```{r}
bpd_clean <- bpd_raw %>% 
  dplyr::mutate(toxemia = factor(toxemia,
                                 levels = c(0, 1),
                                 labels = c("No", "Yes")))
```


```{r}
summary(bpd_clean)
```




## Logistic Regresion: Fit the Model to the data

Instead of using the `lm()` function from base R, you use `glm()`.  You also need to add an option to specify which generalization you want to use.  To do logistic regression for a binary outcome, use `family = binomial(link = "logit")`.

### Null Model: no independent variables

```{r}
fit_glm_0 <- glm(bpd ~ 1, 
                 data = bpd_clean, 
                 family = binomial(link = "logit"))

summary(fit_glm_0)
```

### Main Effects Model: add 3 predictors

> Note: Since the unites of weight are so small, the estimated parameter will be super small.  To offset the small units, we can re-scale the weights by dividing the grams by 100 to create "hectograms".

```{r}
fit_glm_1 <- glm(bpd ~ I(brthwght/100) + gestage + toxemia, 
                 data = bpd_clean, 
                 family = binomial(link = "logit")) 

summary(fit_glm_1)
```




## Model Fit


### Log Likelihood and Deviance

```{r}
logLik(fit_glm_0) # Null Model
```


```{r}
logLik(fit_glm_1) # Full Model
```

> Note: Deviance = -2 times the Log Likelihood

```{r}
deviance(fit_glm_0) # Null Model
```


```{r}
deviance(fit_glm_1) # Full Model
```

### AIC and BIC


```{r}
AIC(fit_glm_0, fit_glm_1)  # Full Model
```


```{r}
BIC(fit_glm_0, fit_glm_1)  # Full Model
```





## Variance Explained

### Many Options

> Technically, $R^2$ cannot be computed the same way in logistic regression as it is in OLS regression. There are several (over 10) alternatives that endever to calculate a similar metric in different ways.  


```{block type='rmdlink', echo=TRUE}
Website: Statistical Horizons

Author: [Paul Allison](https://statisticalhorizons.com/our-instructors/paul-allison)

Blog Post: [What’s the Best R-Squared for Logistic Regression?](https://statisticalhorizons.com/r2logistic)

> Compares and contrasts different options and his/our progression through them, in which he now prefers Tjur’s statistic (pronounced “choor”). 

> Great Quote: "For those who want an R^2 that behaves like a linear-model R^2, this is deeply unsettling."

> Note: Dr. Allison is very active at answering questions in the comments of this post.

```


```{block type='rmdlink', echo=TRUE}
Website: UCLA [Institute for Digital Research and Education (IDRE)](https://stats.idre.ucla.edu/)

Article: [FAQ: WHAT ARE PSEUDO R-SQUAREDS?](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/)

> Describes several of the most comment R-squared type measures for logistic regression (with Stata).
```


```{block type='rmdlink', echo=TRUE}
Website: [The Stats Geek](https://thestatsgeek.com/) 

Author: [Jonathan Bartlett](https://thestatsgeek.com/about-thestatsgeek-com/), Department of Mathematical Sciences, University of Bath and Associate Editor for the journal Biometrics

Blog Post: [2014: R squared in logistic regression](http://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/)

> Focus on McFadden's pseudo-R squared, in R.
```



### McFadden's pseud-R^2

McFadden's $pseudo-R^2$, in logistic regression, is defined as $1−\frac{L_1}{L_0}$, where $L_0$ represents the log likelihood for the "constant-only" or 

$$
R^2_{McF} = 1 - \frac{L_1}{L_0}
$$

```{r}
MFR2 <- 1 - (logLik(fit_glm_1)/logLik(fit_glm_0))
MFR2
```

```{r}
performance::r2_mcfadden(fit_glm_1)
```


### Cox & Snell

$l = e^{L}$, since $L$ is the log of the likelihood and $l$ is the likelihood...$log(l) = L$

$$
R^2_{CS} = 1 - \Bigg( \frac{l_0}{l_1} \Bigg) ^{2 \backslash n} \\
n = \text{sample size}
$$



```{r}
CSR2 <- 1 - (exp(logLik(fit_glm_0))/exp(logLik(fit_glm_1)))^(2/n)
CSR2
```


```{r}
performance::r2_coxsnell(fit_glm_1)
```



### Nagelkerke or Cragg and Uhler's

$$
R^2_{Nag} = \frac{1 - \Bigg( \frac{l_0}{l_1} \Bigg) ^{2 \backslash n}}
                 {1 - \Big( l_0 \Big) ^{2 \backslash n}}
$$

```{r}
NR2 <- CSR2 / (1 - exp(logLik(fit_glm_0))^(2/n))
NR2 
```

```{r}
performance::r2_nagelkerke(fit_glm_1)
```



### Tjur's statistic


```{r}
performance::r2(fit_glm_1)
```


### Several at Once

> the `pscl::pR2()` function 

Outputs:

* `llh` The log-likelihood from the fitted model
* `llhNull` The log-likelihood from the intercept-only restricted model
* `G2` Minus two times the difference in the log-likelihoods
* `McFadden` McFadden's pseudo r-squared
* `r2ML` Maximum likelihood pseudo r-squared
* `r2CU` Cragg and Uhler's pseudo r-squared

```{r}
pscl::pR2(fit_glm_1)
```



## Model Compairisons, Inferential

### Likelihood Ratio Test (LRT, aka. Deviance Difference Test)

```{r}
anova(fit_glm_0, fit_glm_1, test = "LRT")
```


### Bayes Factor and Performance Score


```{r}
performance::compare_performance(fit_glm_0, fit_glm_1, rank = TRUE)
```




## Parameter Estimates

### Link: Logit Scale

```{r}
fit_glm_1 %>% coef()
```


```{r}
fit_glm_1 %>% confint()
```

### Exponentiate: Odds Ratio Scale

```{r}
fit_glm_1 %>% coef() %>% exp()
```


```{r}
fit_glm_1 %>% confint() %>% exp()
```




## Significance of Terms

### Wald's $t$-Test

```{r}
fit_glm_1 %>% summary() %>% coef()
```



### Single term deletion, $\chi^2$ LRT

> Note: Significance of each variable is assessed by comparing it to the model that drops just that one term (`type = 3`); order doesn't matter.

```{r}
drop1(fit_glm_1, type = 3, test = "LRT")
```


### Sequential addition, $\chi^2$  LRT

> Note: Signifcance of each additional variable at a time; ordered first to last

```{r}
anova(fit_glm_1, test = "LRT")
```





## Parameter Estimate Tables


### Logit scale (Link, default)

```{r, results='asis'}
texreg::knitreg(fit_glm_1,
                single.row = TRUE)
```

> Note: You may request: Confidence Intervals on Logit scale with the options: `ci.force = TRUE, ci.test = 1`

```{r, results='asis'}
texreg::knitreg(fit_glm_1,
                single.row = TRUE,
                ci.force = TRUE,
                ci.test = 1,
                digits = 6)
```


### Odds-Ratio Scale (exponentiate)

```{r, results='asis'}
texreg::knitreg(texreghelpr::extract_glm_exp(fit_glm_1),
                single.row = TRUE)
```


### BOTH: Logit and Odds-Ratio

```{r, results='asis'}
texreg::knitreg(list(fit_glm_1,
                     texreghelpr::extract_glm_exp(fit_glm_1,
                                                  include.aic = FALSE,
                                                  include.bic = FALSE,
                                                  include.loglik = FALSE,
                                                  include.deviance = FALSE,
                                                  include.nobs = FALSE)),
                custom.model.names = c("b (SE)",
                                       "OR [95% CI]"),
                single.row = TRUE,
                ci.test = 1)
```


## Marginal or Predicted Values 

### Across All Predictors

> Note: By default it will select 5-6 "nice" values for each continuous variable.  All levels of categorical factors will be included.

```{r}
effects::Effect(focal.predictors = c("brthwght", "gestage", "toxemia"),
                mod = fit_glm_1)
```

### Specify Some Predictors

> Note: if a predictor is left off the`focal.predictors`, the predictions are AVERAGED over that variable.

```{r}
effects::Effect(focal.predictors = c("brthwght", "gestage"),
                mod = fit_glm_1)
```

```{r}
effects::Effect(focal.predictors = c("brthwght", "toxemia"),
                mod = fit_glm_1)
```

### Set a constant (fixed) value for a predictor(s)

```{r}
effects::Effect(focal.predictors = c("brthwght"),
                fixed.predictors = list(gestage = 34, 
                                        toxemia = "no"),
                mod = fit_glm_1)
```

### Set values for a continuous predictor 

```{r}
effects::Effect(focal.predictors = c("brthwght", "gestage"),
                fixed.predictors = list(toxemia = "no"),
                xlevels = list(gestage = c(24, 32, 36)),
                mod = fit_glm_1)
```


### Add SE and 95% Confidence Interval

```{r}
effects::Effect(focal.predictors = c("brthwght", "gestage", "toxemia"),
                xlevels = list(brthwght = c(500, 1000, 1500),
                               gestage = c(30, 36)),
                mod = fit_glm_1) %>% 
  data.frame()
```





## Marginal Model Plots


### Individual Marginal Plots for one IV, individually

> The `sjPlot::plot_model()` function automatically transforms the predictions to the probability score when you include the `type = "pred"` option.


```{r}
sjPlot::plot_model(fit_glm_1,
                   type = "pred")
```


### Combination Marginal Plots for two-three IVs, all at once

For continuous IV that are not on the x-axis (`pred`), be default three values will be selected: the mean and plus-or-minus one stadard error for the mean (SEM).


> The `outcome.scale = "link"` option plots the LOGIT scale on the y-axis.

```{r}
interactions::interact_plot(model = fit_glm_1,
                            pred = brthwght,
                            modx = gestage,
                            mod2 = toxemia,
                            outcome.scale = "link")
```

Alternatively, you may use the `modx.labels` option to set specific values at which to plot the moderator.

> The `outcome.scale = "response"` option plots the PROBABILITY scale on the y-axis.


```{r}
interactions::interact_plot(model = fit_glm_1,
                            pred = brthwght,
                            modx = gestage,
                            modx.labels = c(28, 32, 36),
                            mod2 = toxemia,
                            outcome.scale = "response")
```


You can always do more work to get to a PUBLISH-ABLE version

```{r}
interactions::interact_plot(model = fit_glm_1,
                            pred = brthwght,
                            modx = gestage,
                            modx.labels = c(28, 30, 32),
                            mod2 = toxemia,
                            outcome.scale = "response",
                            x.label = "Birthweight, grams",
                            y.label = "Probability of Bronchopulmonary Dysplasia",
                            legend.main = "Gestational Age, weeks",
                            mod2.label = c("Mother does NOT have Toxemia",
                                           "Mother DOES have Toxemia"),
                            colors = rep("black", 3)) +
  geom_hline(yintercept = .5, alpha = .2) +
  theme_bw() +
  theme(legend.background = element_rect(color = "black"),
        legend.position = c(1, 1),
        legend.justification = c(1.1, 1.1),
        legend.key.width = unit(2, "cm"))
```


### Total Control is also available 

```{r}
effects::Effect(focal.predictors = c("brthwght", "toxemia", "gestage"),
                mod = fit_glm_1,
                xlevels = list(brthwght = seq(from = 450, to = 1730, by = 10),
                               gestage = c(28, 32, 36))) %>% 
  data.frame() %>% 
  dplyr::mutate(gestage = factor(gestage)) %>% 
  ggplot(aes(x = brthwght,
             y = fit)) +
  geom_ribbon(aes(ymin = lower,
                  ymax = upper,
                  fill = toxemia),
              alpha = .2) +
  geom_line(aes(linetype = toxemia,
                color = toxemia),
            size = 1) +
  facet_grid(. ~ gestage, labeller = label_both) +
  theme_bw()
```


```{r}
effects::Effect(focal.predictors = c("brthwght", "toxemia", "gestage"),
                mod = fit_glm_1,
                xlevels = list(brthwght = seq(from = 450, to = 1730, by = 10),
                               gestage = c(28, 32, 36))) %>% 
  data.frame() %>% 
  dplyr::mutate(gestage = factor(gestage)) %>% 
  ggplot(aes(x = brthwght,
             y = fit)) +
  geom_line(aes(linetype = toxemia,
                color = toxemia),
            size = 1) +
  facet_grid(. ~ gestage, labeller = label_both) +
  theme_bw()
```







