# Logistic Regression - Ex: Depression (Hoffman)

```{r, include=FALSE}
knitr::opts_chunk$set(comment     = "",
                      echo        = TRUE, 
                      warning     = FALSE, 
                      message     = FALSE,
                      fig.align   = "center", # center all figures
                      fig.width   = 6,        # set default figure width to 4 inches
                      fig.height  = 4)        # set default figure height to 3 inches
```

```{r, message=FALSE, error=FALSE}
library(tidyverse)
library(haven)        # read in SPSS dataset
library(furniture)    # nice table1() descriptives
library(stargazer)    # display nice tables: summary & regression
library(texreg)       # Convert Regression Output to LaTeX or HTML Tables
library(texreghelpr)  # GITHUB: sarbearschartz/texreghelpr
library(psych)        # contains some useful functions, like headTail
library(car)          # Companion to Applied Regression
library(sjPlot)       # Quick plots and tables for models
library(pscl)         # psudo R-squared function
library(glue)         # Interpreted String Literals 
library(interactions) # interaction plots
library(sjPlot)       # various plots
library(performance)  # r-squared values
```


This dataset comes from John Hoffman's textbook: Regression Models for Categorical, Count, and Related Variables: An Applied Approach (2004) [Amazon link, 2014 edition](https://www.amazon.com/Regression-Models-Categorical-Related-Variables/dp/0520289293/ref=sr_1_2?dchild=1&qid=1603172859&refinements=p_27%3ADr.+John+P.+Hoffmann&s=books&sr=1-2&text=Dr.+John+P.+Hoffmann)

> Chapter 3: Logistic and Probit Regression Models 

Dataset:  The following example uses the SPSS data set `Depress.sav`. The dependent variable of interest is a measure of life satisfaction, labeled `satlife`. 

```{r}
df_depress <- haven::read_spss("https://raw.githubusercontent.com/CEHS-research/data/master/Hoffmann_datasets/depress.sav") %>% 
  haven::as_factor()

tibble::glimpse(df_depress)
```


```{r}
psych::headTail(df_depress)
```

```{r}
df_depress %>% 
  dplyr::select(satlife, lifesat) %>% 
  table() %>% 
  addmargins()
```



## Exploratory Data Analysis

Dependent Variable = `satlife` (numeric version) or `lifesat` (factor version)


### Visualize

```{r, fig.cap="Hoffman's Figure 2.3, top of page 46"}
df_depress %>% 
  ggplot(aes(x = age,
             y = satlife)) +
  geom_count() +
  geom_smooth(method = "lm") +
  theme_bw() +
  labs(x = "Age in Years",
       y = "Life Satisfaction, numeric")
```




```{r}
df_depress %>% 
  ggplot(aes(x = age,
             y = satlife)) +
  geom_count() +
  geom_smooth(method = "lm") +
  theme_bw() +
  labs(x = "Age in Years",
       y = "Life Satisfaction, numeric") +
  facet_grid(~ sex) +
  theme(legend.position = "bottom")
```


### Summary Table

Independent = `sex`

```{r}
df_depress %>% 
  dplyr::select(lifesat, sex) %>% 
  table() %>% 
  addmargins()
```



```{r}
df_depress %>% 
  dplyr::group_by(sex) %>% 
  furniture::table1(lifesat,
                    total = TRUE,
                    caption = "Hoffman's EXAMPLE 3.1 Cross-Tabulation of Gender and Life Satisfaction (top page 50)",
                    output = "markdown")
```

## Calcualte: Probability, Odds, and Odds-Ratios

### Marginal, over all the sample

> Tally the number of participants happy and not happy (i.e. depressed). 

```{r}
df_depress %>% 
  dplyr::select(satlife) %>% 
  table() %>% 
  addmargins()
```


#### Probability of being happy

$$
prob_{yes} = \frac{n_{yes}}{n_{total}} = \frac{n_{yes}}{n_{yes} + n_{no}}
$$

```{r}
prob <- 52 / 117

prob
```

#### Odds of being happy

$$
odds_{yes} = \frac{n_{yes}}{n_{no}} = \frac{n_{yes}}{n_{total} - n_{yes}}
$$

```{r}
odds <- 52/65

odds
```

$$
odds_{yes} = \frac{prob_{yes}}{prob_{no}} = \frac{prob_{yes}}{1 - prob_{yes}}
$$


```{r}
prob/(1 - prob)
```



### Comparing by Sex

> Cross-tabulate happiness (`satlife`) with `sex` *(male vs. female)*.

```{r}
df_depress %>% 
  dplyr::select(satlife, sex) %>% 
  table() %>% 
  addmargins()
```


#### Probability of being happy, by sex

Reference category = male

```{r}
prob_male <- 14 / 21

prob_male
```

Comparison Category = female

```{r}
prob_female <- 38 / 96

prob_female
```


#### Odds of being happy, by sex

Reference category = male

```{r}
odds_male <- 14 / 7

odds_male
```

Comparison Category = female

```{r}
odds_female <- 38 / 58


odds_female
```

#### Odds-Ratio for sex


$$
OR_{\text{female vs. male}} = \frac{odds_{female}}{odds_{male}}
$$

```{r}
odds_ratio <- odds_female / odds_male

odds_ratio
```

$$
OR_{\text{female vs. male}} = \frac{\frac{prob_{female}}{1 - prob_{female}}}{\frac{prob_{male}}{1 - prob_{male}}}
$$

```{r}
(prob_female / (1 - prob_female)) / (prob_male / (1 - prob_male))
```

$$
OR_{\text{female vs. male}} = \frac{\frac{n_{yes|female}}{n_{no|female}}}{\frac{n_{yes|male}}{n_{no|male}}}
$$

```{r}
((38 / 58)/(14 / 7))
```



## Logisitc Regression Model 1: one IV 


### Fit the Unadjusted Model

```{r}
fit_glm_1 <- glm(satlife ~ sex,
                 data = df_depress,
                 family = binomial(link = "logit"))

fit_glm_1 %>% summary() %>% coef()
```

```{r}
summary(fit_glm_1)
```


### Tabulate Parameters


#### Logit Scale


```{r, results='asis'}
texreg::knitreg(fit_glm_1,
                caption = "Hoffman's EXAMPLE 3.2 A Loistic Regression Model of Gender and Life Satisfaction, top of page 51",
                caption.above = TRUE,
                single.row = TRUE,
                digits = 4)
```

#### Both Logit and Odds-ratio Scales

```{r, results='asis'}
texreg::knitreg(list(fit_glm_1,
                     texreghelpr::extract_glm_exp(fit_glm_1)),
                custom.model.names = c("b (SE)",
                                       "OR [95 CI]"),
                caption = "Hoffman's EXAMPLE 3.2 A Loistic Regression Model of Gender and Life Satisfaction, top of page 51",
                caption.above = TRUE,
                single.row = TRUE,
                digits = 4,
                ci.test = 1)
```


### Assess Model Fit

#### Likelihood Ratio Test (LRT, aka. Deviance Difference Test)

```{r}
drop1(fit_glm_1, test = "LRT")
```



```{r}
performance::compare_performance(fit_glm_1)
```


#### R-squared "like" measures


```{r}
performance::r2(fit_glm_1)
```

```{r}
performance::r2_mcfadden(fit_glm_1)
```

```{r}
performance::r2_nagelkerke(fit_glm_1)
```



```{r}
pscl::pR2(fit_glm_1)
```




### Plot Predicted Probabilitites

```{r}
sjPlot::plot_model(model = fit_glm_1,
                   type = "pred")
```

#### Logit scale

```{r}
fit_glm_1 %>% 
  emmeans::emmeans(~ sex)
```

```{r}
fit_glm_1 %>% 
  emmeans::emmeans(~ sex) %>% 
  pairs()
```


#### Response Scale (probability)

```{r}
fit_glm_1 %>% 
  emmeans::emmeans(~ sex,
                   type = "response")
```

```{r}
fit_glm_1 %>% 
  emmeans::emmeans(~ sex,
                   type = "response") %>% 
  pairs()
```


### Interpretation

* On average, two out of every three males is depressed,  b = 0.667, odds = 1.95, 95% CI [1.58, 2.40].

* Females have nearly a quarter lower odds of being depressed, compared to men, b = -0.27, OR = 0.77, 95% IC [0.61, 0.96], p = .028.


### Diagnostics

#### Influential values

Influential values are extreme individual data points that can alter the quality of the logistic regression model.

The most extreme values in the data can be examined by visualizing the Cookâ€™s distance values. Here we label the top 7 largest values:


```{r}
plot(fit_glm_1, which = 4, id.n = 7)
```

> Note that, not all outliers are influential observations. To check whether the data contains potential influential observations, the standardized residual error can be inspected. Data points with an absolute standardized residuals above 3 represent possible outliers and may deserve closer attention.

#### Standardized Residuals

The following R code computes the standardized residuals (`.std.resid`) using the R function `augment()` [`broom` package].


```{r}
fit_glm_1 %>% 
  broom::augment() %>% 
  ggplot(aes(x = .rownames, .std.resid)) + 
  geom_point(aes(color = sex), alpha = .5) +
  theme_bw()
```






## Logisitic Regression Model 2: many IV's


### Fit the Model

```{r}
fit_glm_2 <- glm(satlife ~ sex + iq + age + weight,
                 data = df_depress,
                 family = binomial(link = "logit"))

fit_glm_2 %>% summary() %>% coef()
```

### Tabulate Parameters

```{r, results='asis'}
texreg::knitreg(list(fit_glm_2,
                     texreghelpr::extract_glm_exp(fit_glm_2)),
                custom.model.names = c("b (SE)",
                                       "OR [95 CI]"),
                caption = "EXAMPLE 3.3 A Logistic Regression Model of Life Satisfaction with Multiple Independent Variables, middle of page 52",
                caption.above = TRUE,
                single.row = TRUE,
                digits = 4,
                ci.test = 1)
```


### Assess Model Fit


```{r}
drop1(fit_glm_2, test = "LRT")
```



### Variance Explained


```{r}
performance::compare_performance(fit_glm_2)
```


#### R-squared "lik" measures

```{r}
performance::r2_nagelkerke(fit_glm_2)
```

### Diagnostics

#### Multicollinearity

Multicollinearity corresponds to a situation where the data contain highly correlated predictor variables. Read more in Chapter @ref(multicollinearity).

Multicollinearity is an important issue in regression analysis and should be fixed by removing the concerned variables. It can be assessed using the R function `vif()` [`car` package], which computes the variance inflation factors:

```{r}
car::vif(fit_glm_2)
```

As a rule of thumb, a **`VIF` value that exceeds 5 or 10 indicates a problematic amount of collinearity**. In our example, there is no collinearity: all variables have a value of `VIF` well below 5.


## Compare Models

### Refit to Complete Cases

Restrict the data to only participant that have all four of these predictors.

```{r}
df_depress_model <- df_depress %>% 
                        dplyr::filter(complete.cases(sex, iq, age, weight))
```



Refit Model 1 with only participant complete on all the predictors.

```{r}
fit_glm_1_redo <- glm(satlife ~ sex,
                      data = df_depress_model)

fit_glm_2_redo <- glm(satlife ~ sex + iq + age + weight,
                      data = df_depress_model)
```


```{r, results='asis'}
texreg::knitreg(list(texreghelpr::extract_glm_exp(fit_glm_1_redo),
                     texreghelpr::extract_glm_exp(fit_glm_2_redo)),
                custom.model.names = c("Single IV",
                                       "Multiple IVs"),
                caption.above = TRUE,
                single.row = TRUE,
                digits = 4,
                ci.test = 1)
```


```{r}
anova(fit_glm_1_redo, 
      fit_glm_2_redo,
      test = "LRT")
```


```{r}
performance::compare_performance(fit_glm_1_redo, 
                                 fit_glm_2_redo, 
                                 rank = TRUE)
```


### Interpretation

* Only sex is predictive of depression.  There is no evidence IQ, age, or weight are associated with depression, all p's > .16.